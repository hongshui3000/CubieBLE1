L 1 "..\..\..\..\drivers\uart\uart_acts.c"
L 1 "..\src\include\autoconf_app.h" 1
N/* common config */
N#include "autoconf.h"
L 1 "..\..\..\..\include\generated\autoconf.h" 1
N/*
N *
N * Automatically generated file; DO NOT EDIT.
N * Zephyr Kernel Configuration
N *
N */
N
N/* shell */
N#define CONFIG_KERNEL_SHELL 1
N
N#define CONFIG_CONSOLE_SHELL 1
N#define CONFIG_CONSOLE_SHELL_MAX_CMD_QUEUED 3
N#define CONFIG_CONSOLE_SHELL_STACKSIZE 1024
N#define CONFIG_UART_CONSOLE_INIT_PRIORITY 60
N
N/* printk & sys_log */
N#define CONFIG_PRINTK 1
N#define CONFIG_SYS_LOG 1
N#define CONFIG_SYS_LOG_SHOW_TAGS 0
N#define CONFIG_SYS_LOG_DEFAULT_LEVEL SYS_LOG_LEVEL_INFO
N#define CONFIG_SYS_LOG_OVERRIDE_LEVEL 0
N#define CONFIG_UART_CONSOLE_ON_DEV_NAME "UART_0"
N
N/* workqueue */
N#define CONFIG_SYSTEM_WORKQUEUE_STACK_SIZE 1024
N
N/* stack */
N#define CONFIG_MAIN_STACK_SIZE 1024
N#define CONFIG_IDLE_STACK_SIZE 256
N#define CONFIG_ISR_STACK_SIZE 640
N
N/* init priority level */
N#define CONFIG_KERNEL_INIT_PRIORITY_OBJECTS 30
N#define CONFIG_KERNEL_INIT_PRIORITY_DEFAULT 40
N#define CONFIG_KERNEL_INIT_PRIORITY_DEVICE 50
N
N/* async msg */
N#define CONFIG_NUM_PIPE_ASYNC_MSGS 10
N#define CONFIG_NUM_MBOX_ASYNC_MSGS 10
N
N/* dma */
N#define CONFIG_DMA_ACTS_DEVICE_INIT_PRIORITY 40
N
N#define CONFIG_DMA_0_NAME "DMA_0"
N#define CONFIG_DMA_0_IRQ_PRI 3
N
N/* rtc */
N#define CONFIG_RTC_0_NAME "RTC_0"
N#define CONFIG_RTC_0_IRQ_PRI 0
N
N/* pwm */
N#define CONFIG_PWM_ACTS_DEV_NAME "PWM_0"
N
N/* spi */
N#define CONFIG_SPI_INIT_PRIORITY 70
N
N#define CONFIG_SPI_1_NAME "SPI_1"
N#define CONFIG_SPI_1_IRQ_PRI 0
N#define CONFIG_SPI_1_DEFAULT_CFG 0x80
N#define CONFIG_SPI_1_DEFAULT_BAUD_RATE 500000
N
N#define CONFIG_SPI_2_NAME "SPI_2"
N#define CONFIG_SPI_2_IRQ_PRI 0
N#define CONFIG_SPI_2_DEFAULT_CFG 0x80
N#define CONFIG_SPI_2_DEFAULT_BAUD_RATE 500000
N
N/* spinor */
N#define CONFIG_XSPI_NOR_ACTS_DEV_NAME "xspi_nor"
N#define CONFIG_XSPI_NOR_ACTS_DEV_INIT_PRIORITY 45
N
N/* nvram */
N#define CONFIG_NVRAM_CONFIG 1
N#define CONFIG_NVRAM_ACTS_DRV_NAME "NVRAM"
N#define CONFIG_NVRAM_STORAGE_DEV_NAME CONFIG_XSPI_NOR_ACTS_DEV_NAME
N#define CONFIG_NVRAM_CONFIG_INIT_PRIORITY 48
N#define CONFIG_NVRAM_FACTORY_REGION_BASE_ADDR 0x70000
N#define CONFIG_NVRAM_FACTORY_REGION_SIZE 0x2000
N#define CONFIG_NVRAM_WRITE_REGION_BASE_ADDR 0x72000
N#define CONFIG_NVRAM_WRITE_REGION_SIZE 0x4000
N
N/* adc */
N#define CONFIG_ADC_INIT_PRIORITY 80
N#define CONFIG_ADC_0_NAME "ADC_0"
N#define CONFIG_ADC_0_IRQ_PRI 2
N
N/* i2c */
N#define CONFIG_I2C_INIT_PRIORITY 60
N
N#define CONFIG_I2C_0_NAME "I2C_0"
N#define CONFIG_I2C_0_DEFAULT_CFG 0x0
N#define CONFIG_I2C_0_IRQ_PRI 0
N
N#define CONFIG_I2C_1_NAME "I2C_1"
N#define CONFIG_I2C_1_DEFAULT_CFG 0x0
N#define CONFIG_I2C_1_IRQ_PRI 0
N
N/* gpio */
N#define CONFIG_GPIO_ACTS_DRV_NAME "GPIO"
N#define CONFIG_GPIO_ACTS_INIT_PRIORITY 20
N
N/* input */
N#define CONFIG_SYS_LOG_INPUT_DEV_LEVEL 0
N
N/* inpu-matrix key */
N#define CONFIG_INPUT_DEV_ACTS_MARTRIX_KEYPAD_NAME "MXKEYPAD"
N#define CONFIG_INPUT_DEV_ACTS_MARTRIX_KEYPAD_NAME_IRQ_PRI 0
N
N/* input-adckey */
N#define CONFIG_INPUT_DEV_ACTS_ADCKEY_NAME "ADCKEY"
N
N/* input-ir */
N#define CONFIG_IRC_ACTS_DEV_NAME "IRC" 
N
N#define CONFIG_SW_IRC_ACTS_DEV_NAME "SWIRC"
N
N/* audio_in */
N#define CONFIG_AUDIO_IN_ACTS_NAME "AUDIOIN"
N
N/* audio_out */
N#define CONFIG_AUDIO_OUT_ACTS_NAME "AUDIOOUT"
N
N/* watchdog */
N#define CONFIG_WDG_ACTS_DEV_NAME "WATCHDOG"
N
N/* bt */
N/* host stack Configuration*/
N#define CONFIG_BT_MAX_CONN 4
N#define CONFIG_BT_MAX_PAIRED 2
N
N/* host stack buffer config */
N#define CONFIG_BT_HCI_CMD_COUNT 2
N#define CONFIG_BT_RX_BUF_COUNT 4
N#define CONFIG_BT_RX_BUF_LEN 76
N#define CONFIG_BT_L2CAP_TX_BUF_COUNT 3
N#define CONFIG_BT_L2CAP_TX_MTU 65
N#define CONFIG_BT_CONN_TX_MAX 7
N
N/* bt thread stack size*/
N#define CONFIG_BT_HCI_TX_STACK_SIZE 256
N#define CONFIG_BT_RX_STACK_SIZE 1024
N
N/* att */
N#define CONFIG_BT_ATT_PREPARE_COUNT 0
N#define CONFIG_BT_ATT_TX_MAX 2
N
N/* gap name */
N#define CONFIG_BT_DEVICE_NAME "Zephyr"
N#define CONFIG_BT_DEVICE_APPEARANCE 0
N
N/* private addr */
N#define CONFIG_BT_PRIVACY 0
N#define CONFIG_BT_RPA_TIMEOUT 900
N
N#define CONFIG_BT_DEBUG_LOG 1
N#define CONFIG_CCC_STORE_MAX 10
N
N/* deepsleep */
N#define CONFIG_DEEPSLEEP 0
N#define CONFIG_DEEPSLEEP_TICK_THRESH 10
N#define CONFIG_DEEPSLEEP_SWITCH_32M 1
N
N/* ota */
N#define CONFIG_OTA_WITH_APP 1
L 3 "..\src\include\autoconf_app.h" 2
N
N/* application config */
N/* NVRAM */
N#undef CONFIG_NVRAM_CONFIG
N#define CONFIG_NVRAM_CONFIG 0
N
N/* BT */
N#undef CONFIG_BT_DEVICE_NAME
N#undef CONFIG_BT_MAX_CONN
N#undef CONFIG_BT_MAX_PAIRED
N#undef CONFIG_BT_RX_BUF_COUNT
N#undef CONFIG_BT_L2CAP_TX_BUF_COUNT
N
N#define CONFIG_BT_DEVICE_NAME "110X Airsync"
N#define CONFIG_BT_MAX_CONN 1
N#define CONFIG_BT_MAX_PAIRED 1
N#define CONFIG_BT_RX_BUF_COUNT 3
N#define CONFIG_BT_L2CAP_TX_BUF_COUNT 4
N
N/* STACK 
N#undef CONFIG_IDLE_STACK_SIZE
N#undef CONFIG_BT_RX_STACK_SIZE
N#undef CONFIG_CONSOLE_SHELL_STACKSIZE
N
N#define CONFIG_IDLE_STACK_SIZE (256+256)
N#define CONFIG_BT_RX_STACK_SIZE (1280 - 300)
N#define CONFIG_CONSOLE_SHELL_STACKSIZE 1024*/
N
N/* DEEPSLEEP
N#undef CONFIG_DEEPSLEEP
N#define CONFIG_DEEPSLEEP 0 */
N
N/* BOARD */
N#define CONFIG_UART_0 1
N#define BOARD_PIN_CONFIG	\
N	{2, 3 | GPIO_CTL_SMIT | GPIO_CTL_PADDRV_LEVEL(3)},\
N	{3, 3 | GPIO_CTL_SMIT | GPIO_CTL_PADDRV_LEVEL(3)}
X#define BOARD_PIN_CONFIG		{2, 3 | GPIO_CTL_SMIT | GPIO_CTL_PADDRV_LEVEL(3)},	{3, 3 | GPIO_CTL_SMIT | GPIO_CTL_PADDRV_LEVEL(3)}
L 1 "..\..\..\..\drivers\uart\uart_acts.c" 2
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N#include <kernel.h>
L 1 "..\..\..\..\include\kernel.h" 1
N/*
N * Copyright (c) 2016, Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N *
N * @brief Public kernel APIs.
N */
N
N#ifndef _kernel__h_
N#define _kernel__h_
N
N#if !defined(_ASMLANGUAGE)
X#if !0L
N#include <stddef.h>
L 1 "F:\Keil_v5\ARM\ARMCC\Bin\..\include\stddef.h" 1
N/* stddef.h: ANSI 'C' (X3J11 Oct 88) library header, section 4.1.4 */
N
N/* Copyright (C) ARM Ltd., 1999
N * All rights reserved
N * RCS $Revision$
N * Checkin $Date$
N * Revising $Author: agrant $
N */
N
N/* Copyright (C) Codemist Ltd., 1988                            */
N/* Copyright 1991 ARM Limited. All rights reserved.             */
N/* version 0.05 */
N
N/*
N * The following types and macros are defined in several headers referred to in
N * the descriptions of the functions declared in that header. They are also
N * defined in this header file.
N */
N
N#ifndef __stddef_h
N#define __stddef_h
N#define __ARMCLIB_VERSION 5060034
N
N  #ifndef __STDDEF_DECLS
N  #define __STDDEF_DECLS
N    #undef __CLIBNS
N    #ifdef __cplusplus
S        namespace std {
S        #define __CLIBNS ::std::
S        extern "C" {
N    #else
N      #define __CLIBNS
N    #endif  /* __cplusplus */
N
N#if __sizeof_ptr == 8
X#if 4 == 8
S  typedef signed long ptrdiff_t;
N#else
N  typedef signed int ptrdiff_t;
N#endif
N
N#if defined(__cplusplus) || !defined(__STRICT_ANSI__)
X#if 0L || !0L
N /* unconditional in C++ and non-strict C for consistency of debug info */
N  #if __sizeof_ptr == 8
X  #if 4 == 8
S    typedef unsigned long size_t;   /* see <stddef.h> */
N  #else
N    typedef unsigned int size_t;   /* see <stddef.h> */
N  #endif
N#elif !defined(__size_t)
X#elif !0L
S  #define __size_t 1
S  #if __sizeof_ptr == 8
S    typedef unsigned long size_t;   /* see <stddef.h> */
S  #else
S    typedef unsigned int size_t;   /* see <stddef.h> */
S  #endif
S   /* the unsigned integral type of the result of the sizeof operator. */
N#endif
N
N#ifndef __cplusplus  /* wchar_t is a builtin type for C++ */
N  #if !defined(__STRICT_ANSI__)
X  #if !0L
N  /* unconditional in non-strict C for consistency of debug info */
N    #if defined(__WCHAR32) || (defined(__ARM_SIZEOF_WCHAR_T) && __ARM_SIZEOF_WCHAR_T == 4)
X    #if 0L || (0L && __ARM_SIZEOF_WCHAR_T == 4)
S      typedef unsigned int wchar_t; /* also in <stdlib.h> and <inttypes.h> */
N    #else
N      typedef unsigned short wchar_t; /* also in <stdlib.h> and <inttypes.h> */
N    #endif
N  #elif !defined(__wchar_t)
X  #elif !0L
S    #define __wchar_t 1
S    #if defined(__WCHAR32) || (defined(__ARM_SIZEOF_WCHAR_T) && __ARM_SIZEOF_WCHAR_T == 4)
S      typedef unsigned int wchar_t; /* also in <stdlib.h> and <inttypes.h> */
S    #else
S      typedef unsigned short wchar_t; /* also in <stdlib.h> and <inttypes.h> */
S    #endif
S   /*
S    * An integral type whose range of values can represent distinct codes for
S    * all members of the largest extended character set specified among the
S    * supported locales; the null character shall have the code value zero and
S    * each member of the basic character set shall have a code value when used
S    * as the lone character in an integer character constant.
S    */
N  #endif
N#endif
N
N#undef NULL  /* others (e.g. <stdio.h>) also define */
N#define NULL 0
N   /* null pointer constant. */
N
N#ifdef __clang__
S  #define offsetof(t, d) __builtin_offsetof(t, d)
N#else
N  /* EDG uses __INTADDR__ to avoid errors when strict */
N  #define offsetof(t, memb) ((__CLIBNS size_t)__INTADDR__(&(((t *)0)->memb)))
N#endif
N
N#if !defined(__STRICT_ANSI__) || (defined(__STDC_VERSION__) && 201112L <= __STDC_VERSION__) || (defined(__cplusplus) && 201103L <= __cplusplus)
X#if !0L || (1L && 201112L <= 199409L) || (0L && 201103L <= __cplusplus)
N  typedef long double max_align_t;
N#endif
N
N    #ifdef __cplusplus
S         }  /* extern "C" */
S      }  /* namespace std */
N    #endif /* __cplusplus */
N  #endif /* __STDDEF_DECLS */
N
N
N  #ifdef __cplusplus
S    #ifndef __STDDEF_NO_EXPORTS
S      using ::std::size_t;
S      using ::std::ptrdiff_t;
S      #if !defined(__STRICT_ANSI__) || (defined(__cplusplus) && 201103L <= __cplusplus)
S        using ::std::max_align_t;
S      #endif
S    #endif 
N  #endif /* __cplusplus */
N
N#endif
N
N/* end of stddef.h */
N
L 18 "..\..\..\..\include\kernel.h" 2
N#include <zephyr/types.h>
L 1 "..\..\..\..\include\zephyr/types.h" 1
N/*
N * Copyright (c) 2017 Linaro Limited
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N#ifndef __Z_TYPES_H__
N#define __Z_TYPES_H__
N
N#include <stdint.h>
L 1 "F:\Keil_v5\ARM\ARMCC\Bin\..\include\stdint.h" 1
N/* Copyright (C) ARM Ltd., 1999,2014 */
N/* All rights reserved */
N
N/*
N * RCS $Revision$
N * Checkin $Date$
N * Revising $Author: agrant $
N */
N
N#ifndef __stdint_h
N#define __stdint_h
N#define __ARMCLIB_VERSION 5060034
N
N  #ifdef __INT64_TYPE__
S    /* armclang predefines '__INT64_TYPE__' and '__INT64_C_SUFFIX__' */
S    #define __INT64 __INT64_TYPE__
N  #else
N    /* armcc has builtin '__int64' which can be used in --strict mode */
N    #define __INT64 __int64
N    #define __INT64_C_SUFFIX__ ll
N  #endif
N  #define __PASTE2(x, y) x ## y
N  #define __PASTE(x, y) __PASTE2(x, y)
N  #define __INT64_C(x)  __ESCAPE__(__PASTE(x, __INT64_C_SUFFIX__))
N  #define __UINT64_C(x)  __ESCAPE__(__PASTE(x ## u, __INT64_C_SUFFIX__))
N  #if defined(__clang__) || (defined(__ARMCC_VERSION) && !defined(__STRICT_ANSI__))
X  #if 0L || (1L && !0L)
N    /* armclang and non-strict armcc allow 'long long' in system headers */
N    #define __LONGLONG long long
N  #else
S    /* strict armcc has '__int64' */
S    #define __LONGLONG __int64
N  #endif
N
N  #ifndef __STDINT_DECLS
N  #define __STDINT_DECLS
N
N    #undef __CLIBNS
N
N    #ifdef __cplusplus
S      namespace std {
S          #define __CLIBNS std::
S          extern "C" {
N    #else
N      #define __CLIBNS
N    #endif  /* __cplusplus */
N
N
N/*
N * 'signed' is redundant below, except for 'signed char' and if
N * the typedef is used to declare a bitfield.
N */
N
N    /* 7.18.1.1 */
N
N    /* exact-width signed integer types */
Ntypedef   signed          char int8_t;
Ntypedef   signed short     int int16_t;
Ntypedef   signed           int int32_t;
Ntypedef   signed       __INT64 int64_t;
Xtypedef   signed       __int64 int64_t;
N
N    /* exact-width unsigned integer types */
Ntypedef unsigned          char uint8_t;
Ntypedef unsigned short     int uint16_t;
Ntypedef unsigned           int uint32_t;
Ntypedef unsigned       __INT64 uint64_t;
Xtypedef unsigned       __int64 uint64_t;
N
N    /* 7.18.1.2 */
N
N    /* smallest type of at least n bits */
N    /* minimum-width signed integer types */
Ntypedef   signed          char int_least8_t;
Ntypedef   signed short     int int_least16_t;
Ntypedef   signed           int int_least32_t;
Ntypedef   signed       __INT64 int_least64_t;
Xtypedef   signed       __int64 int_least64_t;
N
N    /* minimum-width unsigned integer types */
Ntypedef unsigned          char uint_least8_t;
Ntypedef unsigned short     int uint_least16_t;
Ntypedef unsigned           int uint_least32_t;
Ntypedef unsigned       __INT64 uint_least64_t;
Xtypedef unsigned       __int64 uint_least64_t;
N
N    /* 7.18.1.3 */
N
N    /* fastest minimum-width signed integer types */
Ntypedef   signed           int int_fast8_t;
Ntypedef   signed           int int_fast16_t;
Ntypedef   signed           int int_fast32_t;
Ntypedef   signed       __INT64 int_fast64_t;
Xtypedef   signed       __int64 int_fast64_t;
N
N    /* fastest minimum-width unsigned integer types */
Ntypedef unsigned           int uint_fast8_t;
Ntypedef unsigned           int uint_fast16_t;
Ntypedef unsigned           int uint_fast32_t;
Ntypedef unsigned       __INT64 uint_fast64_t;
Xtypedef unsigned       __int64 uint_fast64_t;
N
N    /* 7.18.1.4 integer types capable of holding object pointers */
N#if __sizeof_ptr == 8
X#if 4 == 8
Stypedef   signed       __INT64 intptr_t;
Stypedef unsigned       __INT64 uintptr_t;
N#else
Ntypedef   signed           int intptr_t;
Ntypedef unsigned           int uintptr_t;
N#endif
N
N    /* 7.18.1.5 greatest-width integer types */
Ntypedef   signed     __LONGLONG intmax_t;
Xtypedef   signed     long long intmax_t;
Ntypedef unsigned     __LONGLONG uintmax_t;
Xtypedef unsigned     long long uintmax_t;
N
N
N#if !defined(__cplusplus) || defined(__STDC_LIMIT_MACROS)
X#if !0L || 0L
N
N    /* 7.18.2.1 */
N
N    /* minimum values of exact-width signed integer types */
N#define INT8_MIN                   -128
N#define INT16_MIN                -32768
N#define INT32_MIN          (~0x7fffffff)   /* -2147483648 is unsigned */
N#define INT64_MIN  __INT64_C(~0x7fffffffffffffff) /* -9223372036854775808 is unsigned */
N
N    /* maximum values of exact-width signed integer types */
N#define INT8_MAX                    127
N#define INT16_MAX                 32767
N#define INT32_MAX            2147483647
N#define INT64_MAX  __INT64_C(9223372036854775807)
N
N    /* maximum values of exact-width unsigned integer types */
N#define UINT8_MAX                   255
N#define UINT16_MAX                65535
N#define UINT32_MAX           4294967295u
N#define UINT64_MAX __UINT64_C(18446744073709551615)
N
N    /* 7.18.2.2 */
N
N    /* minimum values of minimum-width signed integer types */
N#define INT_LEAST8_MIN                   -128
N#define INT_LEAST16_MIN                -32768
N#define INT_LEAST32_MIN          (~0x7fffffff)
N#define INT_LEAST64_MIN  __INT64_C(~0x7fffffffffffffff)
N
N    /* maximum values of minimum-width signed integer types */
N#define INT_LEAST8_MAX                    127
N#define INT_LEAST16_MAX                 32767
N#define INT_LEAST32_MAX            2147483647
N#define INT_LEAST64_MAX  __INT64_C(9223372036854775807)
N
N    /* maximum values of minimum-width unsigned integer types */
N#define UINT_LEAST8_MAX                   255
N#define UINT_LEAST16_MAX                65535
N#define UINT_LEAST32_MAX           4294967295u
N#define UINT_LEAST64_MAX __UINT64_C(18446744073709551615)
N
N    /* 7.18.2.3 */
N
N    /* minimum values of fastest minimum-width signed integer types */
N#define INT_FAST8_MIN           (~0x7fffffff)
N#define INT_FAST16_MIN          (~0x7fffffff)
N#define INT_FAST32_MIN          (~0x7fffffff)
N#define INT_FAST64_MIN  __INT64_C(~0x7fffffffffffffff)
N
N    /* maximum values of fastest minimum-width signed integer types */
N#define INT_FAST8_MAX             2147483647
N#define INT_FAST16_MAX            2147483647
N#define INT_FAST32_MAX            2147483647
N#define INT_FAST64_MAX  __INT64_C(9223372036854775807)
N
N    /* maximum values of fastest minimum-width unsigned integer types */
N#define UINT_FAST8_MAX            4294967295u
N#define UINT_FAST16_MAX           4294967295u
N#define UINT_FAST32_MAX           4294967295u
N#define UINT_FAST64_MAX __UINT64_C(18446744073709551615)
N
N    /* 7.18.2.4 */
N
N    /* minimum value of pointer-holding signed integer type */
N#if __sizeof_ptr == 8
X#if 4 == 8
S#define INTPTR_MIN INT64_MIN
N#else
N#define INTPTR_MIN INT32_MIN
N#endif
N
N    /* maximum value of pointer-holding signed integer type */
N#if __sizeof_ptr == 8
X#if 4 == 8
S#define INTPTR_MAX INT64_MAX
N#else
N#define INTPTR_MAX INT32_MAX
N#endif
N
N    /* maximum value of pointer-holding unsigned integer type */
N#if __sizeof_ptr == 8
X#if 4 == 8
S#define UINTPTR_MAX UINT64_MAX
N#else
N#define UINTPTR_MAX UINT32_MAX
N#endif
N
N    /* 7.18.2.5 */
N
N    /* minimum value of greatest-width signed integer type */
N#define INTMAX_MIN  __ESCAPE__(~0x7fffffffffffffffll)
N
N    /* maximum value of greatest-width signed integer type */
N#define INTMAX_MAX  __ESCAPE__(9223372036854775807ll)
N
N    /* maximum value of greatest-width unsigned integer type */
N#define UINTMAX_MAX __ESCAPE__(18446744073709551615ull)
N
N    /* 7.18.3 */
N
N    /* limits of ptrdiff_t */
N#if __sizeof_ptr == 8
X#if 4 == 8
S#define PTRDIFF_MIN INT64_MIN
S#define PTRDIFF_MAX INT64_MAX
N#else
N#define PTRDIFF_MIN INT32_MIN
N#define PTRDIFF_MAX INT32_MAX
N#endif
N
N    /* limits of sig_atomic_t */
N#define SIG_ATOMIC_MIN (~0x7fffffff)
N#define SIG_ATOMIC_MAX   2147483647
N
N    /* limit of size_t */
N#if __sizeof_ptr == 8
X#if 4 == 8
S#define SIZE_MAX UINT64_MAX
N#else
N#define SIZE_MAX UINT32_MAX
N#endif
N
N    /* limits of wchar_t */
N    /* NB we have to undef and redef because they're defined in both
N     * stdint.h and wchar.h */
N#undef WCHAR_MIN
N#undef WCHAR_MAX
N
N#if defined(__WCHAR32) || (defined(__ARM_SIZEOF_WCHAR_T) && __ARM_SIZEOF_WCHAR_T == 4)
X#if 0L || (0L && __ARM_SIZEOF_WCHAR_T == 4)
S  #define WCHAR_MIN   0
S  #define WCHAR_MAX   0xffffffffU
N#else
N  #define WCHAR_MIN   0
N  #define WCHAR_MAX   65535
N#endif
N
N    /* limits of wint_t */
N#define WINT_MIN (~0x7fffffff)
N#define WINT_MAX 2147483647
N
N#endif /* __STDC_LIMIT_MACROS */
N
N#if !defined(__cplusplus) || defined(__STDC_CONSTANT_MACROS)
X#if !0L || 0L
N
N    /* 7.18.4.1 macros for minimum-width integer constants */
N#define INT8_C(x)   (x)
N#define INT16_C(x)  (x)
N#define INT32_C(x)  (x)
N#define INT64_C(x)  __INT64_C(x)
N
N#define UINT8_C(x)  (x ## u)
N#define UINT16_C(x) (x ## u)
N#define UINT32_C(x) (x ## u)
N#define UINT64_C(x) __UINT64_C(x)
N
N    /* 7.18.4.2 macros for greatest-width integer constants */
N#define INTMAX_C(x)  __ESCAPE__(x ## ll)
N#define UINTMAX_C(x) __ESCAPE__(x ## ull)
N
N#endif /* __STDC_CONSTANT_MACROS */
N
N    #ifdef __cplusplus
S         }  /* extern "C" */
S      }  /* namespace std */
N    #endif /* __cplusplus */
N  #endif /* __STDINT_DECLS */
N
N  #ifdef __cplusplus
S    #ifndef __STDINT_NO_EXPORTS
S      using ::std::int8_t;
S      using ::std::int16_t;
S      using ::std::int32_t;
S      using ::std::int64_t;
S      using ::std::uint8_t;
S      using ::std::uint16_t;
S      using ::std::uint32_t;
S      using ::std::uint64_t;
S      using ::std::int_least8_t;
S      using ::std::int_least16_t;
S      using ::std::int_least32_t;
S      using ::std::int_least64_t;
S      using ::std::uint_least8_t;
S      using ::std::uint_least16_t;
S      using ::std::uint_least32_t;
S      using ::std::uint_least64_t;
S      using ::std::int_fast8_t;
S      using ::std::int_fast16_t;
S      using ::std::int_fast32_t;
S      using ::std::int_fast64_t;
S      using ::std::uint_fast8_t;
S      using ::std::uint_fast16_t;
S      using ::std::uint_fast32_t;
S      using ::std::uint_fast64_t;
S      using ::std::intptr_t;
S      using ::std::uintptr_t;
S      using ::std::intmax_t;
S      using ::std::uintmax_t;
S    #endif
N  #endif /* __cplusplus */
N
N#undef __INT64
N#undef __LONGLONG
N
N#endif /* __stdint_h */
N
N/* end of stdint.h */
L 11 "..\..\..\..\include\zephyr/types.h" 2
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
Ntypedef signed char         s8_t;
Ntypedef signed short        s16_t;
Ntypedef signed int          s32_t;
Ntypedef signed long long    s64_t;
N
Ntypedef unsigned char       u8_t;
Ntypedef unsigned short      u16_t;
Ntypedef unsigned int        u32_t;
Ntypedef unsigned long long  u64_t;
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* __Z_TYPES_H__ */
L 19 "..\..\..\..\include\kernel.h" 2
N#include <limits.h>
L 1 "F:\Keil_v5\ARM\ARMCC\Bin\..\include\limits.h" 1
N/* limits.h: ANSI 'C' (X3J11 Oct 88) library header, section 2.2.4.2 */
N/* Copyright (C) Codemist Ltd., 1988                            */
N/* Copyright 1991-1997 ARM Limited. All rights reserved         */
N
N/*
N * RCS $Revision$
N * Checkin $Date$
N * Revising $Author: drodgman $
N */
N
N#ifndef __limits_h
N#define __limits_h
N#define __ARMCLIB_VERSION 5060034
N
N#define CHAR_BIT 8
N    /* max number of bits for smallest object that is not a bit-field (byte) */
N#define SCHAR_MIN (-128)
N    /* mimimum value for an object of type signed char */
N#define SCHAR_MAX 127
N    /* maximum value for an object of type signed char */
N#define UCHAR_MAX 255
N    /* maximum value for an object of type unsigned char */
N#ifdef __FEATURE_SIGNED_CHAR
S  #define CHAR_MIN (-128)
S      /* minimum value for an object of type char */
S  #define CHAR_MAX 127
S      /* maximum value for an object of type char */
N#else
N  #define CHAR_MIN 0
N      /* minimum value for an object of type char */
N  #define CHAR_MAX 255
N      /* maximum value for an object of type char */
N#endif
N
N#if _AEABI_PORTABILITY_LEVEL != 0 && !defined _AEABI_PORTABLE
X#if _AEABI_PORTABILITY_LEVEL != 0 && !0L
S  #define _AEABI_PORTABLE
N#endif
N
N#if _AEABI_PORTABILITY_LEVEL != 0 || (!defined _AEABI_PORTABILITY_LEVEL && __DEFAULT_AEABI_PORTABILITY_LEVEL != 0)
X#if _AEABI_PORTABILITY_LEVEL != 0 || (!0L && __DEFAULT_AEABI_PORTABILITY_LEVEL != 0)
Sextern const int __aeabi_MB_LEN_MAX;
S#define MB_LEN_MAX (__aeabi_MB_LEN_MAX)
N#else
N#define MB_LEN_MAX 6
N#endif
N    /* maximum number of bytes in a multibyte character, */
N    /* for any supported locale */
N
N#define SHRT_MIN  (-0x8000)
N    /* minimum value for an object of type short int */
N#define SHRT_MAX  0x7fff
N    /* maximum value for an object of type short int */
N#define USHRT_MAX 65535
N    /* maximum value for an object of type unsigned short int */
N#define INT_MIN   (~0x7fffffff)  /* -2147483648 and 0x80000000 are unsigned */
N    /* minimum value for an object of type int */
N#define INT_MAX   0x7fffffff
N    /* maximum value for an object of type int */
N#define UINT_MAX  0xffffffffU
N    /* maximum value for an object of type unsigned int */
N#if __sizeof_long == 8
X#if 4 == 8
S  #define LONG_MIN  (~0x7fffffffffffffffL)
N#else
N  #define LONG_MIN  (~0x7fffffffL)
N#endif
N    /* minimum value for an object of type long int */
N#if __sizeof_long == 8
X#if 4 == 8
S  #define LONG_MAX  0x7fffffffffffffffL
N#else
N  #define LONG_MAX  0x7fffffffL
N#endif
N    /* maximum value for an object of type long int */
N#if __sizeof_long == 8
X#if 4 == 8
S  #define ULONG_MAX 0xffffffffffffffffUL
N#else
N  #define ULONG_MAX 0xffffffffUL
N#endif
N    /* maximum value for an object of type unsigned long int */
N#if !defined(__STRICT_ANSI__) || (defined(__STDC_VERSION__) && 199901L <= __STDC_VERSION__) || (defined(__cplusplus) && 201103L <= __cplusplus)
X#if !0L || (1L && 199901L <= 199409L) || (0L && 201103L <= __cplusplus)
N  #define LLONG_MIN  (~0x7fffffffffffffffLL)
N      /* minimum value for an object of type long long int */
N  #define LLONG_MAX    0x7fffffffffffffffLL
N      /* maximum value for an object of type long long int */
N  #define ULLONG_MAX   0xffffffffffffffffULL
N      /* maximum value for an object of type unsigned long int */
N#endif
N
N#endif
N
N/* end of limits.h */
N
L 20 "..\..\..\..\include\kernel.h" 2
N#include <toolchain.h>
L 1 "..\..\..\..\include\toolchain.h" 1
N/*
N * Copyright (c) 2010-2014, Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Macros to abstract toolchain specific capabilities
N *
N * This file contains various macros to abstract compiler capabilities that
N * utilize toolchain specific attributes and/or pragmas.
N */
N
N#ifndef _TOOLCHAIN_H
N#define _TOOLCHAIN_H
N
N#include <toolchain/gcc.h>
L 1 "..\..\..\..\include\toolchain/gcc.h" 1
N/*
N * Copyright (c) 2010-2014,2017 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief GCC toolchain abstraction
N *
N * Macros to abstract compiler capabilities for GCC toolchain.
N */
N
N#include <toolchain/common.h>
L 1 "..\..\..\..\include\toolchain/common.h" 1
N/*
N * Copyright (c) 2010-2014 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Common toolchain abstraction
N *
N * Macros to abstract compiler capabilities (common to all toolchains).
N */
N
N/* Abstract use of extern keyword for compatibility between C and C++ */
N#ifdef __cplusplus
S#define EXTERN_C extern "C"
N#else
N#define EXTERN_C extern
N#endif
N
N/* Use TASK_ENTRY_CPP to tag task entry points defined in C++ files. */
N
N#ifdef __cplusplus
S#define TASK_ENTRY_CPP  extern "C"
N#endif
N
N/*
N * Generate a reference to an external symbol.
N * The reference indicates to the linker that the symbol is required
N * by the module containing the reference and should be included
N * in the image if the module is in the image.
N *
N * The assembler directive ".set" is used to define a local symbol.
N * No memory is allocated, and the local symbol does not appear in
N * the symbol table.
N */
N
N#ifdef _ASMLANGUAGE
S  #define REQUIRES(sym) .set sym ## _Requires, sym
N#else
N  #define REQUIRES(sym) __asm__ (".set " # sym "_Requires, " # sym "\n\t");
N#endif
N
N#ifdef _ASMLANGUAGE
S  #define SECTION .section
N#endif
N
N
N/* force inlining a function */
N
N#if !defined(_ASMLANGUAGE)
X#if !0L
N  #define ALWAYS_INLINE inline __attribute__((always_inline))
N#endif
N
N#define _STRINGIFY(x) #x
N#define STRINGIFY(s) _STRINGIFY(s)
N
N/* Indicate that an array will be used for stack space. */
N
N#if !defined(_ASMLANGUAGE)
X#if !0L
N  /* don't use this anymore, use K_DECLARE_STACK instead. Remove for 1.11 */
N  #define __stack __aligned(STACK_ALIGN) __DEPRECATED_MACRO
N#endif
N
N/* concatenate the values of the arguments into one */
N#define _DO_CONCAT(x, y) x ## y
N#define _CONCAT(x, y) _DO_CONCAT(x, y)
N
N#ifndef BUILD_ASSERT
N/* compile-time assertion that makes the build fail */
N#define BUILD_ASSERT(EXPR) typedef char __build_assert_failure[(EXPR) ? 1 : -1]
N#endif
N#ifndef BUILD_ASSERT_MSG
N/* build assertion with message -- common implementation swallows message. */
N#define BUILD_ASSERT_MSG(EXPR, MSG) BUILD_ASSERT(EXPR)
N#endif
L 15 "..\..\..\..\include\toolchain/gcc.h" 2
N
N#define ALIAS_OF(of) __attribute__((alias(#of)))
N
N#define FUNC_ALIAS(real_func, new_alias, return_type) \
N	return_type new_alias() ALIAS_OF(real_func)
X#define FUNC_ALIAS(real_func, new_alias, return_type) 	return_type new_alias() ALIAS_OF(real_func)
N
N#define CODE_UNREACHABLE __builtin_unreachable()
N#define FUNC_NORETURN    __attribute__((__noreturn__))
N
N/* Double indirection to ensure section names are expanded before
N * stringification
N */
N#define __GENERIC_SECTION(segment) __attribute__((section(STRINGIFY(segment))))
N#define _GENERIC_SECTION(segment) __GENERIC_SECTION(segment)
N
N#define ___in_section(a, b, c) \
N	__attribute__((section("." _STRINGIFY(a)			\
N				"." _STRINGIFY(b)			\
N				"." _STRINGIFY(c))))
X#define ___in_section(a, b, c) 	__attribute__((section("." _STRINGIFY(a)							"." _STRINGIFY(b)							"." _STRINGIFY(c))))
N#define __in_section(a, b, c) ___in_section(a, b, c)
N
N#define __in_section_unique(seg) ___in_section(seg, __FILE__, __COUNTER__)
N
N
N#ifndef __packed
N#define __packed        __attribute__((__packed__))
N#endif
N#ifndef __aligned
N#define __aligned(x)	__attribute__((__aligned__(x)))
N#endif
N#define __may_alias     __attribute__((__may_alias__))
N#ifndef __printf_like
N#define __printf_like(f, a)   __attribute__((format (printf, f, a)))
N#endif
N#define __used		__attribute__((__used__))
N#define __deprecated	__attribute__((deprecated))
N#define ARG_UNUSED(x) (void)(x)
N
N#define likely(x)   __builtin_expect((long)!!(x), 1L)
N#define unlikely(x) __builtin_expect((long)!!(x), 0L)
N
N#define popcount(x) __builtin_popcount(x)
N
N#define __weak __attribute__((__weak__))
N#define __unused __attribute__((__unused__))
N
N
N#define compiler_barrier() do { \
N	__asm__ volatile ("dmb"); \
N} while ((0))
X#define compiler_barrier() do { 	__asm__ volatile ("dmb"); } while ((0))
L 19 "..\..\..\..\include\toolchain.h" 2
N
N#endif /* _TOOLCHAIN_H */
L 21 "..\..\..\..\include\kernel.h" 2
N#include <linker/sections.h>
L 1 "..\..\..\..\include\linker/sections.h" 1
N/*
N * Copyright (c) 2013-2014, Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Definitions of various linker Sections.
N *
N * Linker Section declarations used by linker script, C files and Assembly
N * files.
N */
N
N#ifndef _SECTIONS_H
N#define _SECTIONS_H
N
N#define _TEXT_SECTION_NAME text
N#define _RODATA_SECTION_NAME rodata
N#define _CTOR_SECTION_NAME ctors
N/* Linker issue with XIP where the name "data" cannot be used */
N#define _DATA_SECTION_NAME datas
N#define _BSS_SECTION_NAME bss
N#define _NOINIT_SECTION_NAME noinit
N
N#define _APP_DATA_SECTION_NAME		app_datas
N#define _APP_BSS_SECTION_NAME		app_bss
N#define _APP_NOINIT_SECTION_NAME	app_noinit
N
N#define _UNDEFINED_SECTION_NAME undefined
N
N/* Various text section names */
N#define TEXT text
N#if defined(CONFIG_X86)
X#if 0L
S#define TEXT_START text_start /* beginning of TEXT section */
N#else
N#define TEXT_START text /* beginning of TEXT section */
N#endif
N
N/* Various data type section names */
N#define BSS bss
N#define RODATA rodata
N#define DATA data
N#define NOINIT noinit
N#define INIT_ONCE_TEXT init.once.text
N#define INIT_ONCE_DATA init.once.data
N#define DATA_OVERLAY data.overlay
N#define RAMFUNC .ramfunc
N
N/* Interrupts */
N#define IRQ_VECTOR_TABLE	.gnu.linkonce.irq_vector_table
N#define SW_ISR_TABLE		.gnu.linkonce.sw_isr_table
N
N/* Architecture-specific sections */
N
N#include <linker/section_tags.h>
L 1 "..\..\..\..\include\linker/section_tags.h" 1
N/* Macros for tagging symbols and putting them in the correct sections. */
N
N/*
N * Copyright (c) 2013-2014, Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N#ifndef _section_tags__h_
N#define _section_tags__h_
N
N#include <toolchain.h>
N
N#if !defined(_ASMLANGUAGE)
X#if !0L
N
N#define __noinit		__in_section_unique(NOINIT)
N#define __irq_vector_table	_GENERIC_SECTION(IRQ_VECTOR_TABLE)
N#define __sw_isr_table		_GENERIC_SECTION(SW_ISR_TABLE)
N
N#define __init_once_text		__in_section_unique(INIT_ONCE_TEXT)
N#define __init_once_data		__in_section_unique(INIT_ONCE_DATA)
N#define __data_overlay		  __in_section_unique(DATA_OVERLAY)
N
N#ifdef CONFIG_SPI0_XIP
S#define __ramfunc _GENERIC_SECTION(RAMFUNC)
N#else
N#define __ramfunc
N#endif
N
N#define NO_INLINE __attribute__ ( (noinline) )
N
N#endif /* !_ASMLANGUAGE */
N
N#endif /* _section_tags__h_ */
L 57 "..\..\..\..\include\linker/sections.h" 2
N
N#endif /* _SECTIONS_H */
L 22 "..\..\..\..\include\kernel.h" 2
N#include <atomic.h>
L 1 "..\..\..\..\include\atomic.h" 1
N/* atomic operations */
N
N/*
N * Copyright (c) 1997-2015, Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N#ifndef __ATOMIC_H__
N#define __ATOMIC_H__
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
Ntypedef int atomic_t;
Ntypedef atomic_t atomic_val_t;
N
N/**
N * @defgroup atomic_apis Atomic Services APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @brief Atomic compare-and-set.
N *
N * This routine performs an atomic compare-and-set on @a target. If the current
N * value of @a target equals @a old_value, @a target is set to @a new_value.
N * If the current value of @a target does not equal @a old_value, @a target
N * is left unchanged.
N *
N * @param target Address of atomic variable.
N * @param old_value Original value to compare against.
N * @param new_value New value to store.
N * @return 1 if @a new_value is written, 0 otherwise.
N */
N#ifdef CONFIG_ATOMIC_OPERATIONS_BUILTIN
Sstatic inline int atomic_cas(atomic_t *target, atomic_val_t old_value,
S			  atomic_val_t new_value)
S{
S	return __atomic_compare_exchange_n(target, &old_value, new_value,
S					   0, __ATOMIC_SEQ_CST,
S					   __ATOMIC_SEQ_CST);
S}
N#else
Nextern int atomic_cas(atomic_t *target, atomic_val_t old_value,
N		      atomic_val_t new_value);
N#endif
N
N/**
N *
N * @brief Atomic addition.
N *
N * This routine performs an atomic addition on @a target.
N *
N * @param target Address of atomic variable.
N * @param value Value to add.
N *
N * @return Previous value of @a target.
N */
N#ifdef CONFIG_ATOMIC_OPERATIONS_BUILTIN
Sstatic inline atomic_val_t atomic_add(atomic_t *target, atomic_val_t value)
S{
S	return __atomic_fetch_add(target, value, __ATOMIC_SEQ_CST);
S}
N#else
Nextern atomic_val_t atomic_add(atomic_t *target, atomic_val_t value);
N#endif
N
N/**
N *
N * @brief Atomic subtraction.
N *
N * This routine performs an atomic subtraction on @a target.
N *
N * @param target Address of atomic variable.
N * @param value Value to subtract.
N *
N * @return Previous value of @a target.
N */
N#ifdef CONFIG_ATOMIC_OPERATIONS_BUILTIN
Sstatic inline atomic_val_t atomic_sub(atomic_t *target, atomic_val_t value)
S{
S	return __atomic_fetch_sub(target, value, __ATOMIC_SEQ_CST);
S}
N#else
Nextern atomic_val_t atomic_sub(atomic_t *target, atomic_val_t value);
N#endif
N
N/**
N *
N * @brief Atomic increment.
N *
N * This routine performs an atomic increment by 1 on @a target.
N *
N * @param target Address of atomic variable.
N *
N * @return Previous value of @a target.
N */
N#ifdef CONFIG_ATOMIC_OPERATIONS_BUILTIN
Sstatic inline atomic_val_t atomic_inc(atomic_t *target)
S{
S	return atomic_add(target, 1);
S}
N#else
Nextern atomic_val_t atomic_inc(atomic_t *target);
N#endif
N
N/**
N *
N * @brief Atomic decrement.
N *
N * This routine performs an atomic decrement by 1 on @a target.
N *
N * @param target Address of atomic variable.
N *
N * @return Previous value of @a target.
N */
N#ifdef CONFIG_ATOMIC_OPERATIONS_BUILTIN
Sstatic inline atomic_val_t atomic_dec(atomic_t *target)
S{
S	return atomic_sub(target, 1);
S}
N#else
Nextern atomic_val_t atomic_dec(atomic_t *target);
N#endif
N
N/**
N *
N * @brief Atomic get.
N *
N * This routine performs an atomic read on @a target.
N *
N * @param target Address of atomic variable.
N *
N * @return Value of @a target.
N */
N#ifdef CONFIG_ATOMIC_OPERATIONS_BUILTIN
Sstatic inline atomic_val_t atomic_get(const atomic_t *target)
S{
S	return __atomic_load_n(target, __ATOMIC_SEQ_CST);
S}
N#else
Nextern atomic_val_t atomic_get(const atomic_t *target);
N#endif
N
N/**
N *
N * @brief Atomic get-and-set.
N *
N * This routine atomically sets @a target to @a value and returns
N * the previous value of @a target.
N *
N * @param target Address of atomic variable.
N * @param value Value to write to @a target.
N *
N * @return Previous value of @a target.
N */
N#ifdef CONFIG_ATOMIC_OPERATIONS_BUILTIN
Sstatic inline atomic_val_t atomic_set(atomic_t *target, atomic_val_t value)
S{
S	/* This builtin, as described by Intel, is not a traditional
S	 * test-and-set operation, but rather an atomic exchange operation. It
S	 * writes value into *ptr, and returns the previous contents of *ptr.
S	 */
S	return __atomic_exchange_n(target, value, __ATOMIC_SEQ_CST);
S}
N#else
Nextern atomic_val_t atomic_set(atomic_t *target, atomic_val_t value);
N#endif
N
N/**
N *
N * @brief Atomic clear.
N *
N * This routine atomically sets @a target to zero and returns its previous
N * value. (Hence, it is equivalent to atomic_set(target, 0).)
N *
N * @param target Address of atomic variable.
N *
N * @return Previous value of @a target.
N */
N#ifdef CONFIG_ATOMIC_OPERATIONS_BUILTIN
Sstatic inline atomic_val_t atomic_clear(atomic_t *target)
S{
S	return atomic_set(target, 0);
S}
N#else
Nextern atomic_val_t atomic_clear(atomic_t *target);
N#endif
N
N/**
N *
N * @brief Atomic bitwise inclusive OR.
N *
N * This routine atomically sets @a target to the bitwise inclusive OR of
N * @a target and @a value.
N *
N * @param target Address of atomic variable.
N * @param value Value to OR.
N *
N * @return Previous value of @a target.
N */
N#ifdef CONFIG_ATOMIC_OPERATIONS_BUILTIN
Sstatic inline atomic_val_t atomic_or(atomic_t *target, atomic_val_t value)
S{
S	return __atomic_fetch_or(target, value, __ATOMIC_SEQ_CST);
S}
N#else
Nextern atomic_val_t atomic_or(atomic_t *target, atomic_val_t value);
N#endif
N
N/**
N *
N * @brief Atomic bitwise exclusive OR (XOR).
N *
N * This routine atomically sets @a target to the bitwise exclusive OR (XOR) of
N * @a target and @a value.
N *
N * @param target Address of atomic variable.
N * @param value Value to XOR
N *
N * @return Previous value of @a target.
N */
N#ifdef CONFIG_ATOMIC_OPERATIONS_BUILTIN
Sstatic inline atomic_val_t atomic_xor(atomic_t *target, atomic_val_t value)
S{
S	return __atomic_fetch_xor(target, value, __ATOMIC_SEQ_CST);
S}
N#else
Nextern atomic_val_t atomic_xor(atomic_t *target, atomic_val_t value);
N#endif
N
N/**
N *
N * @brief Atomic bitwise AND.
N *
N * This routine atomically sets @a target to the bitwise AND of @a target
N * and @a value.
N *
N * @param target Address of atomic variable.
N * @param value Value to AND.
N *
N * @return Previous value of @a target.
N */
N#ifdef CONFIG_ATOMIC_OPERATIONS_BUILTIN
Sstatic inline atomic_val_t atomic_and(atomic_t *target, atomic_val_t value)
S{
S	return __atomic_fetch_and(target, value, __ATOMIC_SEQ_CST);
S}
N#else
Nextern atomic_val_t atomic_and(atomic_t *target, atomic_val_t value);
N#endif
N
N/**
N *
N * @brief Atomic bitwise NAND.
N *
N * This routine atomically sets @a target to the bitwise NAND of @a target
N * and @a value. (This operation is equivalent to target = ~(target & value).)
N *
N * @param target Address of atomic variable.
N * @param value Value to NAND.
N *
N * @return Previous value of @a target.
N */
N#ifdef CONFIG_ATOMIC_OPERATIONS_BUILTIN
Sstatic inline atomic_val_t atomic_nand(atomic_t *target, atomic_val_t value)
S{
S	return __atomic_fetch_nand(target, value, __ATOMIC_SEQ_CST);
S}
N#else
Nextern atomic_val_t atomic_nand(atomic_t *target, atomic_val_t value);
N#endif
N
N
N/**
N * @brief Initialize an atomic variable.
N *
N * This macro can be used to initialize an atomic variable. For example,
N * @code atomic_t my_var = ATOMIC_INIT(75); @endcode
N *
N * @param i Value to assign to atomic variable.
N */
N#define ATOMIC_INIT(i) (i)
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
N#define ATOMIC_BITS (sizeof(atomic_val_t) * 8)
N#define ATOMIC_MASK(bit) (1 << ((bit) & (ATOMIC_BITS - 1)))
N#define ATOMIC_ELEM(addr, bit) ((addr) + ((bit) / ATOMIC_BITS))
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @brief Define an array of atomic variables.
N *
N * This macro defines an array of atomic variables containing at least
N * @a num_bits bits.
N *
N * @note
N * If used from file scope, the bits of the array are initialized to zero;
N * if used from within a function, the bits are left uninitialized.
N *
N * @param name Name of array of atomic variables.
N * @param num_bits Number of bits needed.
N */
N#define ATOMIC_DEFINE(name, num_bits) \
N	atomic_t name[1 + ((num_bits) - 1) / ATOMIC_BITS]
X#define ATOMIC_DEFINE(name, num_bits) 	atomic_t name[1 + ((num_bits) - 1) / ATOMIC_BITS]
N
N/**
N * @brief Atomically test a bit.
N *
N * This routine tests whether bit number @a bit of @a target is set or not.
N * The target may be a single atomic variable or an array of them.
N *
N * @param target Address of atomic variable or array.
N * @param bit Bit number (starting from 0).
N *
N * @return 1 if the bit was set, 0 if it wasn't.
N */
Nstatic inline int atomic_test_bit(const atomic_t *target, int bit)
N{
N	atomic_val_t val = atomic_get(ATOMIC_ELEM(target, bit));
X	atomic_val_t val = atomic_get(((target) + ((bit) / (sizeof(atomic_val_t) * 8))));
N
N	return (1 & (val >> (bit & (ATOMIC_BITS - 1))));
X	return (1 & (val >> (bit & ((sizeof(atomic_val_t) * 8) - 1))));
N}
N
N/**
N * @brief Atomically test and clear a bit.
N *
N * Atomically clear bit number @a bit of @a target and return its old value.
N * The target may be a single atomic variable or an array of them.
N *
N * @param target Address of atomic variable or array.
N * @param bit Bit number (starting from 0).
N *
N * @return 1 if the bit was set, 0 if it wasn't.
N */
Nstatic inline int atomic_test_and_clear_bit(atomic_t *target, int bit)
N{
N	atomic_val_t mask = ATOMIC_MASK(bit);
X	atomic_val_t mask = (1 << ((bit) & ((sizeof(atomic_val_t) * 8) - 1)));
N	atomic_val_t old;
N
N	old = atomic_and(ATOMIC_ELEM(target, bit), ~mask);
X	old = atomic_and(((target) + ((bit) / (sizeof(atomic_val_t) * 8))), ~mask);
N
N	return (old & mask) != 0;
N}
N
N/**
N * @brief Atomically set a bit.
N *
N * Atomically set bit number @a bit of @a target and return its old value.
N * The target may be a single atomic variable or an array of them.
N *
N * @param target Address of atomic variable or array.
N * @param bit Bit number (starting from 0).
N *
N * @return 1 if the bit was set, 0 if it wasn't.
N */
Nstatic inline int atomic_test_and_set_bit(atomic_t *target, int bit)
N{
N	atomic_val_t mask = ATOMIC_MASK(bit);
X	atomic_val_t mask = (1 << ((bit) & ((sizeof(atomic_val_t) * 8) - 1)));
N	atomic_val_t old;
N
N	old = atomic_or(ATOMIC_ELEM(target, bit), mask);
X	old = atomic_or(((target) + ((bit) / (sizeof(atomic_val_t) * 8))), mask);
N
N	return (old & mask) != 0;
N}
N
N/**
N * @brief Atomically clear a bit.
N *
N * Atomically clear bit number @a bit of @a target.
N * The target may be a single atomic variable or an array of them.
N *
N * @param target Address of atomic variable or array.
N * @param bit Bit number (starting from 0).
N *
N * @return N/A
N */
Nstatic inline void atomic_clear_bit(atomic_t *target, int bit)
N{
N	atomic_val_t mask = ATOMIC_MASK(bit);
X	atomic_val_t mask = (1 << ((bit) & ((sizeof(atomic_val_t) * 8) - 1)));
N
N	atomic_and(ATOMIC_ELEM(target, bit), ~mask);
X	atomic_and(((target) + ((bit) / (sizeof(atomic_val_t) * 8))), ~mask);
N}
N
N/**
N * @brief Atomically set a bit.
N *
N * Atomically set bit number @a bit of @a target.
N * The target may be a single atomic variable or an array of them.
N *
N * @param target Address of atomic variable or array.
N * @param bit Bit number (starting from 0).
N *
N * @return N/A
N */
Nstatic inline void atomic_set_bit(atomic_t *target, int bit)
N{
N	atomic_val_t mask = ATOMIC_MASK(bit);
X	atomic_val_t mask = (1 << ((bit) & ((sizeof(atomic_val_t) * 8) - 1)));
N
N	atomic_or(ATOMIC_ELEM(target, bit), mask);
X	atomic_or(((target) + ((bit) / (sizeof(atomic_val_t) * 8))), mask);
N}
N
N/**
N * @}
N */
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* __ATOMIC_H__ */
L 23 "..\..\..\..\include\kernel.h" 2
N#include <misc/__assert.h>
L 1 "..\..\..\..\include\misc/__assert.h" 1
N/*
N * Copyright (c) 2011-2014 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Debug aid
N *
N *
N * The __ASSERT() macro can be used inside kernel code.
N *
N * Assertions are enabled by setting the __ASSERT_ON symbol to a non-zero value.
N * There are two ways to do this:
N *   a) Use the ASSERT and ASSERT_LEVEL kconfig options
N *   b) Add "CFLAGS += -D__ASSERT_ON=<level>" at the end of a project's Makefile
N * The Makefile method takes precedence over the kconfig option if both are
N * used.
N *
N * Specifying an assertion level of 1 causes the compiler to issue warnings that
N * the kernel contains debug-type __ASSERT() statements; this reminder is issued
N * since assertion code is not normally present in a final product. Specifying
N * assertion level 2 suppresses these warnings.
N *
N * The __ASSERT_EVAL() macro can also be used inside kernel code.
N *
N * It makes use of the __ASSERT() macro, but has some extra flexibility.  It
N * allows the developer to specify different actions depending whether the
N * __ASSERT() macro is enabled or not.  This can be particularly useful to
N * prevent the compiler from generating comments (errors, warnings or remarks)
N * about variables that are only used with __ASSERT() being assigned a value,
N * but otherwise unused when the __ASSERT() macro is disabled.
N *
N * Consider the following example:
N *
N * int  x;
N *
N * x = foo ();
N * __ASSERT (x != 0, "foo() returned zero!");
N *
N * If __ASSERT() is disabled, then 'x' is assigned a value, but never used.
N * This type of situation can be resolved using the __ASSERT_EVAL() macro.
N *
N * __ASSERT_EVAL ((void) foo(),
N *		  int x = foo(),
N *                x != 0,
N *                "foo() returned zero!");
N *
N * The first parameter tells __ASSERT_EVAL() what to do if __ASSERT() is
N * disabled.  The second parameter tells __ASSERT_EVAL() what to do if
N * __ASSERT() is enabled.  The third and fourth parameters are the parameters
N * it passes to __ASSERT().
N *
N * The __ASSERT_NO_MSG() macro can be used to perform an assertion that reports
N * the failed test and its location, but lacks additional debugging information
N * provided to assist the user in diagnosing the problem; its use is
N * discouraged.
N */
N
N#ifndef ___ASSERT__H_
N#define ___ASSERT__H_
N
N#ifdef CONFIG_ASSERT
S#ifndef __ASSERT_ON
S#define __ASSERT_ON CONFIG_ASSERT_LEVEL
S#endif
N#endif
N
N#ifdef __ASSERT_ON
S#if (__ASSERT_ON < 0) || (__ASSERT_ON > 2)
S#error "Invalid __ASSERT() level: must be between 0 and 2"
S#endif
S
S#if __ASSERT_ON
S#include <misc/printk.h>
S#define __ASSERT(test, fmt, ...)                                   \
S	do {                                                       \
S		if (!(test)) {                                     \
S			printk("ASSERTION FAIL [%s] @ %s:%d:\n\t", \
S			       _STRINGIFY(test),                   \
S			       __FILE__,                           \
S			       __LINE__);                          \
S			printk(fmt, ##__VA_ARGS__);                \
S			for (;;)                                   \
S				; /* spin thread */                \
S		}                                                  \
S	} while ((0))
X#define __ASSERT(test, fmt, ...)                                   	do {                                                       		if (!(test)) {                                     			printk("ASSERTION FAIL [%s] @ %s:%d:\n\t", 			       _STRINGIFY(test),                   			       __FILE__,                           			       __LINE__);                          			printk(fmt, ##__VA_ARGS__);                			for (;;)                                   				;                  		}                                                  	} while ((0))
S
S#define __ASSERT_EVAL(expr1, expr2, test, fmt, ...)                \
S	do {                                                       \
S		expr2;                                             \
S		__ASSERT(test, fmt, ##__VA_ARGS__);                \
S	} while (0)
X#define __ASSERT_EVAL(expr1, expr2, test, fmt, ...)                	do {                                                       		expr2;                                             		__ASSERT(test, fmt, ##__VA_ARGS__);                	} while (0)
S
S#if (__ASSERT_ON == 1)
S#warning "__ASSERT() statements are ENABLED"
S#endif
S#else
S#define __ASSERT(test, fmt, ...) \
S	do {/* nothing */        \
S	} while ((0))
X#define __ASSERT(test, fmt, ...) 	do {         	} while ((0))
S#define __ASSERT_EVAL(expr1, expr2, test, fmt, ...) expr1
S#endif
N#else
N#define __ASSERT(test, fmt, ...) \
N	do {/* nothing */        \
N	} while ((0))
X#define __ASSERT(test, fmt, ...) 	do {         	} while ((0))
N#define __ASSERT_EVAL(expr1, expr2, test, fmt, ...) expr1
N#endif
N
N#define __ASSERT_NO_MSG(test) __ASSERT(test, "")
N
N#endif /* ___ASSERT__H_ */
L 24 "..\..\..\..\include\kernel.h" 2
N#include <misc/dlist.h>
L 1 "..\..\..\..\include\misc/dlist.h" 1
N/*
N * Copyright (c) 2013-2015 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Doubly-linked list inline implementation
N *
N * Doubly-linked list implementation.
N *
N * The lists are expected to be initialized such that both the head and tail
N * pointers point to the list itself.  Initializing the lists in such a fashion
N * simplifies the adding and removing of nodes to/from the list.
N */
N
N#ifndef _misc_dlist__h_
N#define _misc_dlist__h_
N
N#include <stddef.h>
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
Nstruct _dnode {
N	union {
N		struct _dnode *head; /* ptr to head of list (sys_dlist_t) */
N		struct _dnode *next; /* ptr to next node    (sys_dnode_t) */
N	};
N	union {
N		struct _dnode *tail; /* ptr to tail of list (sys_dlist_t) */
N		struct _dnode *prev; /* ptr to previous node (sys_dnode_t) */
N	};
N};
N
Ntypedef struct _dnode sys_dlist_t;
Ntypedef struct _dnode sys_dnode_t;
N
N/**
N * @brief Provide the primitive to iterate on a list
N * Note: the loop is unsafe and thus __dn should not be removed
N *
N * User _MUST_ add the loop statement curly braces enclosing its own code:
N *
N *     SYS_DLIST_FOR_EACH_NODE(l, n) {
N *         <user code>
N *     }
N *
N * @param __dl A pointer on a sys_dlist_t to iterate on
N * @param __dn A sys_dnode_t pointer to peek each node of the list
N */
N#define SYS_DLIST_FOR_EACH_NODE(__dl, __dn)				\
N	for (__dn = sys_dlist_peek_head(__dl); __dn;			\
N	     __dn = sys_dlist_peek_next(__dl, __dn))
X#define SYS_DLIST_FOR_EACH_NODE(__dl, __dn)					for (__dn = sys_dlist_peek_head(__dl); __dn;				     __dn = sys_dlist_peek_next(__dl, __dn))
N
N/**
N * @brief Provide the primitive to iterate on a list, from a node in the list
N * Note: the loop is unsafe and thus __dn should not be removed
N *
N * User _MUST_ add the loop statement curly braces enclosing its own code:
N *
N *     SYS_DLIST_ITERATE_FROM_NODE(l, n) {
N *         <user code>
N *     }
N *
N * Like SYS_DLIST_FOR_EACH_NODE(), but __dn already contains a node in the list
N * where to start searching for the next entry from. If NULL, it starts from
N * the head.
N *
N * @param __dl A pointer on a sys_dlist_t to iterate on
N * @param __dn A sys_dnode_t pointer to peek each node of the list;
N *             it contains the starting node, or NULL to start from the head
N */
N#define SYS_DLIST_ITERATE_FROM_NODE(__dl, __dn) \
N	for (__dn = __dn ? sys_dlist_peek_next_no_check(__dl, __dn) \
N			 : sys_dlist_peek_head(__dl); \
N	     __dn; \
N	     __dn = sys_dlist_peek_next(__dl, __dn))
X#define SYS_DLIST_ITERATE_FROM_NODE(__dl, __dn) 	for (__dn = __dn ? sys_dlist_peek_next_no_check(__dl, __dn) 			 : sys_dlist_peek_head(__dl); 	     __dn; 	     __dn = sys_dlist_peek_next(__dl, __dn))
N
N/**
N * @brief Provide the primitive to safely iterate on a list
N * Note: __dn can be removed, it will not break the loop.
N *
N * User _MUST_ add the loop statement curly braces enclosing its own code:
N *
N *     SYS_DLIST_FOR_EACH_NODE_SAFE(l, n, s) {
N *         <user code>
N *     }
N *
N * @param __dl A pointer on a sys_dlist_t to iterate on
N * @param __dn A sys_dnode_t pointer to peek each node of the list
N * @param __dns A sys_dnode_t pointer for the loop to run safely
N */
N#define SYS_DLIST_FOR_EACH_NODE_SAFE(__dl, __dn, __dns)			\
N	for (__dn = sys_dlist_peek_head(__dl),				\
N		     __dns = sys_dlist_peek_next(__dl, __dn);  		\
N	     __dn; __dn = __dns,					\
N		     __dns = sys_dlist_peek_next(__dl, __dn))
X#define SYS_DLIST_FOR_EACH_NODE_SAFE(__dl, __dn, __dns)				for (__dn = sys_dlist_peek_head(__dl),						     __dns = sys_dlist_peek_next(__dl, __dn);  			     __dn; __dn = __dns,							     __dns = sys_dlist_peek_next(__dl, __dn))
N
N/*
N * @brief Provide the primitive to resolve the container of a list node
N * Note: it is safe to use with NULL pointer nodes
N *
N * @param __dn A pointer on a sys_dnode_t to get its container
N * @param __cn Container struct type pointer
N * @param __n The field name of sys_dnode_t within the container struct
N */
N#define SYS_DLIST_CONTAINER(__dn, __cn, __n) \
N	(__dn ? CONTAINER_OF(__dn, __typeof__(*__cn), __n) : NULL)
X#define SYS_DLIST_CONTAINER(__dn, __cn, __n) 	(__dn ? CONTAINER_OF(__dn, __typeof__(*__cn), __n) : NULL)
N/*
N * @brief Provide the primitive to peek container of the list head
N *
N * @param __dl A pointer on a sys_dlist_t to peek
N * @param __cn Container struct type pointer
N * @param __n The field name of sys_dnode_t within the container struct
N */
N#define SYS_DLIST_PEEK_HEAD_CONTAINER(__dl, __cn, __n) \
N	SYS_DLIST_CONTAINER(sys_dlist_peek_head(__dl), __cn, __n)
X#define SYS_DLIST_PEEK_HEAD_CONTAINER(__dl, __cn, __n) 	SYS_DLIST_CONTAINER(sys_dlist_peek_head(__dl), __cn, __n)
N
N/*
N * @brief Provide the primitive to peek the next container
N *
N * @param __dl A pointer on a sys_dlist_t to peek
N * @param __cn Container struct type pointer
N * @param __n The field name of sys_dnode_t within the container struct
N */
N#define SYS_DLIST_PEEK_NEXT_CONTAINER(__dl, __cn, __n) \
N	((__cn) ? SYS_DLIST_CONTAINER(sys_dlist_peek_next(__dl, &(__cn->__n)), \
N				      __cn, __n) : NULL)
X#define SYS_DLIST_PEEK_NEXT_CONTAINER(__dl, __cn, __n) 	((__cn) ? SYS_DLIST_CONTAINER(sys_dlist_peek_next(__dl, &(__cn->__n)), 				      __cn, __n) : NULL)
N
N/**
N * @brief Provide the primitive to iterate on a list under a container
N * Note: the loop is unsafe and thus __cn should not be detached
N *
N * User _MUST_ add the loop statement curly braces enclosing its own code:
N *
N *     SYS_DLIST_FOR_EACH_CONTAINER(l, c, n) {
N *         <user code>
N *     }
N *
N * @param __dl A pointer on a sys_dlist_t to iterate on
N * @param __cn A pointer to peek each entry of the list
N * @param __n The field name of sys_dnode_t within the container struct
N */
N#define SYS_DLIST_FOR_EACH_CONTAINER(__dl, __cn, __n)			\
N	for (__cn = SYS_DLIST_PEEK_HEAD_CONTAINER(__dl, __cn, __n); __cn; \
N	     __cn = SYS_DLIST_PEEK_NEXT_CONTAINER(__dl, __cn, __n))
X#define SYS_DLIST_FOR_EACH_CONTAINER(__dl, __cn, __n)				for (__cn = SYS_DLIST_PEEK_HEAD_CONTAINER(__dl, __cn, __n); __cn; 	     __cn = SYS_DLIST_PEEK_NEXT_CONTAINER(__dl, __cn, __n))
N
N/**
N * @brief Provide the primitive to safely iterate on a list under a container
N * Note: __cn can be detached, it will not break the loop.
N *
N * User _MUST_ add the loop statement curly braces enclosing its own code:
N *
N *     SYS_DLIST_FOR_EACH_CONTAINER_SAFE(l, c, cn, n) {
N *         <user code>
N *     }
N *
N * @param __dl A pointer on a sys_dlist_t to iterate on
N * @param __cn A pointer to peek each entry of the list
N * @param __cns A pointer for the loop to run safely
N * @param __n The field name of sys_dnode_t within the container struct
N */
N#define SYS_DLIST_FOR_EACH_CONTAINER_SAFE(__dl, __cn, __cns, __n)	\
N	for (__cn = SYS_DLIST_PEEK_HEAD_CONTAINER(__dl, __cn, __n),	\
N	     __cns = SYS_DLIST_PEEK_NEXT_CONTAINER(__dl, __cn, __n); __cn; \
N	     __cn = __cns,						\
N	     __cns = SYS_DLIST_PEEK_NEXT_CONTAINER(__dl, __cn, __n))
X#define SYS_DLIST_FOR_EACH_CONTAINER_SAFE(__dl, __cn, __cns, __n)		for (__cn = SYS_DLIST_PEEK_HEAD_CONTAINER(__dl, __cn, __n),		     __cns = SYS_DLIST_PEEK_NEXT_CONTAINER(__dl, __cn, __n); __cn; 	     __cn = __cns,							     __cns = SYS_DLIST_PEEK_NEXT_CONTAINER(__dl, __cn, __n))
N
N/**
N * @brief initialize list
N *
N * @param list the doubly-linked list
N *
N * @return N/A
N */
N
Nstatic inline void sys_dlist_init(sys_dlist_t *list)
N{
N	list->head = (sys_dnode_t *)list;
N	list->tail = (sys_dnode_t *)list;
N}
N
N#define SYS_DLIST_STATIC_INIT(ptr_to_list) {{(ptr_to_list)}, {(ptr_to_list)}}
N
N/**
N * @brief check if a node is the list's head
N *
N * @param list the doubly-linked list to operate on
N * @param node the node to check
N *
N * @return 1 if node is the head, 0 otherwise
N */
N
Nstatic inline int sys_dlist_is_head(sys_dlist_t *list, sys_dnode_t *node)
N{
N	return list->head == node;
N}
N
N/**
N * @brief check if a node is the list's tail
N *
N * @param list the doubly-linked list to operate on
N * @param node the node to check
N *
N * @return 1 if node is the tail, 0 otherwise
N */
N
Nstatic inline int sys_dlist_is_tail(sys_dlist_t *list, sys_dnode_t *node)
N{
N	return list->tail == node;
N}
N
N/**
N * @brief check if the list is empty
N *
N * @param list the doubly-linked list to operate on
N *
N * @return 1 if empty, 0 otherwise
N */
N
Nstatic inline int sys_dlist_is_empty(sys_dlist_t *list)
N{
N	return list->head == list;
N}
N
N/**
N * @brief check if more than one node present
N *
N * @param list the doubly-linked list to operate on
N *
N * @return 1 if multiple nodes, 0 otherwise
N */
N
Nstatic inline int sys_dlist_has_multiple_nodes(sys_dlist_t *list)
N{
N	return list->head != list->tail;
N}
N
N/**
N * @brief get a reference to the head item in the list
N *
N * @param list the doubly-linked list to operate on
N *
N * @return a pointer to the head element, NULL if list is empty
N */
N
Nstatic inline sys_dnode_t *sys_dlist_peek_head(sys_dlist_t *list)
N{
N	return sys_dlist_is_empty(list) ? NULL : list->head;
X	return sys_dlist_is_empty(list) ? 0 : list->head;
N}
N
N/**
N * @brief get a reference to the head item in the list
N *
N * The list must be known to be non-empty.
N *
N * @param list the doubly-linked list to operate on
N *
N * @return a pointer to the head element
N */
N
Nstatic inline sys_dnode_t *sys_dlist_peek_head_not_empty(sys_dlist_t *list)
N{
N	return list->head;
N}
N
N/**
N * @brief get a reference to the next item in the list, node is not NULL
N *
N * Faster than sys_dlist_peek_next() if node is known not to be NULL.
N *
N * @param list the doubly-linked list to operate on
N * @param node the node from which to get the next element in the list
N *
N * @return a pointer to the next element from a node, NULL if node is the tail
N */
N
Nstatic inline sys_dnode_t *sys_dlist_peek_next_no_check(sys_dlist_t *list,
N							sys_dnode_t *node)
N{
N	return (node == list->tail) ? NULL : node->next;
X	return (node == list->tail) ? 0 : node->next;
N}
N
N/**
N * @brief get a reference to the next item in the list
N *
N * @param list the doubly-linked list to operate on
N * @param node the node from which to get the next element in the list
N *
N * @return a pointer to the next element from a node, NULL if node is the tail
N * or NULL (when node comes from reading the head of an empty list).
N */
N
Nstatic inline sys_dnode_t *sys_dlist_peek_next(sys_dlist_t *list,
N					       sys_dnode_t *node)
N{
N	return node ? sys_dlist_peek_next_no_check(list, node) : NULL;
X	return node ? sys_dlist_peek_next_no_check(list, node) : 0;
N}
N
N/**
N * @brief get a reference to the tail item in the list
N *
N * @param list the doubly-linked list to operate on
N *
N * @return a pointer to the tail element, NULL if list is empty
N */
N
Nstatic inline sys_dnode_t *sys_dlist_peek_tail(sys_dlist_t *list)
N{
N	return sys_dlist_is_empty(list) ? NULL : list->tail;
X	return sys_dlist_is_empty(list) ? 0 : list->tail;
N}
N
N/**
N * @brief add node to tail of list
N *
N * @param list the doubly-linked list to operate on
N * @param node the element to append
N *
N * @return N/A
N */
N
Nstatic inline void sys_dlist_append(sys_dlist_t *list, sys_dnode_t *node)
N{
N	node->next = list;
N	node->prev = list->tail;
N
N	list->tail->next = node;
N	list->tail = node;
N}
N
N/**
N * @brief add node to head of list
N *
N * @param list the doubly-linked list to operate on
N * @param node the element to append
N *
N * @return N/A
N */
N
Nstatic inline void sys_dlist_prepend(sys_dlist_t *list, sys_dnode_t *node)
N{
N	node->next = list->head;
N	node->prev = list;
N
N	list->head->prev = node;
N	list->head = node;
N}
N
N/**
N * @brief insert node after a node
N *
N * Insert a node after a specified node in a list.
N *
N * @param list the doubly-linked list to operate on
N * @param insert_point the insert point in the list: if NULL, insert at head
N * @param node the element to append
N *
N * @return N/A
N */
N
Nstatic inline void sys_dlist_insert_after(sys_dlist_t *list,
N	sys_dnode_t *insert_point, sys_dnode_t *node)
N{
N	if (!insert_point) {
N		sys_dlist_prepend(list, node);
N	} else {
N		node->next = insert_point->next;
N		node->prev = insert_point;
N		insert_point->next->prev = node;
N		insert_point->next = node;
N	}
N}
N
N/**
N * @brief insert node before a node
N *
N * Insert a node before a specified node in a list.
N *
N * @param list the doubly-linked list to operate on
N * @param insert_point the insert point in the list: if NULL, insert at tail
N * @param node the element to insert
N *
N * @return N/A
N */
N
Nstatic inline void sys_dlist_insert_before(sys_dlist_t *list,
N	sys_dnode_t *insert_point, sys_dnode_t *node)
N{
N	if (!insert_point) {
N		sys_dlist_append(list, node);
N	} else {
N		node->prev = insert_point->prev;
N		node->next = insert_point;
N		insert_point->prev->next = node;
N		insert_point->prev = node;
N	}
N}
N
N/**
N * @brief insert node at position
N *
N * Insert a node in a location depending on a external condition. The cond()
N * function checks if the node is to be inserted _before_ the current node
N * against which it is checked.
N *
N * @param list the doubly-linked list to operate on
N * @param node the element to insert
N * @param cond a function that determines if the current node is the correct
N *             insert point
N * @param data parameter to cond()
N *
N * @return N/A
N */
N
Nstatic inline void sys_dlist_insert_at(sys_dlist_t *list, sys_dnode_t *node,
N	int (*cond)(sys_dnode_t *, void *), void *data)
N{
N	if (sys_dlist_is_empty(list)) {
N		sys_dlist_append(list, node);
N	} else {
N		sys_dnode_t *pos = sys_dlist_peek_head(list);
N
N		while (pos && !cond(pos, data)) {
N			pos = sys_dlist_peek_next(list, pos);
N		}
N		sys_dlist_insert_before(list, pos, node);
N	}
N}
N
N/**
N * @brief remove a specific node from a list
N *
N * The list is implicit from the node. The node must be part of a list.
N *
N * @param node the node to remove
N *
N * @return N/A
N */
N
Nstatic inline void sys_dlist_remove(sys_dnode_t *node)
N{
N	node->prev->next = node->next;
N	node->next->prev = node->prev;
N}
N
N/**
N * @brief get the first node in a list
N *
N * @param list the doubly-linked list to operate on
N *
N * @return the first node in the list, NULL if list is empty
N */
N
Nstatic inline sys_dnode_t *sys_dlist_get(sys_dlist_t *list)
N{
N	sys_dnode_t *node;
N
N	if (sys_dlist_is_empty(list)) {
N		return NULL;
X		return 0;
N	}
N
N	node = list->head;
N	sys_dlist_remove(node);
N	return node;
N}
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* _misc_dlist__h_ */
L 25 "..\..\..\..\include\kernel.h" 2
N#include <misc/slist.h>
L 1 "..\..\..\..\include\misc/slist.h" 1
N/*
N * Copyright (c) 2016 Intel Corporation
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N *
N * @brief Header where single linked list utility code is found
N */
N
N#ifndef __SLIST_H__
N#define __SLIST_H__
N
N#include <stddef.h>
N#include <stdbool.h>
L 1 "F:\Keil_v5\ARM\ARMCC\Bin\..\include\stdbool.h" 1
N/* stdbool.h: ISO/IEC 9899:1999 (C99), section 7.16 */
N
N/* Copyright (C) ARM Ltd., 2002
N * All rights reserved
N * RCS $Revision$
N * Checkin $Date$
N * Revising $Author: drodgman $
N */
N
N#ifndef __bool_true_false_are_defined
N#define __bool_true_false_are_defined 1
N#define __ARMCLIB_VERSION 5060034
N
N  #ifndef __cplusplus /* In C++, 'bool', 'true' and 'false' and keywords */
N    #define bool _Bool
N    #define true 1
N    #define false 0
N  #else
S    #ifdef __GNUC__
S      /* GNU C++ supports direct inclusion of stdbool.h to provide C99
S         compatibility by defining _Bool */
S      #define _Bool bool
S    #endif
N  #endif
N
N#endif /* __bool_true_false_are_defined */
N
L 18 "..\..\..\..\include\misc/slist.h" 2
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N
Nstruct _snode {
N	struct _snode *next;
N};
N
Ntypedef struct _snode sys_snode_t;
N
Nstruct _slist {
N	sys_snode_t *head;
N	sys_snode_t *tail;
N};
N
Ntypedef struct _slist sys_slist_t;
N
N/**
N * @brief Provide the primitive to iterate on a list
N * Note: the loop is unsafe and thus __sn should not be removed
N *
N * User _MUST_ add the loop statement curly braces enclosing its own code:
N *
N *     SYS_SLIST_FOR_EACH_NODE(l, n) {
N *         <user code>
N *     }
N *
N * @param __sl A pointer on a sys_slist_t to iterate on
N * @param __sn A sys_snode_t pointer to peek each node of the list
N */
N#define SYS_SLIST_FOR_EACH_NODE(__sl, __sn)				\
N	for (__sn = sys_slist_peek_head(__sl); __sn;			\
N	     __sn = sys_slist_peek_next(__sn))
X#define SYS_SLIST_FOR_EACH_NODE(__sl, __sn)					for (__sn = sys_slist_peek_head(__sl); __sn;				     __sn = sys_slist_peek_next(__sn))
N
N/**
N * @brief Provide the primitive to iterate on a list, from a node in the list
N * Note: the loop is unsafe and thus __sn should not be removed
N *
N * User _MUST_ add the loop statement curly braces enclosing its own code:
N *
N *     SYS_SLIST_ITERATE_FROM_NODE(l, n) {
N *         <user code>
N *     }
N *
N * Like SYS_SLIST_FOR_EACH_NODE(), but __dn already contains a node in the list
N * where to start searching for the next entry from. If NULL, it starts from
N * the head.
N *
N * @param __sl A pointer on a sys_slist_t to iterate on
N * @param __sn A sys_snode_t pointer to peek each node of the list
N *             it contains the starting node, or NULL to start from the head
N */
N#define SYS_SLIST_ITERATE_FROM_NODE(__sl, __sn)				\
N	for (__sn = __sn ? sys_slist_peek_next_no_check(__sn)		\
N			 : sys_slist_peek_head(__sl);			\
N	     __sn;							\
N	     __sn = sys_slist_peek_next(__sn))
X#define SYS_SLIST_ITERATE_FROM_NODE(__sl, __sn)					for (__sn = __sn ? sys_slist_peek_next_no_check(__sn)					 : sys_slist_peek_head(__sl);				     __sn;								     __sn = sys_slist_peek_next(__sn))
N
N/**
N * @brief Provide the primitive to safely iterate on a list
N * Note: __sn can be removed, it will not break the loop.
N *
N * User _MUST_ add the loop statement curly braces enclosing its own code:
N *
N *     SYS_SLIST_FOR_EACH_NODE_SAFE(l, n, s) {
N *         <user code>
N *     }
N *
N * @param __sl A pointer on a sys_slist_t to iterate on
N * @param __sn A sys_snode_t pointer to peek each node of the list
N * @param __sns A sys_snode_t pointer for the loop to run safely
N */
N#define SYS_SLIST_FOR_EACH_NODE_SAFE(__sl, __sn, __sns)			\
N	for (__sn = sys_slist_peek_head(__sl),				\
N		     __sns = sys_slist_peek_next(__sn);			\
N	     __sn; __sn = __sns,					\
N		     __sns = sys_slist_peek_next(__sn))
X#define SYS_SLIST_FOR_EACH_NODE_SAFE(__sl, __sn, __sns)				for (__sn = sys_slist_peek_head(__sl),						     __sns = sys_slist_peek_next(__sn);				     __sn; __sn = __sns,							     __sns = sys_slist_peek_next(__sn))
N
N/*
N * @brief Provide the primitive to resolve the container of a list node
N * Note: it is safe to use with NULL pointer nodes
N *
N * @param __ln A pointer on a sys_node_t to get its container
N * @param __cn Container struct type pointer
N * @param __n The field name of sys_node_t within the container struct
N */
N#define SYS_SLIST_CONTAINER(__ln, __cn, __n) \
N	((__ln) ? CONTAINER_OF((__ln), __typeof__(*(__cn)), __n) : NULL)
X#define SYS_SLIST_CONTAINER(__ln, __cn, __n) 	((__ln) ? CONTAINER_OF((__ln), __typeof__(*(__cn)), __n) : NULL)
N/*
N * @brief Provide the primitive to peek container of the list head
N *
N * @param __sl A pointer on a sys_slist_t to peek
N * @param __cn Container struct type pointer
N * @param __n The field name of sys_node_t within the container struct
N */
N#define SYS_SLIST_PEEK_HEAD_CONTAINER(__sl, __cn, __n) \
N	SYS_SLIST_CONTAINER(sys_slist_peek_head(__sl), __cn, __n)
X#define SYS_SLIST_PEEK_HEAD_CONTAINER(__sl, __cn, __n) 	SYS_SLIST_CONTAINER(sys_slist_peek_head(__sl), __cn, __n)
N
N/*
N * @brief Provide the primitive to peek container of the list tail
N *
N * @param __sl A pointer on a sys_slist_t to peek
N * @param __cn Container struct type pointer
N * @param __n The field name of sys_node_t within the container struct
N */
N#define SYS_SLIST_PEEK_TAIL_CONTAINER(__sl, __cn, __n) \
N	SYS_SLIST_CONTAINER(sys_slist_peek_tail(__sl), __cn, __n)
X#define SYS_SLIST_PEEK_TAIL_CONTAINER(__sl, __cn, __n) 	SYS_SLIST_CONTAINER(sys_slist_peek_tail(__sl), __cn, __n)
N
N/*
N * @brief Provide the primitive to peek the next container
N *
N * @param __cn Container struct type pointer
N * @param __n The field name of sys_node_t within the container struct
N */
N
N#define SYS_SLIST_PEEK_NEXT_CONTAINER(__cn, __n) \
N	((__cn) ? SYS_SLIST_CONTAINER(sys_slist_peek_next(&((__cn)->__n)), \
N				      __cn, __n) : NULL)
X#define SYS_SLIST_PEEK_NEXT_CONTAINER(__cn, __n) 	((__cn) ? SYS_SLIST_CONTAINER(sys_slist_peek_next(&((__cn)->__n)), 				      __cn, __n) : NULL)
N
N/**
N * @brief Provide the primitive to iterate on a list under a container
N * Note: the loop is unsafe and thus __cn should not be detached
N *
N * User _MUST_ add the loop statement curly braces enclosing its own code:
N *
N *     SYS_SLIST_FOR_EACH_CONTAINER(l, c, n) {
N *         <user code>
N *     }
N *
N * @param __sl A pointer on a sys_slist_t to iterate on
N * @param __cn A pointer to peek each entry of the list
N * @param __n The field name of sys_node_t within the container struct
N */
N#define SYS_SLIST_FOR_EACH_CONTAINER(__sl, __cn, __n)			\
N	for (__cn = SYS_SLIST_PEEK_HEAD_CONTAINER(__sl, __cn, __n); __cn; \
N	     __cn = SYS_SLIST_PEEK_NEXT_CONTAINER(__cn, __n))
X#define SYS_SLIST_FOR_EACH_CONTAINER(__sl, __cn, __n)				for (__cn = SYS_SLIST_PEEK_HEAD_CONTAINER(__sl, __cn, __n); __cn; 	     __cn = SYS_SLIST_PEEK_NEXT_CONTAINER(__cn, __n))
N
N/**
N * @brief Provide the primitive to safely iterate on a list under a container
N * Note: __cn can be detached, it will not break the loop.
N *
N * User _MUST_ add the loop statement curly braces enclosing its own code:
N *
N *     SYS_SLIST_FOR_EACH_NODE_SAFE(l, c, cn, n) {
N *         <user code>
N *     }
N *
N * @param __sl A pointer on a sys_slist_t to iterate on
N * @param __cn A pointer to peek each entry of the list
N * @param __cns A pointer for the loop to run safely
N * @param __n The field name of sys_node_t within the container struct
N */
N#define SYS_SLIST_FOR_EACH_CONTAINER_SAFE(__sl, __cn, __cns, __n)	\
N	for (__cn = SYS_SLIST_PEEK_HEAD_CONTAINER(__sl, __cn, __n),	\
N	     __cns = SYS_SLIST_PEEK_NEXT_CONTAINER(__cn, __n); __cn;	\
N	     __cn = __cns, __cns = SYS_SLIST_PEEK_NEXT_CONTAINER(__cn, __n))
X#define SYS_SLIST_FOR_EACH_CONTAINER_SAFE(__sl, __cn, __cns, __n)		for (__cn = SYS_SLIST_PEEK_HEAD_CONTAINER(__sl, __cn, __n),		     __cns = SYS_SLIST_PEEK_NEXT_CONTAINER(__cn, __n); __cn;		     __cn = __cns, __cns = SYS_SLIST_PEEK_NEXT_CONTAINER(__cn, __n))
N
N/**
N * @brief Initialize a list
N *
N * @param list A pointer on the list to initialize
N */
Nstatic inline void sys_slist_init(sys_slist_t *list)
N{
N	list->head = NULL;
X	list->head = 0;
N	list->tail = NULL;
X	list->tail = 0;
N}
N
N#define SYS_SLIST_STATIC_INIT(ptr_to_list) {NULL, NULL}
N
N/**
N * @brief Test if the given list is empty
N *
N * @param list A pointer on the list to test
N *
N * @return a boolean, true if it's empty, false otherwise
N */
Nstatic inline bool sys_slist_is_empty(sys_slist_t *list)
Xstatic inline _Bool sys_slist_is_empty(sys_slist_t *list)
N{
N	return (!list->head);
N}
N
N/**
N * @brief Peek the first node from the list
N *
N * @param list A point on the list to peek the first node from
N *
N * @return A pointer on the first node of the list (or NULL if none)
N */
Nstatic inline sys_snode_t *sys_slist_peek_head(sys_slist_t *list)
N{
N	return list->head;
N}
N
N/**
N * @brief Peek the last node from the list
N *
N * @param list A point on the list to peek the last node from
N *
N * @return A pointer on the last node of the list (or NULL if none)
N */
Nstatic inline sys_snode_t *sys_slist_peek_tail(sys_slist_t *list)
N{
N	return list->tail;
N}
N
N/**
N * @brief Peek the next node from current node, node is not NULL
N *
N * Faster then sys_slist_peek_next() if node is known not to be NULL.
N *
N * @param node A pointer on the node where to peek the next node
N *
N * @return a pointer on the next node (or NULL if none)
N */
Nstatic inline sys_snode_t *sys_slist_peek_next_no_check(sys_snode_t *node)
N{
N	return node->next;
N}
N
N/**
N * @brief Peek the next node from current node
N *
N * @param node A pointer on the node where to peek the next node
N *
N * @return a pointer on the next node (or NULL if none)
N */
Nstatic inline sys_snode_t *sys_slist_peek_next(sys_snode_t *node)
N{
N	return node ? sys_slist_peek_next_no_check(node) : NULL;
X	return node ? sys_slist_peek_next_no_check(node) : 0;
N}
N
N/**
N * @brief Prepend a node to the given list
N *
N * @param list A pointer on the list to affect
N * @param node A pointer on the node to prepend
N */
Nstatic inline void sys_slist_prepend(sys_slist_t *list,
N				     sys_snode_t *node)
N{
N	node->next = list->head;
N	list->head = node;
N
N	if (!list->tail) {
N		list->tail = list->head;
N	}
N}
N
N/**
N * @brief Append a node to the given list
N *
N * @param list A pointer on the list to affect
N * @param node A pointer on the node to append
N */
Nstatic inline void sys_slist_append(sys_slist_t *list,
N				    sys_snode_t *node)
N{
N	node->next = NULL;
X	node->next = 0;
N
N	if (!list->tail) {
N		list->tail = node;
N		list->head = node;
N	} else {
N		list->tail->next = node;
N		list->tail = node;
N	}
N}
N
N/**
N * @brief Append a list to the given list
N *
N * Append a singly-linked, NULL-terminated list consisting of nodes containing
N * the pointer to the next node as the first element of a node, to @a list.
N *
N * @param list A pointer on the list to affect
N * @param head A pointer to the first element of the list to append
N * @param tail A pointer to the last element of the list to append
N */
Nstatic inline void sys_slist_append_list(sys_slist_t *list,
N					 void *head, void *tail)
N{
N	if (!list->tail) {
N		list->head = (sys_snode_t *)head;
N		list->tail = (sys_snode_t *)tail;
N	} else {
N		list->tail->next = (sys_snode_t *)head;
N		list->tail = (sys_snode_t *)tail;
N	}
N}
N
N/**
N * @brief merge two slists, appending the second one to the first
N *
N * When the operation is completed, the appending list is empty.
N *
N * @param list A pointer on the list to affect
N * @param list_to_append A pointer to the list to append.
N */
Nstatic inline void sys_slist_merge_slist(sys_slist_t *list,
N					 sys_slist_t *list_to_append)
N{
N	sys_slist_append_list(list, list_to_append->head,
N				    list_to_append->tail);
N	sys_slist_init(list_to_append);
N}
N
N/**
N * @brief Insert a node to the given list
N *
N * @param list A pointer on the list to affect
N * @param prev A pointer on the previous node
N * @param node A pointer on the node to insert
N */
Nstatic inline void sys_slist_insert(sys_slist_t *list,
N				    sys_snode_t *prev,
N				    sys_snode_t *node)
N{
N	if (!prev) {
N		sys_slist_prepend(list, node);
N	} else if (!prev->next) {
N		sys_slist_append(list, node);
N	} else {
N		node->next = prev->next;
N		prev->next = node;
N	}
N}
N
N/**
N * @brief Fetch and remove the first node of the given list
N *
N * List must be known to be non-empty.
N *
N * @param list A pointer on the list to affect
N *
N * @return A pointer to the first node of the list
N */
Nstatic inline sys_snode_t *sys_slist_get_not_empty(sys_slist_t *list)
N{
N	sys_snode_t *node = list->head;
N
N	list->head = node->next;
N	if (list->tail == node) {
N		list->tail = list->head;
N	}
N
N	return node;
N}
N
N/**
N * @brief Fetch and remove the first node of the given list
N *
N * @param list A pointer on the list to affect
N *
N * @return A pointer to the first node of the list (or NULL if empty)
N */
Nstatic inline sys_snode_t *sys_slist_get(sys_slist_t *list)
N{
N	return sys_slist_is_empty(list) ? NULL : sys_slist_get_not_empty(list);
X	return sys_slist_is_empty(list) ? 0 : sys_slist_get_not_empty(list);
N}
N
N/**
N * @brief Remove a node
N *
N * @param list A pointer on the list to affect
N * @param prev_node A pointer on the previous node
N *        (can be NULL, which means the node is the list's head)
N * @param node A pointer on the node to remove
N */
Nstatic inline void sys_slist_remove(sys_slist_t *list,
N				    sys_snode_t *prev_node,
N				    sys_snode_t *node)
N{
N	if (!prev_node) {
N		list->head = node->next;
N
N		/* Was node also the tail? */
N		if (list->tail == node) {
N			list->tail = list->head;
N		}
N	} else {
N		prev_node->next = node->next;
N
N		/* Was node the tail? */
N		if (list->tail == node) {
N			list->tail = prev_node;
N		}
N	}
N
N	node->next = NULL;
X	node->next = 0;
N}
N
N/**
N * @brief Find and remove a node from a list
N *
N * @param list A pointer on the list to affect
N * @param node A pointer on the node to remove from the list
N *
N * @return true if node was removed
N */
Nstatic inline bool sys_slist_find_and_remove(sys_slist_t *list,
Xstatic inline _Bool sys_slist_find_and_remove(sys_slist_t *list,
N					     sys_snode_t *node)
N{
N	sys_snode_t *prev = NULL;
X	sys_snode_t *prev = 0;
N	sys_snode_t *test;
N
N	SYS_SLIST_FOR_EACH_NODE(list, test) {
X	for (test = sys_slist_peek_head(list); test; test = sys_slist_peek_next(test)) {
N		if (test == node) {
N			sys_slist_remove(list, prev, node);
N			return true;
X			return 1;
N		}
N
N		prev = test;
N	}
N
N	return false;
X	return 0;
N}
N
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* __SLIST_H__ */
L 26 "..\..\..\..\include\kernel.h" 2
N#include <misc/util.h>
L 1 "..\..\..\..\include\misc/util.h" 1
N/*
N * Copyright (c) 2011-2014, Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Misc utilities
N *
N * Misc utilities usable by the kernel and application code.
N */
N
N#ifndef _UTIL__H_
N#define _UTIL__H_
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N#ifndef _ASMLANGUAGE
N
N#include <zephyr/types.h>
N
N/* Helper to pass a int as a pointer or vice-versa.
N * Those are available for 32 bits architectures:
N */
N#define POINTER_TO_UINT(x) ((u32_t) (x))
N#define UINT_TO_POINTER(x) ((void *) (x))
N#define POINTER_TO_INT(x)  ((s32_t) (x))
N#define INT_TO_POINTER(x)  ((void *) (x))
N
N/* Evaluates to 0 if cond is true-ish; compile error otherwise */
N#define ZERO_OR_COMPILE_ERROR(cond) ((int) sizeof(char[1 - 2 * !(cond)]) - 1)
N
N/* Evaluates to 0 if array is an array; compile error if not array (e.g.
N * pointer)
N */
N#define IS_ARRAY(array) \
N	ZERO_OR_COMPILE_ERROR( \
N		!__builtin_types_compatible_p(__typeof__(array), \
N					      __typeof__(&(array)[0])))
X#define IS_ARRAY(array) 	ZERO_OR_COMPILE_ERROR( 		!__builtin_types_compatible_p(__typeof__(array), 					      __typeof__(&(array)[0])))
N
N/* Evaluates to number of elements in an array; compile error if not
N * an array (e.g. pointer)
N */
N#define ARRAY_SIZE(array) \
N	((unsigned long) (IS_ARRAY(array) + \
N		(sizeof(array) / sizeof((array)[0]))))
X#define ARRAY_SIZE(array) 	((unsigned long) (IS_ARRAY(array) + 		(sizeof(array) / sizeof((array)[0]))))
N
N/* Evaluates to 1 if ptr is part of array, 0 otherwise; compile error if
N * "array" argument is not an array (e.g. "ptr" and "array" mixed up)
N */
N#define PART_OF_ARRAY(array, ptr) \
N	((ptr) && ((ptr) >= &array[0] && (ptr) < &array[ARRAY_SIZE(array)]))
X#define PART_OF_ARRAY(array, ptr) 	((ptr) && ((ptr) >= &array[0] && (ptr) < &array[ARRAY_SIZE(array)]))
N
N#define CONTAINER_OF(ptr, type, field) \
N	((type *)(((char *)(ptr)) - offsetof(type, field)))
X#define CONTAINER_OF(ptr, type, field) 	((type *)(((char *)(ptr)) - offsetof(type, field)))
N
N/* round "x" up/down to next multiple of "align" (which must be a power of 2) */
N#define ROUND_UP(x, align)                                   \
N	(((unsigned long)(x) + ((unsigned long)align - 1)) & \
N	 ~((unsigned long)align - 1))
X#define ROUND_UP(x, align)                                   	(((unsigned long)(x) + ((unsigned long)align - 1)) & 	 ~((unsigned long)align - 1))
N#define ROUND_DOWN(x, align) ((unsigned long)(x) & ~((unsigned long)align - 1))
N
N#define ceiling_fraction(numerator, divider) \
N	(((numerator) + ((divider) - 1)) / (divider))
X#define ceiling_fraction(numerator, divider) 	(((numerator) + ((divider) - 1)) / (divider))
N
N#ifdef INLINED
S#define INLINE inline
N#else
N#define INLINE
N#endif
N
N#ifndef max
N#define max(a, b) (((a) > (b)) ? (a) : (b))
N#endif
N
N#ifndef min
N#define min(a, b) (((a) < (b)) ? (a) : (b))
N#endif
N
Nstatic inline int is_power_of_two(unsigned int x)
N{
N	return (x != 0) && !(x & (x - 1));
N}
N
Nstatic inline s64_t arithmetic_shift_right(s64_t value, u8_t shift)
N{
N	s64_t sign_ext;
N
N	if (shift == 0) {
N		return value;
N	}
N
N	/* extract sign bit */
N	sign_ext = (value >> 63) & 1;
N
N	/* make all bits of sign_ext be the same as the value's sign bit */
N	sign_ext = -sign_ext;
N
N	/* shift value and fill opened bit positions with sign bit */
N	return (value >> shift) | (sign_ext << (64 - shift));
N}
N
N#endif /* !_ASMLANGUAGE */
N
N/* KB, MB, GB */
N#define KB(x) ((x) << 10)
N#define MB(x) (KB(x) << 10)
N#define GB(x) (MB(x) << 10)
N
N/* KHZ, MHZ */
N#define KHZ(x) ((x) * 1000)
N#define MHZ(x) (KHZ(x) * 1000)
N
N#ifndef BIT
N#define BIT(n)  (1UL << (n))
N#endif
N
N#define BIT_MASK(n) (BIT(n) - 1)
N
N/**
N * @brief Check for macro definition in compiler-visible expressions
N *
N * This trick was pioneered in Linux as the config_enabled() macro.
N * The madness has the effect of taking a macro value that may be
N * defined to "1" (e.g. CONFIG_MYFEATURE), or may not be defined at
N * all and turning it into a literal expression that can be used at
N * "runtime".  That is, it works similarly to
N * "defined(CONFIG_MYFEATURE)" does except that it is an expansion
N * that can exist in a standard expression and be seen by the compiler
N * and optimizer.  Thus much ifdef usage can be replaced with cleaner
N * expressions like:
N *
N *     if (IS_ENABLED(CONFIG_MYFEATURE))
N *             myfeature_enable();
N *
N * INTERNAL
N * First pass just to expand any existing macros, we need the macro
N * value to be e.g. a literal "1" at expansion time in the next macro,
N * not "(1)", etc...  Standard recursive expansion does not work.
N */
N#define IS_ENABLED(config_macro) _IS_ENABLED1(config_macro)
N
N/* Now stick on a "_XXXX" prefix, it will now be "_XXXX1" if config_macro
N * is "1", or just "_XXXX" if it's undefined.
N *   ENABLED:   _IS_ENABLED2(_XXXX1)
N *   DISABLED   _IS_ENABLED2(_XXXX)
N */
N#define _IS_ENABLED1(config_macro) _IS_ENABLED2(_XXXX##config_macro)
N
N/* Here's the core trick, we map "_XXXX1" to "_YYYY," (i.e. a string
N * with a trailing comma), so it has the effect of making this a
N * two-argument tuple to the preprocessor only in the case where the
N * value is defined to "1"
N *   ENABLED:    _YYYY,    <--- note comma!
N *   DISABLED:   _XXXX
N */
N#define _XXXX1 _YYYY,
N
N/* Then we append an extra argument to fool the gcc preprocessor into
N * accepting it as a varargs macro.
N *                         arg1   arg2  arg3
N *   ENABLED:   _IS_ENABLED3(_YYYY,    1,    0)
N *   DISABLED   _IS_ENABLED3(_XXXX 1,  0)
N */
N#define _IS_ENABLED2(one_or_two_args) _IS_ENABLED3(one_or_two_args 1, 0)
N
N/* And our second argument is thus now cooked to be 1 in the case
N * where the value is defined to 1, and 0 if not:
N */
N#define _IS_ENABLED3(ignore_this, val, ...) val
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* _UTIL__H_ */
L 27 "..\..\..\..\include\kernel.h" 2
N#include <drivers/rand32.h>
L 1 "..\..\..\..\include\drivers/rand32.h" 1
N/*
N * Copyright (c) 2013-2014 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Random number generator header file
N *
N * This header file declares prototypes for the kernel's random number generator
N * APIs.
N *
N * Typically, a platform enables the hidden CUSTOM_RANDOM_GENERATOR or
N * (for testing purposes only) enables the TEST_RANDOM_GENERATOR
N * configuration option and provide its own driver that implements
N * sys_rand32_get().
N */
N
N#ifndef __INCrand32h
N#define __INCrand32h
N
N#include <zephyr/types.h>
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
Nextern u32_t sys_rand32_get(void);
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* __INCrand32h */
L 28 "..\..\..\..\include\kernel.h" 2
N#include <kernel_arch_thread.h>
L 1 "..\..\..\..\arch\kernel_arch_thread.h" 1
N/*
N * Copyright (c) 2017 Intel Corporation
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Per-arch thread definition
N *
N * This file contains defintions for
N *
N *  struct _thread_arch
N *  struct _callee_saved
N *  struct _caller_saved
N *
N * necessary to instantiate instances of struct k_thread.
N */
N
N#ifndef _kernel_arch_thread_h_
N#define _kernel_arch_thread_h_
N
N#ifndef _ASMLANGUAGE
N#include <zephyr/types.h>
N
Nstruct _caller_saved {
N	/*
N	 * Unused for Cortex-M, which automatically saves the necessary
N	 * registers in its exception stack frame.
N	 *
N	 * For Cortex-A, this may be:
N	 *
N	 * u32_t a1;    // r0
N	 * u32_t a2;    // r1
N	 * u32_t a3;    // r2
N	 * u32_t a4;    // r3
N	 * u32_t ip;    // r12
N	 * u32_t lr;    // r14
N	 * u32_t pc;    // r15
N	 * u32_t xpsr;
N	 */
N};
N
Ntypedef struct _caller_saved _caller_saved_t;
N
Nstruct _callee_saved {
N	u32_t v1;  /* r4 */
N	u32_t v2;  /* r5 */
N	u32_t v3;  /* r6 */
N	u32_t v4;  /* r7 */
N	u32_t v5;  /* r8 */
N	u32_t v6;  /* r9 */
N	u32_t v7;  /* r10 */
N	u32_t v8;  /* r11 */
N	u32_t psp; /* r13 */
N};
N
Ntypedef struct _callee_saved _callee_saved_t;
N
N#ifdef CONFIG_FLOAT
Sstruct _preempt_float {
S	float  s16;
S	float  s17;
S	float  s18;
S	float  s19;
S	float  s20;
S	float  s21;
S	float  s22;
S	float  s23;
S	float  s24;
S	float  s25;
S	float  s26;
S	float  s27;
S	float  s28;
S	float  s29;
S	float  s30;
S	float  s31;
S};
N#endif
N
Nstruct _thread_arch {
N
N	/* interrupt locking key */
N	u32_t basepri;
N
N	/* r0 in stack frame cannot be written to reliably */
N	u32_t swap_return_value;
N
N#ifdef CONFIG_FLOAT
S	/*
S	 * No cooperative floating point register set structure exists for
S	 * the Cortex-M as it automatically saves the necessary registers
S	 * in its exception stack frame.
S	 */
S	struct _preempt_float  preempt_float;
N#endif
N};
N
Ntypedef struct _thread_arch _thread_arch_t;
N
N#endif /* _ASMLANGUAGE */
N
N#endif /* _kernel_arch_thread__h_ */
L 29 "..\..\..\..\include\kernel.h" 2
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N/**
N * @brief Kernel APIs
N * @defgroup kernel_apis Kernel APIs
N * @{
N * @}
N */
N
N#ifdef CONFIG_KERNEL_DEBUG
S#include <misc/printk.h>
S#define K_DEBUG(fmt, ...) printk("[%s]  " fmt, __func__, ##__VA_ARGS__)
N#else
N#define K_DEBUG(fmt, ...)
N#endif
N
N#define _NUM_COOP_PRIO (16)
N#define _NUM_PREEMPT_PRIO (15 + 1)
N
N#define K_PRIO_COOP(x) (-(_NUM_COOP_PRIO - (x)))
N#define K_PRIO_PREEMPT(x) (x)
N
N#define K_ANY NULL
N#define K_END NULL
N
N#define K_HIGHEST_THREAD_PRIO (-16)
N
N#define K_LOWEST_THREAD_PRIO 15
N
N#define K_IDLE_PRIO K_LOWEST_THREAD_PRIO
N
N#define K_HIGHEST_APPLICATION_THREAD_PRIO (K_HIGHEST_THREAD_PRIO)
N#define K_LOWEST_APPLICATION_THREAD_PRIO (K_LOWEST_THREAD_PRIO - 1)
N
Ntypedef sys_dlist_t _wait_q_t;
N
N#ifdef CONFIG_OBJECT_TRACING
S#define _OBJECT_TRACING_NEXT_PTR(type) struct type *__next
S#define _OBJECT_TRACING_INIT .__next = NULL,
N#else
N#define _OBJECT_TRACING_INIT
N#define _OBJECT_TRACING_NEXT_PTR(type)
N#endif
N
N#define _POLL_EVENT_OBJ_INIT(obj) \
N	.poll_events = SYS_DLIST_STATIC_INIT(&obj.poll_events),
X#define _POLL_EVENT_OBJ_INIT(obj) 	.poll_events = SYS_DLIST_STATIC_INIT(&obj.poll_events),
N#define _POLL_EVENT sys_dlist_t poll_events
N
Nstruct k_thread;
Nstruct k_mutex;
Nstruct k_sem;
Nstruct k_alert;
Nstruct k_msgq;
Nstruct k_mbox;
Nstruct k_pipe;
Nstruct k_queue;
Nstruct k_fifo;
Nstruct k_lifo;
Nstruct k_stack;
Nstruct k_mem_slab;
Nstruct k_mem_pool;
Nstruct k_timer;
Nstruct k_poll_event;
Nstruct k_poll_signal;
N
N/* timeouts */
N
Nstruct _timeout;
Ntypedef void (*_timeout_func_t)(struct _timeout *t);
N
Nstruct _timeout {
N	sys_dnode_t node;
N	struct k_thread *thread;
N	sys_dlist_t *wait_q;
N	s32_t delta_ticks_from_prev;
N	_timeout_func_t func;
N};
N
Nextern s32_t _timeout_remaining_get(struct _timeout *timeout);
N
N/* Threads */
Ntypedef void (*_thread_entry_t)(void *, void *, void *);
N
N#ifdef CONFIG_THREAD_MONITOR
Sstruct __thread_entry {
S	_thread_entry_t pEntry;
S	void *parameter1;
S	void *parameter2;
S	void *parameter3;
S};
N#endif
N
N/* can be used for creating 'dummy' threads, e.g. for pending on objects */
Nstruct _thread_base {
N
N	/* this thread's entry in a ready/wait queue */
N	sys_dnode_t k_q_node;
N
N	/* user facing 'thread options'; values defined in include/kernel.h */
N	u8_t user_options;
N
N	/* thread state */
N	u8_t thread_state;
N
N	/*
N	 * scheduler lock count and thread priority
N	 *
N	 * These two fields control the preemptibility of a thread.
N	 *
N	 * When the scheduler is locked, sched_locked is decremented, which
N	 * means that the scheduler is locked for values from 0xff to 0x01. A
N	 * thread is coop if its prio is negative, thus 0x80 to 0xff when
N	 * looked at the value as unsigned.
N	 *
N	 * By putting them end-to-end, this means that a thread is
N	 * non-preemptible if the bundled value is greater than or equal to
N	 * 0x0080.
N	 */
N	union {
N		struct {
N#if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
N			u8_t sched_locked;
N			s8_t prio;
N#else /* LITTLE and PDP */
S			s8_t prio;
S			u8_t sched_locked;
N#endif
N		};
N		u16_t preempt;
N	};
N
N	/* data returned by APIs */
N	void *swap_data;
N
N	/* this thread's entry in a timeout queue */
N	struct _timeout timeout;
N
N};
N
Ntypedef struct _thread_base _thread_base_t;
N
N/* Contains the stack information of a thread */
Nstruct _thread_stack_info {
N	/* Stack Start */
N	u32_t start;
N	/* Stack Size */
N	u32_t size;
N};
N
Ntypedef struct _thread_stack_info _thread_stack_info_t;
N
Nstruct k_thread {
N
N	struct _thread_base base;
N
N	/* defined by the architecture, but all archs need these */
N	struct _caller_saved caller_saved;
N	struct _callee_saved callee_saved;
N
N	/* static thread init data */
N	void *init_data;
N
N	/* abort function */
N	void (*fn_abort)(void);
N
N#if defined(CONFIG_THREAD_MONITOR)
X#if 0L
S	/* thread entry and parameters description */
S	struct __thread_entry *entry;
S
S	/* next item in list of all threads */
S	struct k_thread *next_thread;
N#endif
N
N#ifdef CONFIG_THREAD_CUSTOM_DATA
S	/* crude thread-local storage */
S	void *custom_data;
N#endif
N
N	/* per-thread errno variable */
N	int errno_var;
N
N	/* Stack Info */
N	struct _thread_stack_info stack_info;
N
N	/* arch-specifics: must always be at the end */
N	struct _thread_arch arch;
N};
N
Ntypedef struct k_thread _thread_t;
Ntypedef struct k_thread *k_tid_t;
N#define tcs k_thread
N
Nenum execution_context_types {
N	K_ISR = 0,
N	K_COOP_THREAD,
N	K_PREEMPT_THREAD,
N};
N
N/**
N * @defgroup profiling_apis Profiling APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @brief Analyze the main, idle, interrupt and system workqueue call stacks
N *
N * This routine calls @ref STACK_ANALYZE on the 4 call stacks declared and
N * maintained by the kernel. The sizes of those 4 call stacks are defined by:
N *
N * CONFIG_MAIN_STACK_SIZE
N * CONFIG_IDLE_STACK_SIZE
N * CONFIG_ISR_STACK_SIZE
N * CONFIG_SYSTEM_WORKQUEUE_STACK_SIZE
N *
N * @note CONFIG_INIT_STACKS and CONFIG_PRINTK must be set for this function to
N * produce output.
N *
N * @return N/A
N */
Nextern void k_call_stacks_analyze(void);
N
N/**
N * @} end defgroup profiling_apis
N */
N
N/**
N * @defgroup thread_apis Thread APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @typedef k_thread_entry_t
N * @brief Thread entry point function type.
N *
N * A thread's entry point function is invoked when the thread starts executing.
N * Up to 3 argument values can be passed to the function.
N *
N * The thread terminates execution permanently if the entry point function
N * returns. The thread is responsible for releasing any shared resources
N * it may own (such as mutexes and dynamically allocated memory), prior to
N * returning.
N *
N * @param p1 First argument.
N * @param p2 Second argument.
N * @param p3 Third argument.
N *
N * @return N/A
N */
Ntypedef void (*k_thread_entry_t)(void *p1, void *p2, void *p3);
N
N#endif /* !_ASMLANGUAGE */
N
N
N/*
N * Thread user options. May be needed by assembly code. Common part uses low
N * bits, arch-specific use high bits.
N */
N
N/* system thread that must not abort */
N#define K_ESSENTIAL (1 << 0)
N
N#if defined(CONFIG_FP_SHARING)
X#if 0L
S/* thread uses floating point registers */
S#define K_FP_REGS (1 << 1)
N#endif
N
N#ifdef CONFIG_X86
S/* x86 Bitmask definitions for threads user options */
S
S#if defined(CONFIG_FP_SHARING) && defined(CONFIG_SSE)
S/* thread uses SSEx (and also FP) registers */
S#define K_SSE_REGS (1 << 7)
S#endif
N#endif
N
N/* end - thread options */
N
N#if !defined(_ASMLANGUAGE)
X#if !0L
N
N/* Using typedef deliberately here, this is quite intended to be an opaque
N * type. K_THREAD_STACK_BUFFER() should be used to access the data within.
N *
N * The purpose of this data type is to clearly distinguish between the
N * declared symbol for a stack (of type k_thread_stack_t) and the underlying
N * buffer which composes the stack data actually used by the underlying
N * thread; they cannot be used interchangably as some arches precede the
N * stack buffer region with guard areas that trigger a MPU or MMU fault
N * if written to.
N *
N * APIs that want to work with the buffer inside should continue to use
N * char *.
N *
N * Stacks should always be created with K_THREAD_STACK_DEFINE().
N */
Nstruct __packed _k_thread_stack_element {
Xstruct __attribute__((__packed__)) _k_thread_stack_element {
N	char data;
N};
Ntypedef struct _k_thread_stack_element *k_thread_stack_t;
N
N/**
N * @brief Spawn a thread.
N *
N * This routine initializes a thread, then schedules it for execution.
N *
N * The new thread may be scheduled for immediate execution or a delayed start.
N * If the newly spawned thread does not have a delayed start the kernel
N * scheduler may preempt the current thread to allow the new thread to
N * execute.
N *
N * Kernel data structures for bookkeeping and context storage for this thread
N * will be placed at the beginning of the thread's stack memory region and may
N * become corrupted if too much of the stack is used. This function has been
N * deprecated in favor of k_thread_create() to give the user more control on
N * where these data structures reside.
N *
N * Thread options are architecture-specific, and can include K_ESSENTIAL,
N * K_FP_REGS, and K_SSE_REGS. Multiple options may be specified by separating
N * them using "|" (the logical OR operator).
N *
N * The stack itself should be declared with K_THREAD_STACK_DEFINE or variant
N * macros. The stack size parameter should either be a defined constant
N * also passed to K_THREAD_STACK_DEFINE, or the value of K_THREAD_STACK_SIZEOF.
N * Do not use regular C sizeof().
N *
N * @param stack Pointer to the stack space.
N * @param stack_size Stack size in bytes.
N * @param entry Thread entry function.
N * @param p1 1st entry point parameter.
N * @param p2 2nd entry point parameter.
N * @param p3 3rd entry point parameter.
N * @param prio Thread priority.
N * @param options Thread options.
N * @param delay Scheduling delay (in milliseconds), or K_NO_WAIT (for no delay).
N *
N * @return ID of new thread.
N */
Nextern __deprecated k_tid_t k_thread_spawn(k_thread_stack_t stack,
Xextern __attribute__((deprecated)) k_tid_t k_thread_spawn(k_thread_stack_t stack,
N			size_t stack_size, k_thread_entry_t entry,
N			void *p1, void *p2, void *p3,
N			int prio, u32_t options, s32_t delay);
N
N/**
N * @brief Create a thread.
N *
N * This routine initializes a thread, then schedules it for execution.
N *
N * The new thread may be scheduled for immediate execution or a delayed start.
N * If the newly spawned thread does not have a delayed start the kernel
N * scheduler may preempt the current thread to allow the new thread to
N * execute.
N *
N * Thread options are architecture-specific, and can include K_ESSENTIAL,
N * K_FP_REGS, and K_SSE_REGS. Multiple options may be specified by separating
N * them using "|" (the logical OR operator).
N *
N * Historically, users often would use the beginning of the stack memory region
N * to store the struct k_thread data, although corruption will occur if the
N * stack overflows this region and stack protection features may not detect this
N * situation.
N *
N * @param new_thread Pointer to uninitialized struct k_thread
N * @param stack Pointer to the stack space.
N * @param stack_size Stack size in bytes.
N * @param entry Thread entry function.
N * @param p1 1st entry point parameter.
N * @param p2 2nd entry point parameter.
N * @param p3 3rd entry point parameter.
N * @param prio Thread priority.
N * @param options Thread options.
N * @param delay Scheduling delay (in milliseconds), or K_NO_WAIT (for no delay).
N *
N * @return ID of new thread.
N */
Nextern k_tid_t k_thread_create(struct k_thread *new_thread,
N			       k_thread_stack_t stack,
N			       size_t stack_size,
N			       void (*entry)(void *, void *, void*),
N			       void *p1, void *p2, void *p3,
N			       int prio, u32_t options, s32_t delay);
N
N/**
N * @brief Put the current thread to sleep.
N *
N * This routine puts the current thread to sleep for @a duration
N * milliseconds.
N *
N * @param duration Number of milliseconds to sleep.
N *
N * @return N/A
N */
Nextern void k_sleep(s32_t duration);
N
N/**
N * @brief Cause the current thread to busy wait.
N *
N * This routine causes the current thread to execute a "do nothing" loop for
N * @a usec_to_wait microseconds.
N *
N * @return N/A
N */
Nextern void k_busy_wait(u32_t usec_to_wait);
N
N/**
N * @brief Yield the current thread.
N *
N * This routine causes the current thread to yield execution to another
N * thread of the same or higher priority. If there are no other ready threads
N * of the same or higher priority, the routine returns immediately.
N *
N * @return N/A
N */
Nextern void k_yield(void);
N
N/**
N * @brief Wake up a sleeping thread.
N *
N * This routine prematurely wakes up @a thread from sleeping.
N *
N * If @a thread is not currently sleeping, the routine has no effect.
N *
N * @param thread ID of thread to wake.
N *
N * @return N/A
N */
Nextern void k_wakeup(k_tid_t thread);
N
N/**
N * @brief Get thread ID of the current thread.
N *
N * @return ID of current thread.
N */
Nextern k_tid_t k_current_get(void);
N
N/**
N * @brief Cancel thread performing a delayed start.
N *
N * This routine prevents @a thread from executing if it has not yet started
N * execution. The thread must be re-spawned before it will execute.
N *
N * @param thread ID of thread to cancel.
N *
N * @retval 0 Thread spawning canceled.
N * @retval -EINVAL Thread has already started executing.
N */
Nextern int k_thread_cancel(k_tid_t thread);
N
N/**
N * @brief Abort a thread.
N *
N * This routine permanently stops execution of @a thread. The thread is taken
N * off all kernel queues it is part of (i.e. the ready queue, the timeout
N * queue, or a kernel object wait queue). However, any kernel resources the
N * thread might currently own (such as mutexes or memory blocks) are not
N * released. It is the responsibility of the caller of this routine to ensure
N * all necessary cleanup is performed.
N *
N * @param thread ID of thread to abort.
N *
N * @return N/A
N */
Nextern void k_thread_abort(k_tid_t thread);
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
N/* timeout has timed out and is not on _timeout_q anymore */
N#define _EXPIRED (-2)
N
N/* timeout is not in use */
N#define _INACTIVE (-1)
N
Nstruct _static_thread_data {
N	struct k_thread *init_thread;
N	k_thread_stack_t init_stack;
N	unsigned int init_stack_size;
N	void (*init_entry)(void *, void *, void *);
N	void *init_p1;
N	void *init_p2;
N	void *init_p3;
N	int init_prio;
N	u32_t init_options;
N	s32_t init_delay;
N	void (*init_abort)(void);
N	u32_t init_groups;
N};
N
N#define _THREAD_INITIALIZER(thread, stack, stack_size,           \
N			    entry, p1, p2, p3,                   \
N			    prio, options, delay, abort, groups) \
N	{                                                        \
N	.init_thread = (thread),				 \
N	.init_stack = (stack),					 \
N	.init_stack_size = (stack_size),                         \
N	.init_entry = (void (*)(void *, void *, void *))entry,   \
N	.init_p1 = (void *)p1,                                   \
N	.init_p2 = (void *)p2,                                   \
N	.init_p3 = (void *)p3,                                   \
N	.init_prio = (prio),                                     \
N	.init_options = (options),                               \
N	.init_delay = (delay),                                   \
N	.init_abort = (abort),                                   \
N	.init_groups = (groups),                                 \
N	}
X#define _THREAD_INITIALIZER(thread, stack, stack_size,           			    entry, p1, p2, p3,                   			    prio, options, delay, abort, groups) 	{                                                        	.init_thread = (thread),				 	.init_stack = (stack),					 	.init_stack_size = (stack_size),                         	.init_entry = (void (*)(void *, void *, void *))entry,   	.init_p1 = (void *)p1,                                   	.init_p2 = (void *)p2,                                   	.init_p3 = (void *)p3,                                   	.init_prio = (prio),                                     	.init_options = (options),                               	.init_delay = (delay),                                   	.init_abort = (abort),                                   	.init_groups = (groups),                                 	}
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @brief Statically define and initialize a thread.
N *
N * The thread may be scheduled for immediate execution or a delayed start.
N *
N * Thread options are architecture-specific, and can include K_ESSENTIAL,
N * K_FP_REGS, and K_SSE_REGS. Multiple options may be specified by separating
N * them using "|" (the logical OR operator).
N *
N * The ID of the thread can be accessed using:
N *
N * @code extern const k_tid_t <name>; @endcode
N *
N * @param name Name of the thread.
N * @param stack_size Stack size in bytes.
N * @param entry Thread entry function.
N * @param p1 1st entry point parameter.
N * @param p2 2nd entry point parameter.
N * @param p3 3rd entry point parameter.
N * @param prio Thread priority.
N * @param options Thread options.
N * @param delay Scheduling delay (in milliseconds), or K_NO_WAIT (for no delay).
N *
N * @internal It has been observed that the x86 compiler by default aligns
N * these _static_thread_data structures to 32-byte boundaries, thereby
N * wasting space. To work around this, force a 4-byte alignment.
N */
N#define K_THREAD_DEFINE(name, stack_size,                                \
N			entry, p1, p2, p3,                               \
N			prio, options, delay)                            \
N	K_THREAD_STACK_DEFINE(_k_thread_stack_##name, stack_size);	 \
N	struct k_thread _k_thread_obj_##name;				 \
N	struct _static_thread_data _k_thread_data_##name __aligned(4)    \
N		__in_section(_static_thread_data, static, name) =        \
N		_THREAD_INITIALIZER(&_k_thread_obj_##name,		 \
N				    _k_thread_stack_##name, stack_size,  \
N				entry, p1, p2, p3, prio, options, delay, \
N				NULL, 0);				 \
N	const k_tid_t name = (k_tid_t)&_k_thread_obj_##name
X#define K_THREAD_DEFINE(name, stack_size,                                			entry, p1, p2, p3,                               			prio, options, delay)                            	K_THREAD_STACK_DEFINE(_k_thread_stack_##name, stack_size);	 	struct k_thread _k_thread_obj_##name;				 	struct _static_thread_data _k_thread_data_##name __aligned(4)    		__in_section(_static_thread_data, static, name) =        		_THREAD_INITIALIZER(&_k_thread_obj_##name,		 				    _k_thread_stack_##name, stack_size,  				entry, p1, p2, p3, prio, options, delay, 				NULL, 0);				 	const k_tid_t name = (k_tid_t)&_k_thread_obj_##name
N
N/**
N * @brief Get a thread's priority.
N *
N * This routine gets the priority of @a thread.
N *
N * @param thread ID of thread whose priority is needed.
N *
N * @return Priority of @a thread.
N */
Nextern int  k_thread_priority_get(k_tid_t thread);
N
N/**
N * @brief Set a thread's priority.
N *
N * This routine immediately changes the priority of @a thread.
N *
N * Rescheduling can occur immediately depending on the priority @a thread is
N * set to:
N *
N * - If its priority is raised above the priority of the caller of this
N * function, and the caller is preemptible, @a thread will be scheduled in.
N *
N * - If the caller operates on itself, it lowers its priority below that of
N * other threads in the system, and the caller is preemptible, the thread of
N * highest priority will be scheduled in.
N *
N * Priority can be assigned in the range of -CONFIG_NUM_COOP_PRIORITIES to
N * CONFIG_NUM_PREEMPT_PRIORITIES-1, where -CONFIG_NUM_COOP_PRIORITIES is the
N * highest priority.
N *
N * @param thread ID of thread whose priority is to be set.
N * @param prio New priority.
N *
N * @warning Changing the priority of a thread currently involved in mutex
N * priority inheritance may result in undefined behavior.
N *
N * @return N/A
N */
Nextern void k_thread_priority_set(k_tid_t thread, int prio);
N
N/**
N * @brief Suspend a thread.
N *
N * This routine prevents the kernel scheduler from making @a thread the
N * current thread. All other internal operations on @a thread are still
N * performed; for example, any timeout it is waiting on keeps ticking,
N * kernel objects it is waiting on are still handed to it, etc.
N *
N * If @a thread is already suspended, the routine has no effect.
N *
N * @param thread ID of thread to suspend.
N *
N * @return N/A
N */
Nextern void k_thread_suspend(k_tid_t thread);
N
N/**
N * @brief Resume a suspended thread.
N *
N * This routine allows the kernel scheduler to make @a thread the current
N * thread, when it is next eligible for that role.
N *
N * If @a thread is not currently suspended, the routine has no effect.
N *
N * @param thread ID of thread to resume.
N *
N * @return N/A
N */
Nextern void k_thread_resume(k_tid_t thread);
N
N/**
N * @brief Set time-slicing period and scope.
N *
N * This routine specifies how the scheduler will perform time slicing of
N * preemptible threads.
N *
N * To enable time slicing, @a slice must be non-zero. The scheduler
N * ensures that no thread runs for more than the specified time limit
N * before other threads of that priority are given a chance to execute.
N * Any thread whose priority is higher than @a prio is exempted, and may
N * execute as long as desired without being preempted due to time slicing.
N *
N * Time slicing only limits the maximum amount of time a thread may continuously
N * execute. Once the scheduler selects a thread for execution, there is no
N * minimum guaranteed time the thread will execute before threads of greater or
N * equal priority are scheduled.
N *
N * When the current thread is the only one of that priority eligible
N * for execution, this routine has no effect; the thread is immediately
N * rescheduled after the slice period expires.
N *
N * To disable timeslicing, set both @a slice and @a prio to zero.
N *
N * @param slice Maximum time slice length (in milliseconds).
N * @param prio Highest thread priority level eligible for time slicing.
N *
N * @return N/A
N */
Nextern void k_sched_time_slice_set(s32_t slice, int prio);
N
N/**
N * @} end defgroup thread_apis
N */
N
N/**
N * @addtogroup isr_apis
N * @{
N */
N
N/**
N * @brief Determine if code is running at interrupt level.
N *
N * This routine allows the caller to customize its actions, depending on
N * whether it is a thread or an ISR.
N *
N * @note Can be called by ISRs.
N *
N * @return 0 if invoked by a thread.
N * @return Non-zero if invoked by an ISR.
N */
Nextern int k_is_in_isr(void);
N
N/**
N * @brief Determine if code is running in a preemptible thread.
N *
N * This routine allows the caller to customize its actions, depending on
N * whether it can be preempted by another thread. The routine returns a 'true'
N * value if all of the following conditions are met:
N *
N * - The code is running in a thread, not at ISR.
N * - The thread's priority is in the preemptible range.
N * - The thread has not locked the scheduler.
N *
N * @note Can be called by ISRs.
N *
N * @return 0 if invoked by an ISR or by a cooperative thread.
N * @return Non-zero if invoked by a preemptible thread.
N */
Nextern int k_is_preempt_thread(void);
N
N/**
N * @} end addtogroup isr_apis
N */
N
N/**
N * @addtogroup thread_apis
N * @{
N */
N
N/**
N * @brief Lock the scheduler.
N *
N * This routine prevents the current thread from being preempted by another
N * thread by instructing the scheduler to treat it as a cooperative thread.
N * If the thread subsequently performs an operation that makes it unready,
N * it will be context switched out in the normal manner. When the thread
N * again becomes the current thread, its non-preemptible status is maintained.
N *
N * This routine can be called recursively.
N *
N * @note k_sched_lock() and k_sched_unlock() should normally be used
N * when the operation being performed can be safely interrupted by ISRs.
N * However, if the amount of processing involved is very small, better
N * performance may be obtained by using irq_lock() and irq_unlock().
N *
N * @return N/A
N */
Nextern void k_sched_lock(void);
N
N/**
N * @brief Unlock the scheduler.
N *
N * This routine reverses the effect of a previous call to k_sched_lock().
N * A thread must call the routine once for each time it called k_sched_lock()
N * before the thread becomes preemptible.
N *
N * @return N/A
N */
Nextern void k_sched_unlock(void);
N
N/**
N * @brief Set current thread's custom data.
N *
N * This routine sets the custom data for the current thread to @ value.
N *
N * Custom data is not used by the kernel itself, and is freely available
N * for a thread to use as it sees fit. It can be used as a framework
N * upon which to build thread-local storage.
N *
N * @param value New custom data value.
N *
N * @return N/A
N */
Nextern void k_thread_custom_data_set(void *value);
N
N/**
N * @brief Get current thread's custom data.
N *
N * This routine returns the custom data for the current thread.
N *
N * @return Current custom data value.
N */
Nextern void *k_thread_custom_data_get(void);
N
N/**
N * @} end addtogroup thread_apis
N */
N
N#include <sys_clock.h>
L 1 "..\..\..\..\include\sys_clock.h" 1
N/*
N * Copyright (c) 2014-2015 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Variables needed needed for system clock
N *
N *
N * Declare variables used by both system timer device driver and kernel
N * components that use timer functionality.
N */
N
N#ifndef _SYS_CLOCK__H_
N#define _SYS_CLOCK__H_
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N#ifndef _ASMLANGUAGE
N#include <zephyr/types.h>
N
N#define sys_clock_ticks_per_sec 100
N
Nextern int sys_clock_hw_cycles_per_sec;
N
N/*
N * sys_clock_us_per_tick global variable represents a number
N * of microseconds in one OS timer tick
N */
Nextern int sys_clock_us_per_tick;
N
N/*
N * sys_clock_hw_cycles_per_tick global variable represents a number
N * of platform clock ticks in one OS timer tick.
N * sys_clock_hw_cycles_per_tick often represents a value of divider
N * of the board clock frequency
N */
Nextern int sys_clock_hw_cycles_per_tick;
N
N/* number of nsec per usec */
N#define NSEC_PER_USEC 1000
N
N/* number of microseconds per millisecond */
N#define USEC_PER_MSEC 1000
N
N/* number of milliseconds per second */
N#define MSEC_PER_SEC 1000
N
N/* number of microseconds per second */
N#define USEC_PER_SEC ((USEC_PER_MSEC) * (MSEC_PER_SEC))
N
N/* number of nanoseconds per second */
N#define NSEC_PER_SEC ((NSEC_PER_USEC) * (USEC_PER_MSEC) * (MSEC_PER_SEC))
N
N
N/* SYS_CLOCK_HW_CYCLES_TO_NS64 converts CPU clock cycles to nanoseconds */
N#define SYS_CLOCK_HW_CYCLES_TO_NS64(X) \
N	(((u64_t)(X) * sys_clock_us_per_tick * NSEC_PER_USEC) / \
N	 sys_clock_hw_cycles_per_tick)
X#define SYS_CLOCK_HW_CYCLES_TO_NS64(X) 	(((u64_t)(X) * sys_clock_us_per_tick * NSEC_PER_USEC) / 	 sys_clock_hw_cycles_per_tick)
N
N/*
N * SYS_CLOCK_HW_CYCLES_TO_NS_AVG converts CPU clock cycles to nanoseconds
N * and calculates the average cycle time
N */
N#define SYS_CLOCK_HW_CYCLES_TO_NS_AVG(X, NCYCLES) \
N	(u32_t)(SYS_CLOCK_HW_CYCLES_TO_NS64(X) / NCYCLES)
X#define SYS_CLOCK_HW_CYCLES_TO_NS_AVG(X, NCYCLES) 	(u32_t)(SYS_CLOCK_HW_CYCLES_TO_NS64(X) / NCYCLES)
N
N/**
N * @defgroup clock_apis Kernel Clock APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @brief Compute nanoseconds from hardware clock cycles.
N *
N * This macro converts a time duration expressed in hardware clock cycles
N * to the equivalent duration expressed in nanoseconds.
N *
N * @param X Duration in hardware clock cycles.
N *
N * @return Duration in nanoseconds.
N */
N#define SYS_CLOCK_HW_CYCLES_TO_NS(X) (u32_t)(SYS_CLOCK_HW_CYCLES_TO_NS64(X))
N
N/**
N * @} end defgroup clock_apis
N */
N
Nextern volatile u64_t _sys_clock_tick_count;
N
N/*
N * Number of ticks for x seconds. NOTE: With MSEC() or USEC(),
N * since it does an integer division, x must be greater or equal to
N * 1000/sys_clock_ticks_per_sec to get a non-zero value.
N * You may want to raise CONFIG_SYS_CLOCK_TICKS_PER_SEC depending on
N * your requirements.
N */
N#define SECONDS(x)	((x) * sys_clock_ticks_per_sec)
N#define MSEC(x)		(SECONDS(x) / MSEC_PER_SEC)
N#define USEC(x)		(MSEC(x) / USEC_PER_MSEC)
N
N#endif /* !_ASMLANGUAGE */
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* _SYS_CLOCK__H_ */
L 792 "..\..\..\..\include\kernel.h" 2
N
N/**
N * @addtogroup clock_apis
N * @{
N */
N
N/**
N * @brief Generate null timeout delay.
N *
N * This macro generates a timeout delay that that instructs a kernel API
N * not to wait if the requested operation cannot be performed immediately.
N *
N * @return Timeout delay value.
N */
N#define K_NO_WAIT 0
N
N/**
N * @brief Generate timeout delay from milliseconds.
N *
N * This macro generates a timeout delay that that instructs a kernel API
N * to wait up to @a ms milliseconds to perform the requested operation.
N *
N * @param ms Duration in milliseconds.
N *
N * @return Timeout delay value.
N */
N#define K_MSEC(ms)     (ms)
N
N/**
N * @brief Generate timeout delay from seconds.
N *
N * This macro generates a timeout delay that that instructs a kernel API
N * to wait up to @a s seconds to perform the requested operation.
N *
N * @param s Duration in seconds.
N *
N * @return Timeout delay value.
N */
N#define K_SECONDS(s)   K_MSEC((s) * MSEC_PER_SEC)
N
N/**
N * @brief Generate timeout delay from minutes.
N *
N * This macro generates a timeout delay that that instructs a kernel API
N * to wait up to @a m minutes to perform the requested operation.
N *
N * @param m Duration in minutes.
N *
N * @return Timeout delay value.
N */
N#define K_MINUTES(m)   K_SECONDS((m) * 60)
N
N/**
N * @brief Generate timeout delay from hours.
N *
N * This macro generates a timeout delay that that instructs a kernel API
N * to wait up to @a h hours to perform the requested operation.
N *
N * @param h Duration in hours.
N *
N * @return Timeout delay value.
N */
N#define K_HOURS(h)     K_MINUTES((h) * 60)
N
N/**
N * @brief Generate infinite timeout delay.
N *
N * This macro generates a timeout delay that that instructs a kernel API
N * to wait as long as necessary to perform the requested operation.
N *
N * @return Timeout delay value.
N */
N#define K_FOREVER (-1)
N
N/**
N * @} end addtogroup clock_apis
N */
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
N/* kernel clocks */
N
N#if	(sys_clock_ticks_per_sec == 1000) || \
N	(sys_clock_ticks_per_sec == 500)  || \
N	(sys_clock_ticks_per_sec == 250)  || \
N	(sys_clock_ticks_per_sec == 125)  || \
N	(sys_clock_ticks_per_sec == 100)  || \
N	(sys_clock_ticks_per_sec == 50)   || \
N	(sys_clock_ticks_per_sec == 25)   || \
N	(sys_clock_ticks_per_sec == 20)   || \
N	(sys_clock_ticks_per_sec == 10)   || \
N	(sys_clock_ticks_per_sec == 1)
X#if	(100 == 1000) || 	(100 == 500)  || 	(100 == 250)  || 	(100 == 125)  || 	(100 == 100)  || 	(100 == 50)   || 	(100 == 25)   || 	(100 == 20)   || 	(100 == 10)   || 	(100 == 1)
N
N	#define _ms_per_tick (MSEC_PER_SEC / sys_clock_ticks_per_sec)
N#else
S	/* yields horrible 64-bit math on many architectures: try to avoid */
S	#define _NON_OPTIMIZED_TICKS_PER_SEC
N#endif
N
N#ifdef _NON_OPTIMIZED_TICKS_PER_SEC
Sextern s32_t _ms_to_ticks(s32_t ms);
N#else
Nstatic ALWAYS_INLINE s32_t _ms_to_ticks(s32_t ms)
Xstatic inline __attribute__((always_inline)) s32_t _ms_to_ticks(s32_t ms)
N{
N	return (s32_t)ceiling_fraction((u32_t)ms, _ms_per_tick);
X	return (s32_t)((((u32_t)ms) + (((1000 / 100)) - 1)) / ((1000 / 100)));
N}
N#endif
N
N/* added tick needed to account for tick in progress */
N#define _TICK_ALIGN 1
N
Nstatic inline s64_t __ticks_to_ms(s64_t ticks)
N{
N#ifdef _NON_OPTIMIZED_TICKS_PER_SEC
S	return (MSEC_PER_SEC * (u64_t)ticks) / sys_clock_ticks_per_sec;
N#else
N	return (u64_t)ticks * _ms_per_tick;
X	return (u64_t)ticks * (1000 / 100);
N#endif
N}
N
Nstruct k_timer {
N	/*
N	 * _timeout structure must be first here if we want to use
N	 * dynamic timer allocation. timeout.node is used in the double-linked
N	 * list of free timers
N	 */
N	struct _timeout timeout;
N
N	/* wait queue for the (single) thread waiting on this timer */
N	_wait_q_t wait_q;
N
N	/* runs in ISR context */
N	void (*expiry_fn)(struct k_timer *);
N
N	/* runs in the context of the thread that calls k_timer_stop() */
N	void (*stop_fn)(struct k_timer *);
N
N	/* timer period */
N	s32_t period;
N
N	/* timer status */
N	u32_t status;
N
N	/* user-specific data, also used to support legacy features */
N	void *user_data;
N
N//	_OBJECT_TRACING_NEXT_PTR(k_timer);
N};
N
N#define _K_TIMER_INITIALIZER(obj, expiry, stop) \
N	{ \
N	.timeout.delta_ticks_from_prev = _INACTIVE, \
N	.timeout.wait_q = NULL, \
N	.timeout.thread = NULL, \
N	.timeout.func = _timer_expiration_handler, \
N	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), \
N	.expiry_fn = expiry, \
N	.stop_fn = stop, \
N	.status = 0, \
N	.user_data = 0, \
N	_OBJECT_TRACING_INIT \
N	}
X#define _K_TIMER_INITIALIZER(obj, expiry, stop) 	{ 	.timeout.delta_ticks_from_prev = _INACTIVE, 	.timeout.wait_q = NULL, 	.timeout.thread = NULL, 	.timeout.func = _timer_expiration_handler, 	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), 	.expiry_fn = expiry, 	.stop_fn = stop, 	.status = 0, 	.user_data = 0, 	_OBJECT_TRACING_INIT 	}
N
N#define K_TIMER_INITIALIZER DEPRECATED_MACRO _K_TIMER_INITIALIZER
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @defgroup timer_apis Timer APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @typedef k_timer_expiry_t
N * @brief Timer expiry function type.
N *
N * A timer's expiry function is executed by the system clock interrupt handler
N * each time the timer expires. The expiry function is optional, and is only
N * invoked if the timer has been initialized with one.
N *
N * @param timer     Address of timer.
N *
N * @return N/A
N */
Ntypedef void (*k_timer_expiry_t)(struct k_timer *timer);
N
N/**
N * @typedef k_timer_stop_t
N * @brief Timer stop function type.
N *
N * A timer's stop function is executed if the timer is stopped prematurely.
N * The function runs in the context of the thread that stops the timer.
N * The stop function is optional, and is only invoked if the timer has been
N * initialized with one.
N *
N * @param timer     Address of timer.
N *
N * @return N/A
N */
Ntypedef void (*k_timer_stop_t)(struct k_timer *timer);
N
N/**
N * @brief Statically define and initialize a timer.
N *
N * The timer can be accessed outside the module where it is defined using:
N *
N * @code extern struct k_timer <name>; @endcode
N *
N * @param name Name of the timer variable.
N * @param expiry_fn Function to invoke each time the timer expires.
N * @param stop_fn   Function to invoke if the timer is stopped while running.
N */
N#define K_TIMER_DEFINE(name, expiry_fn, stop_fn) \
N	struct k_timer name \
N		__in_section(_k_timer, static, name) = \
N		_K_TIMER_INITIALIZER(name, expiry_fn, stop_fn)
X#define K_TIMER_DEFINE(name, expiry_fn, stop_fn) 	struct k_timer name 		__in_section(_k_timer, static, name) = 		_K_TIMER_INITIALIZER(name, expiry_fn, stop_fn)
N
N/**
N * @brief Initialize a timer.
N *
N * This routine initializes a timer, prior to its first use.
N *
N * @param timer     Address of timer.
N * @param expiry_fn Function to invoke each time the timer expires.
N * @param stop_fn   Function to invoke if the timer is stopped while running.
N *
N * @return N/A
N */
Nextern void k_timer_init(struct k_timer *timer,
N			 k_timer_expiry_t expiry_fn,
N			 k_timer_stop_t stop_fn);
N
N/**
N * @brief Start a timer.
N *
N * This routine starts a timer, and resets its status to zero. The timer
N * begins counting down using the specified duration and period values.
N *
N * Attempting to start a timer that is already running is permitted.
N * The timer's status is reset to zero and the timer begins counting down
N * using the new duration and period values.
N *
N * @param timer     Address of timer.
N * @param duration  Initial timer duration (in milliseconds).
N * @param period    Timer period (in milliseconds).
N *
N * @return N/A
N */
Nextern void k_timer_start(struct k_timer *timer,
N			  s32_t duration, s32_t period);
N
N/**
N * @brief Stop a timer.
N *
N * This routine stops a running timer prematurely. The timer's stop function,
N * if one exists, is invoked by the caller.
N *
N * Attempting to stop a timer that is not running is permitted, but has no
N * effect on the timer.
N *
N * @note Can be called by ISRs.  The stop handler has to be callable from ISRs
N * if @a k_timer_stop is to be called from ISRs.
N *
N * @param timer     Address of timer.
N *
N * @return N/A
N */
Nextern void k_timer_stop(struct k_timer *timer);
N
N/**
N * @brief Read timer status.
N *
N * This routine reads the timer's status, which indicates the number of times
N * it has expired since its status was last read.
N *
N * Calling this routine resets the timer's status to zero.
N *
N * @param timer     Address of timer.
N *
N * @return Timer status.
N */
Nextern u32_t k_timer_status_get(struct k_timer *timer);
N
N/**
N * @brief Synchronize thread to timer expiration.
N *
N * This routine blocks the calling thread until the timer's status is non-zero
N * (indicating that it has expired at least once since it was last examined)
N * or the timer is stopped. If the timer status is already non-zero,
N * or the timer is already stopped, the caller continues without waiting.
N *
N * Calling this routine resets the timer's status to zero.
N *
N * This routine must not be used by interrupt handlers, since they are not
N * allowed to block.
N *
N * @param timer     Address of timer.
N *
N * @return Timer status.
N */
Nextern u32_t k_timer_status_sync(struct k_timer *timer);
N
N/**
N * @brief Get time remaining before a timer next expires.
N *
N * This routine computes the (approximate) time remaining before a running
N * timer next expires. If the timer is not running, it returns zero.
N *
N * @param timer     Address of timer.
N *
N * @return Remaining time (in milliseconds).
N */
Nstatic inline s32_t k_timer_remaining_get(struct k_timer *timer)
N{
N	return _timeout_remaining_get(&timer->timeout);
N}
N
N/**
N * @brief Associate user-specific data with a timer.
N *
N * This routine records the @a user_data with the @a timer, to be retrieved
N * later.
N *
N * It can be used e.g. in a timer handler shared across multiple subsystems to
N * retrieve data specific to the subsystem this timer is associated with.
N *
N * @param timer     Address of timer.
N * @param user_data User data to associate with the timer.
N *
N * @return N/A
N */
Nstatic inline void k_timer_user_data_set(struct k_timer *timer,
N					 void *user_data)
N{
N	timer->user_data = user_data;
N}
N
N/**
N * @brief Retrieve the user-specific data from a timer.
N *
N * @param timer     Address of timer.
N *
N * @return The user data.
N */
Nstatic inline void *k_timer_user_data_get(struct k_timer *timer)
N{
N	return timer->user_data;
N}
N
N/**
N * @} end defgroup timer_apis
N */
N
N/**
N * @addtogroup clock_apis
N * @{
N */
N
N/**
N * @brief Get system uptime.
N *
N * This routine returns the elapsed time since the system booted,
N * in milliseconds.
N *
N * @return Current uptime.
N */
Nextern s64_t k_uptime_get(void);
N
N#define k_enable_sys_clock_always_on() do { } while ((0))
N#define k_disable_sys_clock_always_on() do { } while ((0))
N
N/**
N * @brief Get system uptime (32-bit version).
N *
N * This routine returns the lower 32-bits of the elapsed time since the system
N * booted, in milliseconds.
N *
N * This routine can be more efficient than k_uptime_get(), as it reduces the
N * need for interrupt locking and 64-bit math. However, the 32-bit result
N * cannot hold a system uptime time larger than approximately 50 days, so the
N * caller must handle possible rollovers.
N *
N * @return Current uptime.
N */
Nextern u32_t k_uptime_get_32(void);
N
N/**
N * @brief Get elapsed time.
N *
N * This routine computes the elapsed time between the current system uptime
N * and an earlier reference time, in milliseconds.
N *
N * @param reftime Pointer to a reference time, which is updated to the current
N *                uptime upon return.
N *
N * @return Elapsed time.
N */
Nextern s64_t k_uptime_delta(s64_t *reftime);
N
N/**
N * @brief Get elapsed time (32-bit version).
N *
N * This routine computes the elapsed time between the current system uptime
N * and an earlier reference time, in milliseconds.
N *
N * This routine can be more efficient than k_uptime_delta(), as it reduces the
N * need for interrupt locking and 64-bit math. However, the 32-bit result
N * cannot hold an elapsed time larger than approximately 50 days, so the
N * caller must handle possible rollovers.
N *
N * @param reftime Pointer to a reference time, which is updated to the current
N *                uptime upon return.
N *
N * @return Elapsed time.
N */
Nextern u32_t k_uptime_delta_32(s64_t *reftime);
N
N/**
N * @brief Read the hardware clock.
N *
N * This routine returns the current time, as measured by the system's hardware
N * clock.
N *
N * @return Current hardware clock up-counter (in cycles).
N */
N#define k_cycle_get_32()	_arch_k_cycle_get_32()
N
N/**
N * @} end addtogroup clock_apis
N */
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
Nstruct k_queue {
N	sys_slist_t data_q;
N	union {
N		_wait_q_t wait_q;
N
N		_POLL_EVENT;
X		sys_dlist_t poll_events;
N	};
N
N//	_OBJECT_TRACING_NEXT_PTR(k_queue);
N};
N
N#define _K_QUEUE_INITIALIZER(obj) \
N	{ \
N	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), \
N	.data_q = SYS_SLIST_STATIC_INIT(&obj.data_q), \
N	_POLL_EVENT_OBJ_INIT(obj) \
N	_OBJECT_TRACING_INIT \
N	}
X#define _K_QUEUE_INITIALIZER(obj) 	{ 	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), 	.data_q = SYS_SLIST_STATIC_INIT(&obj.data_q), 	_POLL_EVENT_OBJ_INIT(obj) 	_OBJECT_TRACING_INIT 	}
N
N#define K_QUEUE_INITIALIZER DEPRECATED_MACRO _K_QUEUE_INITIALIZER
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @defgroup queue_apis Queue APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @brief Initialize a queue.
N *
N * This routine initializes a queue object, prior to its first use.
N *
N * @param queue Address of the queue.
N *
N * @return N/A
N */
Nextern void k_queue_init(struct k_queue *queue);
N
N/**
N * @brief Cancel waiting on a queue.
N *
N * This routine causes first thread pending on @a queue, if any, to
N * return from k_queue_get() call with NULL value (as if timeout expired).
N *
N * @note Can be called by ISRs.
N *
N * @param queue Address of the queue.
N *
N * @return N/A
N */
Nextern void k_queue_cancel_wait(struct k_queue *queue);
N
N/**
N * @brief Append an element to the end of a queue.
N *
N * This routine appends a data item to @a queue. A queue data item must be
N * aligned on a 4-byte boundary, and the first 32 bits of the item are
N * reserved for the kernel's use.
N *
N * @note Can be called by ISRs.
N *
N * @param queue Address of the queue.
N * @param data Address of the data item.
N *
N * @return N/A
N */
Nextern void k_queue_append(struct k_queue *queue, void *data);
N
N/**
N * @brief Prepend an element to a queue.
N *
N * This routine prepends a data item to @a queue. A queue data item must be
N * aligned on a 4-byte boundary, and the first 32 bits of the item are
N * reserved for the kernel's use.
N *
N * @note Can be called by ISRs.
N *
N * @param queue Address of the queue.
N * @param data Address of the data item.
N *
N * @return N/A
N */
Nextern void k_queue_prepend(struct k_queue *queue, void *data);
N
N/**
N * @brief Inserts an element to a queue.
N *
N * This routine inserts a data item to @a queue after previous item. A queue
N * data item must be aligned on a 4-byte boundary, and the first 32 bits of the
N * item are reserved for the kernel's use.
N *
N * @note Can be called by ISRs.
N *
N * @param queue Address of the queue.
N * @param prev Address of the previous data item.
N * @param data Address of the data item.
N *
N * @return N/A
N */
Nextern void k_queue_insert(struct k_queue *queue, void *prev, void *data);
N
N/**
N * @brief Atomically append a list of elements to a queue.
N *
N * This routine adds a list of data items to @a queue in one operation.
N * The data items must be in a singly-linked list, with the first 32 bits
N * in each data item pointing to the next data item; the list must be
N * NULL-terminated.
N *
N * @note Can be called by ISRs.
N *
N * @param queue Address of the queue.
N * @param head Pointer to first node in singly-linked list.
N * @param tail Pointer to last node in singly-linked list.
N *
N * @return N/A
N */
Nextern void k_queue_append_list(struct k_queue *queue, void *head, void *tail);
N
N/**
N * @brief Atomically add a list of elements to a queue.
N *
N * This routine adds a list of data items to @a queue in one operation.
N * The data items must be in a singly-linked list implemented using a
N * sys_slist_t object. Upon completion, the original list is empty.
N *
N * @note Can be called by ISRs.
N *
N * @param queue Address of the queue.
N * @param list Pointer to sys_slist_t object.
N *
N * @return N/A
N */
Nextern void k_queue_merge_slist(struct k_queue *queue, sys_slist_t *list);
N
N/**
N * @brief Get an element from a queue.
N *
N * This routine removes first data item from @a queue. The first 32 bits of the
N * data item are reserved for the kernel's use.
N *
N * @note Can be called by ISRs, but @a timeout must be set to K_NO_WAIT.
N *
N * @param queue Address of the queue.
N * @param timeout Waiting period to obtain a data item (in milliseconds),
N *                or one of the special values K_NO_WAIT and K_FOREVER.
N *
N * @return Address of the data item if successful; NULL if returned
N * without waiting, or waiting period timed out.
N */
Nextern void *k_queue_get(struct k_queue *queue, s32_t timeout);
N
N/**
N * @brief Remove an element from a queue.
N *
N * This routine removes data item from @a queue. The first 32 bits of the
N * data item are reserved for the kernel's use. Removing elements from k_queue
N * rely on sys_slist_find_and_remove which is not a constant time operation.
N *
N * @note Can be called by ISRs
N *
N * @param queue Address of the queue.
N * @param data Address of the data item.
N *
N * @return true if data item was removed
N */
Nstatic inline bool k_queue_remove(struct k_queue *queue, void *data)
Xstatic inline _Bool k_queue_remove(struct k_queue *queue, void *data)
N{
N	return sys_slist_find_and_remove(&queue->data_q, (sys_snode_t *)data);
N}
N
N/**
N * @brief Query a queue to see if it has data available.
N *
N * Note that the data might be already gone by the time this function returns
N * if other threads are also trying to read from the queue.
N *
N * @note Can be called by ISRs.
N *
N * @param queue Address of the queue.
N *
N * @return Non-zero if the queue is empty.
N * @return 0 if data is available.
N */
Nstatic inline int k_queue_is_empty(struct k_queue *queue)
N{
N	return (int)sys_slist_is_empty(&queue->data_q);
N}
N
N/**
N * @brief Peek element at the head of queue.
N *
N * Return element from the head of queue without removing it.
N *
N * @param queue Address of the queue.
N *
N * @return Head element, or NULL if queue is empty.
N */
Nstatic inline void *k_queue_peek_head(struct k_queue *queue)
N{
N	return sys_slist_peek_head(&queue->data_q);
N}
N
N/**
N * @brief Peek element at the tail of queue.
N *
N * Return element from the tail of queue without removing it.
N *
N * @param queue Address of the queue.
N *
N * @return Tail element, or NULL if queue is empty.
N */
Nstatic inline void *k_queue_peek_tail(struct k_queue *queue)
N{
N	return sys_slist_peek_tail(&queue->data_q);
N}
N
N/**
N * @brief Statically define and initialize a queue.
N *
N * The queue can be accessed outside the module where it is defined using:
N *
N * @code extern struct k_queue <name>; @endcode
N *
N * @param name Name of the queue.
N */
N#define K_QUEUE_DEFINE(name) \
N	struct k_queue name \
N		__in_section(_k_queue, static, name) = \
N		_K_QUEUE_INITIALIZER(name)
X#define K_QUEUE_DEFINE(name) 	struct k_queue name 		__in_section(_k_queue, static, name) = 		_K_QUEUE_INITIALIZER(name)
N
N/**
N * @} end defgroup queue_apis
N */
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
Nstruct k_fifo {
N	struct k_queue _queue;
N};
N
N#define _K_FIFO_INITIALIZER(obj) \
N	{ \
N	._queue = _K_QUEUE_INITIALIZER(obj._queue) \
N	}
X#define _K_FIFO_INITIALIZER(obj) 	{ 	._queue = _K_QUEUE_INITIALIZER(obj._queue) 	}
N
N#define K_FIFO_INITIALIZER DEPRECATED_MACRO _K_FIFO_INITIALIZER
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @defgroup fifo_apis Fifo APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @brief Initialize a fifo.
N *
N * This routine initializes a fifo object, prior to its first use.
N *
N * @param fifo Address of the fifo.
N *
N * @return N/A
N */
N#define k_fifo_init(fifo) \
N	k_queue_init((struct k_queue *) fifo)
X#define k_fifo_init(fifo) 	k_queue_init((struct k_queue *) fifo)
N
N/**
N * @brief Cancel waiting on a fifo.
N *
N * This routine causes first thread pending on @a fifo, if any, to
N * return from k_fifo_get() call with NULL value (as if timeout
N * expired).
N *
N * @note Can be called by ISRs.
N *
N * @param fifo Address of the fifo.
N *
N * @return N/A
N */
N#define k_fifo_cancel_wait(fifo) \
N	k_queue_cancel_wait((struct k_queue *) fifo)
X#define k_fifo_cancel_wait(fifo) 	k_queue_cancel_wait((struct k_queue *) fifo)
N
N/**
N * @brief Add an element to a fifo.
N *
N * This routine adds a data item to @a fifo. A fifo data item must be
N * aligned on a 4-byte boundary, and the first 32 bits of the item are
N * reserved for the kernel's use.
N *
N * @note Can be called by ISRs.
N *
N * @param fifo Address of the fifo.
N * @param data Address of the data item.
N *
N * @return N/A
N */
N#define k_fifo_put(fifo, data) \
N	k_queue_append((struct k_queue *) fifo, data)
X#define k_fifo_put(fifo, data) 	k_queue_append((struct k_queue *) fifo, data)
N
N/**
N * @brief Atomically add a list of elements to a fifo.
N *
N * This routine adds a list of data items to @a fifo in one operation.
N * The data items must be in a singly-linked list, with the first 32 bits
N * each data item pointing to the next data item; the list must be
N * NULL-terminated.
N *
N * @note Can be called by ISRs.
N *
N * @param fifo Address of the fifo.
N * @param head Pointer to first node in singly-linked list.
N * @param tail Pointer to last node in singly-linked list.
N *
N * @return N/A
N */
N#define k_fifo_put_list(fifo, head, tail) \
N	k_queue_append_list((struct k_queue *) fifo, head, tail)
X#define k_fifo_put_list(fifo, head, tail) 	k_queue_append_list((struct k_queue *) fifo, head, tail)
N
N/**
N * @brief Atomically add a list of elements to a fifo.
N *
N * This routine adds a list of data items to @a fifo in one operation.
N * The data items must be in a singly-linked list implemented using a
N * sys_slist_t object. Upon completion, the sys_slist_t object is invalid
N * and must be re-initialized via sys_slist_init().
N *
N * @note Can be called by ISRs.
N *
N * @param fifo Address of the fifo.
N * @param list Pointer to sys_slist_t object.
N *
N * @return N/A
N */
N#define k_fifo_put_slist(fifo, list) \
N	k_queue_merge_slist((struct k_queue *) fifo, list)
X#define k_fifo_put_slist(fifo, list) 	k_queue_merge_slist((struct k_queue *) fifo, list)
N
N/**
N * @brief Get an element from a fifo.
N *
N * This routine removes a data item from @a fifo in a "first in, first out"
N * manner. The first 32 bits of the data item are reserved for the kernel's use.
N *
N * @note Can be called by ISRs, but @a timeout must be set to K_NO_WAIT.
N *
N * @param fifo Address of the fifo.
N * @param timeout Waiting period to obtain a data item (in milliseconds),
N *                or one of the special values K_NO_WAIT and K_FOREVER.
N *
N * @return Address of the data item if successful; NULL if returned
N * without waiting, or waiting period timed out.
N */
N#define k_fifo_get(fifo, timeout) \
N	k_queue_get((struct k_queue *) fifo, timeout)
X#define k_fifo_get(fifo, timeout) 	k_queue_get((struct k_queue *) fifo, timeout)
N
N/**
N * @brief Query a fifo to see if it has data available.
N *
N * Note that the data might be already gone by the time this function returns
N * if other threads is also trying to read from the fifo.
N *
N * @note Can be called by ISRs.
N *
N * @param fifo Address of the fifo.
N *
N * @return Non-zero if the fifo is empty.
N * @return 0 if data is available.
N */
N#define k_fifo_is_empty(fifo) \
N	k_queue_is_empty((struct k_queue *) fifo)
X#define k_fifo_is_empty(fifo) 	k_queue_is_empty((struct k_queue *) fifo)
N
N/**
N * @brief Peek element at the head of fifo.
N *
N * Return element from the head of fifo without removing it. A usecase
N * for this is if elements of the fifo are themselves containers. Then
N * on each iteration of processing, a head container will be peeked,
N * and some data processed out of it, and only if the container is empty,
N * it will be completely remove from the fifo.
N *
N * @param fifo Address of the fifo.
N *
N * @return Head element, or NULL if the fifo is empty.
N */
N#define k_fifo_peek_head(fifo) \
N	k_queue_peek_head((struct k_queue *) fifo)
X#define k_fifo_peek_head(fifo) 	k_queue_peek_head((struct k_queue *) fifo)
N
N/**
N * @brief Peek element at the tail of fifo.
N *
N * Return element from the tail of fifo (without removing it). A usecase
N * for this is if elements of the fifo are themselves containers. Then
N * it may be useful to add more data to the last container in fifo.
N *
N * @param fifo Address of the fifo.
N *
N * @return Tail element, or NULL if fifo is empty.
N */
N#define k_fifo_peek_tail(fifo) \
N	k_queue_peek_tail((struct k_queue *) fifo)
X#define k_fifo_peek_tail(fifo) 	k_queue_peek_tail((struct k_queue *) fifo)
N
N/**
N * @brief Statically define and initialize a fifo.
N *
N * The fifo can be accessed outside the module where it is defined using:
N *
N * @code extern struct k_fifo <name>; @endcode
N *
N * @param name Name of the fifo.
N */
N#define K_FIFO_DEFINE(name) \
N	struct k_fifo name \
N		__in_section(_k_queue, static, name) = \
N		_K_FIFO_INITIALIZER(name)
X#define K_FIFO_DEFINE(name) 	struct k_fifo name 		__in_section(_k_queue, static, name) = 		_K_FIFO_INITIALIZER(name)
N
N/**
N * @} end defgroup fifo_apis
N */
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
Nstruct k_lifo {
N	struct k_queue _queue;
N};
N
N#define _K_LIFO_INITIALIZER(obj) \
N	{ \
N	._queue = _K_QUEUE_INITIALIZER(obj._queue) \
N	}
X#define _K_LIFO_INITIALIZER(obj) 	{ 	._queue = _K_QUEUE_INITIALIZER(obj._queue) 	}
N
N#define K_LIFO_INITIALIZER DEPRECATED_MACRO _K_LIFO_INITIALIZER
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @defgroup lifo_apis Lifo APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @brief Initialize a lifo.
N *
N * This routine initializes a lifo object, prior to its first use.
N *
N * @param lifo Address of the lifo.
N *
N * @return N/A
N */
N#define k_lifo_init(lifo) \
N	k_queue_init((struct k_queue *) lifo)
X#define k_lifo_init(lifo) 	k_queue_init((struct k_queue *) lifo)
N
N/**
N * @brief Add an element to a lifo.
N *
N * This routine adds a data item to @a lifo. A lifo data item must be
N * aligned on a 4-byte boundary, and the first 32 bits of the item are
N * reserved for the kernel's use.
N *
N * @note Can be called by ISRs.
N *
N * @param lifo Address of the lifo.
N * @param data Address of the data item.
N *
N * @return N/A
N */
N#define k_lifo_put(lifo, data) \
N	k_queue_prepend((struct k_queue *) lifo, data)
X#define k_lifo_put(lifo, data) 	k_queue_prepend((struct k_queue *) lifo, data)
N
N/**
N * @brief Get an element from a lifo.
N *
N * This routine removes a data item from @a lifo in a "last in, first out"
N * manner. The first 32 bits of the data item are reserved for the kernel's use.
N *
N * @note Can be called by ISRs, but @a timeout must be set to K_NO_WAIT.
N *
N * @param lifo Address of the lifo.
N * @param timeout Waiting period to obtain a data item (in milliseconds),
N *                or one of the special values K_NO_WAIT and K_FOREVER.
N *
N * @return Address of the data item if successful; NULL if returned
N * without waiting, or waiting period timed out.
N */
N#define k_lifo_get(lifo, timeout) \
N	k_queue_get((struct k_queue *) lifo, timeout)
X#define k_lifo_get(lifo, timeout) 	k_queue_get((struct k_queue *) lifo, timeout)
N
N/**
N * @brief Statically define and initialize a lifo.
N *
N * The lifo can be accessed outside the module where it is defined using:
N *
N * @code extern struct k_lifo <name>; @endcode
N *
N * @param name Name of the fifo.
N */
N#define K_LIFO_DEFINE(name) \
N	struct k_lifo name \
N		__in_section(_k_queue, static, name) = \
N		_K_LIFO_INITIALIZER(name)
X#define K_LIFO_DEFINE(name) 	struct k_lifo name 		__in_section(_k_queue, static, name) = 		_K_LIFO_INITIALIZER(name)
N
N/**
N * @} end defgroup lifo_apis
N */
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
Nstruct k_stack {
N	_wait_q_t wait_q;
N	u32_t *base, *next, *top;
N
N//	_OBJECT_TRACING_NEXT_PTR(k_stack);
N};
N
N#define _K_STACK_INITIALIZER(obj, stack_buffer, stack_num_entries) \
N	{ \
N	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), \
N	.base = stack_buffer, \
N	.next = stack_buffer, \
N	.top = stack_buffer + stack_num_entries, \
N	_OBJECT_TRACING_INIT \
N	}
X#define _K_STACK_INITIALIZER(obj, stack_buffer, stack_num_entries) 	{ 	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), 	.base = stack_buffer, 	.next = stack_buffer, 	.top = stack_buffer + stack_num_entries, 	_OBJECT_TRACING_INIT 	}
N
N#define K_STACK_INITIALIZER DEPRECATED_MACRO _K_STACK_INITIALIZER
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @defgroup stack_apis Stack APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @brief Initialize a stack.
N *
N * This routine initializes a stack object, prior to its first use.
N *
N * @param stack Address of the stack.
N * @param buffer Address of array used to hold stacked values.
N * @param num_entries Maximum number of values that can be stacked.
N *
N * @return N/A
N */
Nextern void k_stack_init(struct k_stack *stack,
N			 u32_t *buffer, int num_entries);
N
N/**
N * @brief Push an element onto a stack.
N *
N * This routine adds a 32-bit value @a data to @a stack.
N *
N * @note Can be called by ISRs.
N *
N * @param stack Address of the stack.
N * @param data Value to push onto the stack.
N *
N * @return N/A
N */
Nextern void k_stack_push(struct k_stack *stack, u32_t data);
N
N/**
N * @brief Pop an element from a stack.
N *
N * This routine removes a 32-bit value from @a stack in a "last in, first out"
N * manner and stores the value in @a data.
N *
N * @note Can be called by ISRs, but @a timeout must be set to K_NO_WAIT.
N *
N * @param stack Address of the stack.
N * @param data Address of area to hold the value popped from the stack.
N * @param timeout Waiting period to obtain a value (in milliseconds),
N *                or one of the special values K_NO_WAIT and K_FOREVER.
N *
N * @retval 0 Element popped from stack.
N * @retval -EBUSY Returned without waiting.
N * @retval -EAGAIN Waiting period timed out.
N */
Nextern int k_stack_pop(struct k_stack *stack, u32_t *data, s32_t timeout);
N
N/**
N * @brief Statically define and initialize a stack
N *
N * The stack can be accessed outside the module where it is defined using:
N *
N * @code extern struct k_stack <name>; @endcode
N *
N * @param name Name of the stack.
N * @param stack_num_entries Maximum number of values that can be stacked.
N */
N#define K_STACK_DEFINE(name, stack_num_entries)                \
N	u32_t __noinit                                      \
N		_k_stack_buf_##name[stack_num_entries];        \
N	struct k_stack name                                    \
N		__in_section(_k_stack, static, name) =    \
N		_K_STACK_INITIALIZER(name, _k_stack_buf_##name, \
N				    stack_num_entries)
X#define K_STACK_DEFINE(name, stack_num_entries)                	u32_t __noinit                                      		_k_stack_buf_##name[stack_num_entries];        	struct k_stack name                                    		__in_section(_k_stack, static, name) =    		_K_STACK_INITIALIZER(name, _k_stack_buf_##name, 				    stack_num_entries)
N
N/**
N * @} end defgroup stack_apis
N */
N
Nstruct k_work;
N
N/**
N * @defgroup workqueue_apis Workqueue Thread APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @typedef k_work_handler_t
N * @brief Work item handler function type.
N *
N * A work item's handler function is executed by a workqueue's thread
N * when the work item is processed by the workqueue.
N *
N * @param work Address of the work item.
N *
N * @return N/A
N */
Ntypedef void (*k_work_handler_t)(struct k_work *work);
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
Nstruct k_work_q {
N	struct k_queue queue;
N	struct k_thread thread;
N};
N
Nenum {
N	K_WORK_STATE_PENDING,	/* Work item pending state */
N};
N
Nstruct k_work {
N	void *_reserved;		/* Used by k_queue implementation. */
N	k_work_handler_t handler;
N	atomic_t flags[1];
N};
N
Nstruct k_delayed_work {
N	struct k_work work;
N	struct _timeout timeout;
N	struct k_work_q *work_q;
N};
N
Nextern struct k_work_q k_sys_work_q;
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N#define _K_WORK_INITIALIZER(work_handler) \
N	{ \
N	._reserved = NULL, \
N	.handler = work_handler, \
N	.flags = { 0 } \
N	}
X#define _K_WORK_INITIALIZER(work_handler) 	{ 	._reserved = NULL, 	.handler = work_handler, 	.flags = { 0 } 	}
N
N#define K_WORK_INITIALIZER DEPRECATED_MACRO _K_WORK_INITIALIZER
N
N/**
N * @brief Initialize a statically-defined work item.
N *
N * This macro can be used to initialize a statically-defined workqueue work
N * item, prior to its first use. For example,
N *
N * @code static K_WORK_DEFINE(<work>, <work_handler>); @endcode
N *
N * @param work Symbol name for work item object
N * @param work_handler Function to invoke each time work item is processed.
N */
N#define K_WORK_DEFINE(work, work_handler) \
N	struct k_work work \
N		__in_section(_k_work, static, work) = \
N		_K_WORK_INITIALIZER(work_handler)
X#define K_WORK_DEFINE(work, work_handler) 	struct k_work work 		__in_section(_k_work, static, work) = 		_K_WORK_INITIALIZER(work_handler)
N
N/**
N * @brief Initialize a work item.
N *
N * This routine initializes a workqueue work item, prior to its first use.
N *
N * @param work Address of work item.
N * @param handler Function to invoke each time work item is processed.
N *
N * @return N/A
N */
Nstatic inline void k_work_init(struct k_work *work, k_work_handler_t handler)
N{
N	atomic_clear_bit(work->flags, K_WORK_STATE_PENDING);
N	work->handler = handler;
N}
N
N/**
N * @brief Submit a work item.
N *
N * This routine submits work item @a work to be processed by workqueue
N * @a work_q. If the work item is already pending in the workqueue's queue
N * as a result of an earlier submission, this routine has no effect on the
N * work item. If the work item has already been processed, or is currently
N * being processed, its work is considered complete and the work item can be
N * resubmitted.
N *
N * @warning
N * A submitted work item must not be modified until it has been processed
N * by the workqueue.
N *
N * @note Can be called by ISRs.
N *
N * @param work_q Address of workqueue.
N * @param work Address of work item.
N *
N * @return N/A
N */
Nstatic inline void k_work_submit_to_queue(struct k_work_q *work_q,
N					  struct k_work *work)
N{
N	if (!atomic_test_and_set_bit(work->flags, K_WORK_STATE_PENDING)) {
N		k_queue_append(&work_q->queue, work);
N	}
N}
N
N/**
N * @brief Check if a work item is pending.
N *
N * This routine indicates if work item @a work is pending in a workqueue's
N * queue.
N *
N * @note Can be called by ISRs.
N *
N * @param work Address of work item.
N *
N * @return 1 if work item is pending, or 0 if it is not pending.
N */
Nstatic inline int k_work_pending(struct k_work *work)
N{
N	return atomic_test_bit(work->flags, K_WORK_STATE_PENDING);
N}
N
N/**
N * @brief Start a workqueue.
N *
N * This routine starts workqueue @a work_q. The workqueue spawns its work
N * processing thread, which runs forever.
N *
N * @param work_q Address of workqueue.
N * @param stack Pointer to work queue thread's stack space, as defined by
N *		K_THREAD_STACK_DEFINE()
N * @param stack_size Size of the work queue thread's stack (in bytes), which
N *		should either be the same constant passed to
N *		K_THREAD_STACK_DEFINE() or the value of K_THREAD_STACK_SIZEOF().
N * @param prio Priority of the work queue's thread.
N *
N * @return N/A
N */
Nextern void k_work_q_start(struct k_work_q *work_q,
N			   k_thread_stack_t stack,
N			   size_t stack_size, int prio);
N
N/**
N * @brief Initialize a delayed work item.
N *
N * This routine initializes a workqueue delayed work item, prior to
N * its first use.
N *
N * @param work Address of delayed work item.
N * @param handler Function to invoke each time work item is processed.
N *
N * @return N/A
N */
Nextern void k_delayed_work_init(struct k_delayed_work *work,
N				k_work_handler_t handler);
N
N/**
N * @brief Submit a delayed work item.
N *
N * This routine schedules work item @a work to be processed by workqueue
N * @a work_q after a delay of @a delay milliseconds. The routine initiates
N * an asynchronous countdown for the work item and then returns to the caller.
N * Only when the countdown completes is the work item actually submitted to
N * the workqueue and becomes pending.
N *
N * Submitting a previously submitted delayed work item that is still
N * counting down cancels the existing submission and restarts the countdown
N * using the new delay. If the work item is currently pending on the
N * workqueue's queue because the countdown has completed it is too late to
N * resubmit the item, and resubmission fails without impacting the work item.
N * If the work item has already been processed, or is currently being processed,
N * its work is considered complete and the work item can be resubmitted.
N *
N * @warning
N * A delayed work item must not be modified until it has been processed
N * by the workqueue.
N *
N * @note Can be called by ISRs.
N *
N * @param work_q Address of workqueue.
N * @param work Address of delayed work item.
N * @param delay Delay before submitting the work item (in milliseconds).
N *
N * @retval 0 Work item countdown started.
N * @retval -EINPROGRESS Work item is already pending.
N * @retval -EINVAL Work item is being processed or has completed its work.
N * @retval -EADDRINUSE Work item is pending on a different workqueue.
N */
Nextern int k_delayed_work_submit_to_queue(struct k_work_q *work_q,
N					  struct k_delayed_work *work,
N					  s32_t delay);
N
N/**
N * @brief Cancel a delayed work item.
N *
N * This routine cancels the submission of delayed work item @a work.
N * A delayed work item can only be canceled while its countdown is still
N * underway.
N *
N * @note Can be called by ISRs.
N *
N * @param work Address of delayed work item.
N *
N * @retval 0 Work item countdown canceled.
N * @retval -EINPROGRESS Work item is already pending.
N * @retval -EINVAL Work item is being processed or has completed its work.
N */
Nextern int k_delayed_work_cancel(struct k_delayed_work *work);
N
N/**
N * @brief Submit a work item to the system workqueue.
N *
N * This routine submits work item @a work to be processed by the system
N * workqueue. If the work item is already pending in the workqueue's queue
N * as a result of an earlier submission, this routine has no effect on the
N * work item. If the work item has already been processed, or is currently
N * being processed, its work is considered complete and the work item can be
N * resubmitted.
N *
N * @warning
N * Work items submitted to the system workqueue should avoid using handlers
N * that block or yield since this may prevent the system workqueue from
N * processing other work items in a timely manner.
N *
N * @note Can be called by ISRs.
N *
N * @param work Address of work item.
N *
N * @return N/A
N */
Nstatic inline void k_work_submit(struct k_work *work)
N{
N	k_work_submit_to_queue(&k_sys_work_q, work);
N}
N
N/**
N * @brief Submit a delayed work item to the system workqueue.
N *
N * This routine schedules work item @a work to be processed by the system
N * workqueue after a delay of @a delay milliseconds. The routine initiates
N * an asynchronous countdown for the work item and then returns to the caller.
N * Only when the countdown completes is the work item actually submitted to
N * the workqueue and becomes pending.
N *
N * Submitting a previously submitted delayed work item that is still
N * counting down cancels the existing submission and restarts the countdown
N * using the new delay. If the work item is currently pending on the
N * workqueue's queue because the countdown has completed it is too late to
N * resubmit the item, and resubmission fails without impacting the work item.
N * If the work item has already been processed, or is currently being processed,
N * its work is considered complete and the work item can be resubmitted.
N *
N * @warning
N * Work items submitted to the system workqueue should avoid using handlers
N * that block or yield since this may prevent the system workqueue from
N * processing other work items in a timely manner.
N *
N * @note Can be called by ISRs.
N *
N * @param work Address of delayed work item.
N * @param delay Delay before submitting the work item (in milliseconds).
N *
N * @retval 0 Work item countdown started.
N * @retval -EINPROGRESS Work item is already pending.
N * @retval -EINVAL Work item is being processed or has completed its work.
N * @retval -EADDRINUSE Work item is pending on a different workqueue.
N */
Nstatic inline int k_delayed_work_submit(struct k_delayed_work *work,
N					s32_t delay)
N{
N	return k_delayed_work_submit_to_queue(&k_sys_work_q, work, delay);
N}
N
N/**
N * @brief Get time remaining before a delayed work gets scheduled.
N *
N * This routine computes the (approximate) time remaining before a
N * delayed work gets executed. If the delayed work is not waiting to be
N * schedules, it returns zero.
N *
N * @param work     Delayed work item.
N *
N * @return Remaining time (in milliseconds).
N */
Nstatic inline s32_t k_delayed_work_remaining_get(struct k_delayed_work *work)
N{
N	return _timeout_remaining_get(&work->timeout);
N}
N
N/**
N * @} end defgroup workqueue_apis
N */
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
Nstruct k_mutex {
N	_wait_q_t wait_q;
N	struct k_thread *owner;
N	u32_t lock_count;
N	int owner_orig_prio;
N
N//	_OBJECT_TRACING_NEXT_PTR(k_mutex);
N};
N
N#define _K_MUTEX_INITIALIZER(obj) \
N	{ \
N	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), \
N	.owner = NULL, \
N	.lock_count = 0, \
N	.owner_orig_prio = K_LOWEST_THREAD_PRIO, \
N	_OBJECT_TRACING_INIT \
N	}
X#define _K_MUTEX_INITIALIZER(obj) 	{ 	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), 	.owner = NULL, 	.lock_count = 0, 	.owner_orig_prio = K_LOWEST_THREAD_PRIO, 	_OBJECT_TRACING_INIT 	}
N
N#define K_MUTEX_INITIALIZER DEPRECATED_MACRO _K_MUTEX_INITIALIZER
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @defgroup mutex_apis Mutex APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @brief Statically define and initialize a mutex.
N *
N * The mutex can be accessed outside the module where it is defined using:
N *
N * @code extern struct k_mutex <name>; @endcode
N *
N * @param name Name of the mutex.
N */
N#define K_MUTEX_DEFINE(name) \
N	struct k_mutex name \
N		__in_section(_k_mutex, static, name) = \
N		_K_MUTEX_INITIALIZER(name)
X#define K_MUTEX_DEFINE(name) 	struct k_mutex name 		__in_section(_k_mutex, static, name) = 		_K_MUTEX_INITIALIZER(name)
N
N/**
N * @brief Initialize a mutex.
N *
N * This routine initializes a mutex object, prior to its first use.
N *
N * Upon completion, the mutex is available and does not have an owner.
N *
N * @param mutex Address of the mutex.
N *
N * @return N/A
N */
Nextern void k_mutex_init(struct k_mutex *mutex);
N
N/**
N * @brief Lock a mutex.
N *
N * This routine locks @a mutex. If the mutex is locked by another thread,
N * the calling thread waits until the mutex becomes available or until
N * a timeout occurs.
N *
N * A thread is permitted to lock a mutex it has already locked. The operation
N * completes immediately and the lock count is increased by 1.
N *
N * @param mutex Address of the mutex.
N * @param timeout Waiting period to lock the mutex (in milliseconds),
N *                or one of the special values K_NO_WAIT and K_FOREVER.
N *
N * @retval 0 Mutex locked.
N * @retval -EBUSY Returned without waiting.
N * @retval -EAGAIN Waiting period timed out.
N */
Nextern int k_mutex_lock(struct k_mutex *mutex, s32_t timeout);
N
N/**
N * @brief Unlock a mutex.
N *
N * This routine unlocks @a mutex. The mutex must already be locked by the
N * calling thread.
N *
N * The mutex cannot be claimed by another thread until it has been unlocked by
N * the calling thread as many times as it was previously locked by that
N * thread.
N *
N * @param mutex Address of the mutex.
N *
N * @return N/A
N */
Nextern void k_mutex_unlock(struct k_mutex *mutex);
N
N/**
N * @} end defgroup mutex_apis
N */
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
Nstruct k_sem {
N	_wait_q_t wait_q;
N	unsigned int count;
N	unsigned int limit;
N	_POLL_EVENT;
X	sys_dlist_t poll_events;
N
N//	_OBJECT_TRACING_NEXT_PTR(k_sem);
N};
N
N#define _K_SEM_INITIALIZER(obj, initial_count, count_limit) \
N	{ \
N	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), \
N	.count = initial_count, \
N	.limit = count_limit, \
N	_POLL_EVENT_OBJ_INIT(obj) \
N	_OBJECT_TRACING_INIT \
N	}
X#define _K_SEM_INITIALIZER(obj, initial_count, count_limit) 	{ 	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), 	.count = initial_count, 	.limit = count_limit, 	_POLL_EVENT_OBJ_INIT(obj) 	_OBJECT_TRACING_INIT 	}
N
N#define K_SEM_INITIALIZER DEPRECATED_MACRO _K_SEM_INITIALIZER
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @defgroup semaphore_apis Semaphore APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @brief Initialize a semaphore.
N *
N * This routine initializes a semaphore object, prior to its first use.
N *
N * @param sem Address of the semaphore.
N * @param initial_count Initial semaphore count.
N * @param limit Maximum permitted semaphore count.
N *
N * @return N/A
N */
Nextern void k_sem_init(struct k_sem *sem, unsigned int initial_count,
N			unsigned int limit);
N
N/**
N * @brief Take a semaphore.
N *
N * This routine takes @a sem.
N *
N * @note Can be called by ISRs, but @a timeout must be set to K_NO_WAIT.
N *
N * @param sem Address of the semaphore.
N * @param timeout Waiting period to take the semaphore (in milliseconds),
N *                or one of the special values K_NO_WAIT and K_FOREVER.
N *
N * @note When porting code from the nanokernel legacy API to the new API, be
N * careful with the return value of this function. The return value is the
N * reverse of the one of nano_sem_take family of APIs: 0 means success, and
N * non-zero means failure, while the nano_sem_take family returns 1 for success
N * and 0 for failure.
N *
N * @retval 0 Semaphore taken.
N * @retval -EBUSY Returned without waiting.
N * @retval -EAGAIN Waiting period timed out.
N */
Nextern int k_sem_take(struct k_sem *sem, s32_t timeout);
N
N/**
N * @brief Give a semaphore.
N *
N * This routine gives @a sem, unless the semaphore is already at its maximum
N * permitted count.
N *
N * @note Can be called by ISRs.
N *
N * @param sem Address of the semaphore.
N *
N * @return N/A
N */
Nextern void k_sem_give(struct k_sem *sem);
N
N/**
N * @brief Reset a semaphore's count to zero.
N *
N * This routine sets the count of @a sem to zero.
N *
N * @param sem Address of the semaphore.
N *
N * @return N/A
N */
Nstatic inline void k_sem_reset(struct k_sem *sem)
N{
N	sem->count = 0;
N}
N
N/**
N * @brief Get a semaphore's count.
N *
N * This routine returns the current count of @a sem.
N *
N * @param sem Address of the semaphore.
N *
N * @return Current semaphore count.
N */
Nstatic inline unsigned int k_sem_count_get(struct k_sem *sem)
N{
N	return sem->count;
N}
N
N/**
N * @brief Statically define and initialize a semaphore.
N *
N * The semaphore can be accessed outside the module where it is defined using:
N *
N * @code extern struct k_sem <name>; @endcode
N *
N * @param name Name of the semaphore.
N * @param initial_count Initial semaphore count.
N * @param count_limit Maximum permitted semaphore count.
N */
N#define K_SEM_DEFINE(name, initial_count, count_limit) \
N	struct k_sem name \
N		__in_section(_k_sem, static, name) = \
N		_K_SEM_INITIALIZER(name, initial_count, count_limit)
X#define K_SEM_DEFINE(name, initial_count, count_limit) 	struct k_sem name 		__in_section(_k_sem, static, name) = 		_K_SEM_INITIALIZER(name, initial_count, count_limit)
N
N/**
N * @} end defgroup semaphore_apis
N */
N
N/**
N * @defgroup alert_apis Alert APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @typedef k_alert_handler_t
N * @brief Alert handler function type.
N *
N * An alert's alert handler function is invoked by the system workqueue
N * when the alert is signaled. The alert handler function is optional,
N * and is only invoked if the alert has been initialized with one.
N *
N * @param alert Address of the alert.
N *
N * @return 0 if alert has been consumed; non-zero if alert should pend.
N */
Ntypedef int (*k_alert_handler_t)(struct k_alert *alert);
N
N/**
N * @} end defgroup alert_apis
N */
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
N#define K_ALERT_DEFAULT NULL
N#define K_ALERT_IGNORE ((void *)(-1))
N
Nstruct k_alert {
N	k_alert_handler_t handler;
N	atomic_t send_count;
N	struct k_work work_item;
N	struct k_sem sem;
N
N//	_OBJECT_TRACING_NEXT_PTR(k_alert);
N};
N
Nextern void _alert_deliver(struct k_work *work);
N
N#define _K_ALERT_INITIALIZER(obj, alert_handler, max_num_pending_alerts) \
N	{ \
N	.handler = (k_alert_handler_t)alert_handler, \
N	.send_count = ATOMIC_INIT(0), \
N	.work_item = _K_WORK_INITIALIZER(_alert_deliver), \
N	.sem = _K_SEM_INITIALIZER(obj.sem, 0, max_num_pending_alerts), \
N	_OBJECT_TRACING_INIT \
N	}
X#define _K_ALERT_INITIALIZER(obj, alert_handler, max_num_pending_alerts) 	{ 	.handler = (k_alert_handler_t)alert_handler, 	.send_count = ATOMIC_INIT(0), 	.work_item = _K_WORK_INITIALIZER(_alert_deliver), 	.sem = _K_SEM_INITIALIZER(obj.sem, 0, max_num_pending_alerts), 	_OBJECT_TRACING_INIT 	}
N
N#define K_ALERT_INITIALIZER DEPRECATED_MACRO _K_ALERT_INITIALIZER
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @addtogroup alert_apis
N * @{
N */
N
N/**
N * @brief Statically define and initialize an alert.
N *
N * The alert can be accessed outside the module where it is defined using:
N *
N * @code extern struct k_alert <name>; @endcode
N *
N * @param name Name of the alert.
N * @param alert_handler Action to take when alert is sent. Specify either
N *        the address of a function to be invoked by the system workqueue
N *        thread, K_ALERT_IGNORE (which causes the alert to be ignored), or
N *        K_ALERT_DEFAULT (which causes the alert to pend).
N * @param max_num_pending_alerts Maximum number of pending alerts.
N */
N#define K_ALERT_DEFINE(name, alert_handler, max_num_pending_alerts) \
N	struct k_alert name \
N		__in_section(_k_alert, static, name) = \
N		_K_ALERT_INITIALIZER(name, alert_handler, \
N				    max_num_pending_alerts)
X#define K_ALERT_DEFINE(name, alert_handler, max_num_pending_alerts) 	struct k_alert name 		__in_section(_k_alert, static, name) = 		_K_ALERT_INITIALIZER(name, alert_handler, 				    max_num_pending_alerts)
N
N/**
N * @brief Initialize an alert.
N *
N * This routine initializes an alert object, prior to its first use.
N *
N * @param alert Address of the alert.
N * @param handler Action to take when alert is sent. Specify either the address
N *                of a function to be invoked by the system workqueue thread,
N *                K_ALERT_IGNORE (which causes the alert to be ignored), or
N *                K_ALERT_DEFAULT (which causes the alert to pend).
N * @param max_num_pending_alerts Maximum number of pending alerts.
N *
N * @return N/A
N */
Nextern void k_alert_init(struct k_alert *alert, k_alert_handler_t handler,
N			 unsigned int max_num_pending_alerts);
N
N/**
N * @brief Receive an alert.
N *
N * This routine receives a pending alert for @a alert.
N *
N * @note Can be called by ISRs, but @a timeout must be set to K_NO_WAIT.
N *
N * @param alert Address of the alert.
N * @param timeout Waiting period to receive the alert (in milliseconds),
N *                or one of the special values K_NO_WAIT and K_FOREVER.
N *
N * @retval 0 Alert received.
N * @retval -EBUSY Returned without waiting.
N * @retval -EAGAIN Waiting period timed out.
N */
Nextern int k_alert_recv(struct k_alert *alert, s32_t timeout);
N
N/**
N * @brief Signal an alert.
N *
N * This routine signals @a alert. The action specified for @a alert will
N * be taken, which may trigger the execution of an alert handler function
N * and/or cause the alert to pend (assuming the alert has not reached its
N * maximum number of pending alerts).
N *
N * @note Can be called by ISRs.
N *
N * @param alert Address of the alert.
N *
N * @return N/A
N */
Nextern void k_alert_send(struct k_alert *alert);
N
N/**
N * @} end addtogroup alert_apis
N */
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
Nstruct k_msgq {
N	_wait_q_t wait_q;
N	size_t msg_size;
N	u32_t max_msgs;
N	char *buffer_start;
N	char *buffer_end;
N	char *read_ptr;
N	char *write_ptr;
N	u32_t used_msgs;
N
N//	_OBJECT_TRACING_NEXT_PTR(k_msgq);
N};
N
N#define _K_MSGQ_INITIALIZER(obj, q_buffer, q_msg_size, q_max_msgs) \
N	{ \
N	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), \
N	.max_msgs = q_max_msgs, \
N	.msg_size = q_msg_size, \
N	.buffer_start = q_buffer, \
N	.buffer_end = q_buffer + (q_max_msgs * q_msg_size), \
N	.read_ptr = q_buffer, \
N	.write_ptr = q_buffer, \
N	.used_msgs = 0, \
N	_OBJECT_TRACING_INIT \
N	}
X#define _K_MSGQ_INITIALIZER(obj, q_buffer, q_msg_size, q_max_msgs) 	{ 	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), 	.max_msgs = q_max_msgs, 	.msg_size = q_msg_size, 	.buffer_start = q_buffer, 	.buffer_end = q_buffer + (q_max_msgs * q_msg_size), 	.read_ptr = q_buffer, 	.write_ptr = q_buffer, 	.used_msgs = 0, 	_OBJECT_TRACING_INIT 	}
N
N#define K_MSGQ_INITIALIZER DEPRECATED_MACRO _K_MSGQ_INITIALIZER
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @defgroup msgq_apis Message Queue APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @brief Statically define and initialize a message queue.
N *
N * The message queue's ring buffer contains space for @a q_max_msgs messages,
N * each of which is @a q_msg_size bytes long. The buffer is aligned to a
N * @a q_align -byte boundary, which must be a power of 2. To ensure that each
N * message is similarly aligned to this boundary, @a q_msg_size must also be
N * a multiple of @a q_align.
N *
N * The message queue can be accessed outside the module where it is defined
N * using:
N *
N * @code extern struct k_msgq <name>; @endcode
N *
N * @param q_name Name of the message queue.
N * @param q_msg_size Message size (in bytes).
N * @param q_max_msgs Maximum number of messages that can be queued.
N * @param q_align Alignment of the message queue's ring buffer.
N */
N#define K_MSGQ_DEFINE(q_name, q_msg_size, q_max_msgs, q_align)      \
N	static char __noinit __aligned(q_align)                     \
N		_k_fifo_buf_##q_name[(q_max_msgs) * (q_msg_size)];  \
N	struct k_msgq q_name                                        \
N		__in_section(_k_msgq, static, q_name) =        \
N	       _K_MSGQ_INITIALIZER(q_name, _k_fifo_buf_##q_name,     \
N				  q_msg_size, q_max_msgs)
X#define K_MSGQ_DEFINE(q_name, q_msg_size, q_max_msgs, q_align)      	static char __noinit __aligned(q_align)                     		_k_fifo_buf_##q_name[(q_max_msgs) * (q_msg_size)];  	struct k_msgq q_name                                        		__in_section(_k_msgq, static, q_name) =        	       _K_MSGQ_INITIALIZER(q_name, _k_fifo_buf_##q_name,     				  q_msg_size, q_max_msgs)
N
N/**
N * @brief Initialize a message queue.
N *
N * This routine initializes a message queue object, prior to its first use.
N *
N * The message queue's ring buffer must contain space for @a max_msgs messages,
N * each of which is @a msg_size bytes long. The buffer must be aligned to an
N * N-byte boundary, where N is a power of 2 (i.e. 1, 2, 4, ...). To ensure
N * that each message is similarly aligned to this boundary, @a q_msg_size
N * must also be a multiple of N.
N *
N * @param q Address of the message queue.
N * @param buffer Pointer to ring buffer that holds queued messages.
N * @param msg_size Message size (in bytes).
N * @param max_msgs Maximum number of messages that can be queued.
N *
N * @return N/A
N */
Nextern void k_msgq_init(struct k_msgq *q, char *buffer,
N			size_t msg_size, u32_t max_msgs);
N
N/**
N * @brief Send a message to a message queue.
N *
N * This routine sends a message to message queue @a q.
N *
N * @note Can be called by ISRs.
N *
N * @param q Address of the message queue.
N * @param data Pointer to the message.
N * @param timeout Waiting period to add the message (in milliseconds),
N *                or one of the special values K_NO_WAIT and K_FOREVER.
N *
N * @retval 0 Message sent.
N * @retval -ENOMSG Returned without waiting or queue purged.
N * @retval -EAGAIN Waiting period timed out.
N */
Nextern int k_msgq_put(struct k_msgq *q, void *data, s32_t timeout);
N
N/**
N * @brief Receive a message from a message queue.
N *
N * This routine receives a message from message queue @a q in a "first in,
N * first out" manner.
N *
N * @note Can be called by ISRs, but @a timeout must be set to K_NO_WAIT.
N *
N * @param q Address of the message queue.
N * @param data Address of area to hold the received message.
N * @param timeout Waiting period to receive the message (in milliseconds),
N *                or one of the special values K_NO_WAIT and K_FOREVER.
N *
N * @retval 0 Message received.
N * @retval -ENOMSG Returned without waiting.
N * @retval -EAGAIN Waiting period timed out.
N */
Nextern int k_msgq_get(struct k_msgq *q, void *data, s32_t timeout);
N
N/**
N * @brief Purge a message queue.
N *
N * This routine discards all unreceived messages in a message queue's ring
N * buffer. Any threads that are blocked waiting to send a message to the
N * message queue are unblocked and see an -ENOMSG error code.
N *
N * @param q Address of the message queue.
N *
N * @return N/A
N */
Nextern void k_msgq_purge(struct k_msgq *q);
N
N/**
N * @brief Get the amount of free space in a message queue.
N *
N * This routine returns the number of unused entries in a message queue's
N * ring buffer.
N *
N * @param q Address of the message queue.
N *
N * @return Number of unused ring buffer entries.
N */
Nstatic inline u32_t k_msgq_num_free_get(struct k_msgq *q)
N{
N	return q->max_msgs - q->used_msgs;
N}
N
N/**
N * @brief Get the number of messages in a message queue.
N *
N * This routine returns the number of messages in a message queue's ring buffer.
N *
N * @param q Address of the message queue.
N *
N * @return Number of messages.
N */
Nstatic inline u32_t k_msgq_num_used_get(struct k_msgq *q)
N{
N	return q->used_msgs;
N}
N
N/**
N * @} end defgroup msgq_apis
N */
N
N/**
N * @defgroup mem_pool_apis Memory Pool APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/* Note on sizing: the use of a 20 bit field for block means that,
N * assuming a reasonable minimum block size of 16 bytes, we're limited
N * to 16M of memory managed by a single pool.  Long term it would be
N * good to move to a variable bit size based on configuration.
N */
Nstruct k_mem_block_id {
N	u32_t pool : 8;
N	u32_t level : 4;
N	u32_t block : 20;
N};
N
Nstruct k_mem_block {
N	void *data;
N	struct k_mem_block_id id;
N};
N
N/**
N * @} end defgroup mem_pool_apis
N */
N
N/**
N * @defgroup mailbox_apis Mailbox APIs
N * @ingroup kernel_apis
N * @{
N */
N
Nstruct k_mbox_msg {
N	/** internal use only - needed for legacy API support */
N	u32_t _mailbox;
N	/** size of message (in bytes) */
N	size_t size;
N	/** application-defined information value */
N	u32_t info;
N	/** sender's message data buffer */
N	void *tx_data;
N	/** internal use only - needed for legacy API support */
N	void *_rx_data;
N	/** message data block descriptor */
N	struct k_mem_block tx_block;
N	/** source thread id */
N	k_tid_t rx_source_thread;
N	/** target thread id */
N	k_tid_t tx_target_thread;
N	/** internal use only - thread waiting on send (may be a dummy) */
N	k_tid_t _syncing_thread;
N#if (CONFIG_NUM_MBOX_ASYNC_MSGS > 0)
X#if (10 > 0)
N	/** internal use only - semaphore used during asynchronous send */
N	struct k_sem *_async_sem;
N#endif
N};
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
Nstruct k_mbox {
N	_wait_q_t tx_msg_queue;
N	_wait_q_t rx_msg_queue;
N
N//	_OBJECT_TRACING_NEXT_PTR(k_mbox);
N};
N
N#define _K_MBOX_INITIALIZER(obj) \
N	{ \
N	.tx_msg_queue = SYS_DLIST_STATIC_INIT(&obj.tx_msg_queue), \
N	.rx_msg_queue = SYS_DLIST_STATIC_INIT(&obj.rx_msg_queue), \
N	_OBJECT_TRACING_INIT \
N	}
X#define _K_MBOX_INITIALIZER(obj) 	{ 	.tx_msg_queue = SYS_DLIST_STATIC_INIT(&obj.tx_msg_queue), 	.rx_msg_queue = SYS_DLIST_STATIC_INIT(&obj.rx_msg_queue), 	_OBJECT_TRACING_INIT 	}
N
N#define K_MBOX_INITIALIZER DEPRECATED_MACRO _K_MBOX_INITIALIZER
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @brief Statically define and initialize a mailbox.
N *
N * The mailbox is to be accessed outside the module where it is defined using:
N *
N * @code extern struct k_mbox <name>; @endcode
N *
N * @param name Name of the mailbox.
N */
N#define K_MBOX_DEFINE(name) \
N	struct k_mbox name \
N		__in_section(_k_mbox, static, name) = \
N		_K_MBOX_INITIALIZER(name) \
N
X#define K_MBOX_DEFINE(name) 	struct k_mbox name 		__in_section(_k_mbox, static, name) = 		_K_MBOX_INITIALIZER(name) 
N/**
N * @brief Initialize a mailbox.
N *
N * This routine initializes a mailbox object, prior to its first use.
N *
N * @param mbox Address of the mailbox.
N *
N * @return N/A
N */
Nextern void k_mbox_init(struct k_mbox *mbox);
N
N/**
N * @brief Send a mailbox message in a synchronous manner.
N *
N * This routine sends a message to @a mbox and waits for a receiver to both
N * receive and process it. The message data may be in a buffer, in a memory
N * pool block, or non-existent (i.e. an empty message).
N *
N * @param mbox Address of the mailbox.
N * @param tx_msg Address of the transmit message descriptor.
N * @param timeout Waiting period for the message to be received (in
N *                milliseconds), or one of the special values K_NO_WAIT
N *                and K_FOREVER. Once the message has been received,
N *                this routine waits as long as necessary for the message
N *                to be completely processed.
N *
N * @retval 0 Message sent.
N * @retval -ENOMSG Returned without waiting.
N * @retval -EAGAIN Waiting period timed out.
N */
Nextern int k_mbox_put(struct k_mbox *mbox, struct k_mbox_msg *tx_msg,
N		      s32_t timeout);
N
N/**
N * @brief Send a mailbox message in an asynchronous manner.
N *
N * This routine sends a message to @a mbox without waiting for a receiver
N * to process it. The message data may be in a buffer, in a memory pool block,
N * or non-existent (i.e. an empty message). Optionally, the semaphore @a sem
N * will be given when the message has been both received and completely
N * processed by the receiver.
N *
N * @param mbox Address of the mailbox.
N * @param tx_msg Address of the transmit message descriptor.
N * @param sem Address of a semaphore, or NULL if none is needed.
N *
N * @return N/A
N */
Nextern void k_mbox_async_put(struct k_mbox *mbox, struct k_mbox_msg *tx_msg,
N			     struct k_sem *sem);
N
N/**
N * @brief Receive a mailbox message.
N *
N * This routine receives a message from @a mbox, then optionally retrieves
N * its data and disposes of the message.
N *
N * @param mbox Address of the mailbox.
N * @param rx_msg Address of the receive message descriptor.
N * @param buffer Address of the buffer to receive data, or NULL to defer data
N *               retrieval and message disposal until later.
N * @param timeout Waiting period for a message to be received (in
N *                milliseconds), or one of the special values K_NO_WAIT
N *                and K_FOREVER.
N *
N * @retval 0 Message received.
N * @retval -ENOMSG Returned without waiting.
N * @retval -EAGAIN Waiting period timed out.
N */
Nextern int k_mbox_get(struct k_mbox *mbox, struct k_mbox_msg *rx_msg,
N		      void *buffer, s32_t timeout);
N
N/**
N * @brief Retrieve mailbox message data into a buffer.
N *
N * This routine completes the processing of a received message by retrieving
N * its data into a buffer, then disposing of the message.
N *
N * Alternatively, this routine can be used to dispose of a received message
N * without retrieving its data.
N *
N * @param rx_msg Address of the receive message descriptor.
N * @param buffer Address of the buffer to receive data, or NULL to discard
N *               the data.
N *
N * @return N/A
N */
Nextern void k_mbox_data_get(struct k_mbox_msg *rx_msg, void *buffer);
N
N/**
N * @brief Retrieve mailbox message data into a memory pool block.
N *
N * This routine completes the processing of a received message by retrieving
N * its data into a memory pool block, then disposing of the message.
N * The memory pool block that results from successful retrieval must be
N * returned to the pool once the data has been processed, even in cases
N * where zero bytes of data are retrieved.
N *
N * Alternatively, this routine can be used to dispose of a received message
N * without retrieving its data. In this case there is no need to return a
N * memory pool block to the pool.
N *
N * This routine allocates a new memory pool block for the data only if the
N * data is not already in one. If a new block cannot be allocated, the routine
N * returns a failure code and the received message is left unchanged. This
N * permits the caller to reattempt data retrieval at a later time or to dispose
N * of the received message without retrieving its data.
N *
N * @param rx_msg Address of a receive message descriptor.
N * @param pool Address of memory pool, or NULL to discard data.
N * @param block Address of the area to hold memory pool block info.
N * @param timeout Waiting period to wait for a memory pool block (in
N *                milliseconds), or one of the special values K_NO_WAIT
N *                and K_FOREVER.
N *
N * @retval 0 Data retrieved.
N * @retval -ENOMEM Returned without waiting.
N * @retval -EAGAIN Waiting period timed out.
N */
Nextern int k_mbox_data_block_get(struct k_mbox_msg *rx_msg,
N				 struct k_mem_pool *pool,
N				 struct k_mem_block *block, s32_t timeout);
N
N/**
N * @} end defgroup mailbox_apis
N */
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
Nstruct k_pipe {
N	unsigned char *buffer;          /* Pipe buffer: may be NULL */
N	size_t         size;            /* Buffer size */
N	size_t         bytes_used;      /* # bytes used in buffer */
N	size_t         read_index;      /* Where in buffer to read from */
N	size_t         write_index;     /* Where in buffer to write */
N
N	struct {
N		_wait_q_t      readers; /* Reader wait queue */
N		_wait_q_t      writers; /* Writer wait queue */
N	} wait_q;
N
N//	_OBJECT_TRACING_NEXT_PTR(k_pipe);
N};
N
N#define _K_PIPE_INITIALIZER(obj, pipe_buffer, pipe_buffer_size)        \
N	{                                                             \
N	.buffer = pipe_buffer,                                        \
N	.size = pipe_buffer_size,                                     \
N	.bytes_used = 0,                                              \
N	.read_index = 0,                                              \
N	.write_index = 0,                                             \
N	.wait_q.writers = SYS_DLIST_STATIC_INIT(&obj.wait_q.writers), \
N	.wait_q.readers = SYS_DLIST_STATIC_INIT(&obj.wait_q.readers), \
N	_OBJECT_TRACING_INIT                            \
N	}
X#define _K_PIPE_INITIALIZER(obj, pipe_buffer, pipe_buffer_size)        	{                                                             	.buffer = pipe_buffer,                                        	.size = pipe_buffer_size,                                     	.bytes_used = 0,                                              	.read_index = 0,                                              	.write_index = 0,                                             	.wait_q.writers = SYS_DLIST_STATIC_INIT(&obj.wait_q.writers), 	.wait_q.readers = SYS_DLIST_STATIC_INIT(&obj.wait_q.readers), 	_OBJECT_TRACING_INIT                            	}
N
N#define K_PIPE_INITIALIZER DEPRECATED_MACRO _K_PIPE_INITIALIZER
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @defgroup pipe_apis Pipe APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @brief Statically define and initialize a pipe.
N *
N * The pipe can be accessed outside the module where it is defined using:
N *
N * @code extern struct k_pipe <name>; @endcode
N *
N * @param name Name of the pipe.
N * @param pipe_buffer_size Size of the pipe's ring buffer (in bytes),
N *                         or zero if no ring buffer is used.
N * @param pipe_align Alignment of the pipe's ring buffer (power of 2).
N */
N#define K_PIPE_DEFINE(name, pipe_buffer_size, pipe_align)     \
N	static unsigned char __noinit __aligned(pipe_align)   \
N		_k_pipe_buf_##name[pipe_buffer_size];         \
N	struct k_pipe name                                    \
N		__in_section(_k_pipe, static, name) =    \
N		_K_PIPE_INITIALIZER(name, _k_pipe_buf_##name, pipe_buffer_size)
X#define K_PIPE_DEFINE(name, pipe_buffer_size, pipe_align)     	static unsigned char __noinit __aligned(pipe_align)   		_k_pipe_buf_##name[pipe_buffer_size];         	struct k_pipe name                                    		__in_section(_k_pipe, static, name) =    		_K_PIPE_INITIALIZER(name, _k_pipe_buf_##name, pipe_buffer_size)
N
N/**
N * @brief Initialize a pipe.
N *
N * This routine initializes a pipe object, prior to its first use.
N *
N * @param pipe Address of the pipe.
N * @param buffer Address of the pipe's ring buffer, or NULL if no ring buffer
N *               is used.
N * @param size Size of the pipe's ring buffer (in bytes), or zero if no ring
N *             buffer is used.
N *
N * @return N/A
N */
Nextern void k_pipe_init(struct k_pipe *pipe, unsigned char *buffer,
N			size_t size);
N
N/**
N * @brief Write data to a pipe.
N *
N * This routine writes up to @a bytes_to_write bytes of data to @a pipe.
N *
N * @param pipe Address of the pipe.
N * @param data Address of data to write.
N * @param bytes_to_write Size of data (in bytes).
N * @param bytes_written Address of area to hold the number of bytes written.
N * @param min_xfer Minimum number of bytes to write.
N * @param timeout Waiting period to wait for the data to be written (in
N *                milliseconds), or one of the special values K_NO_WAIT
N *                and K_FOREVER.
N *
N * @retval 0 At least @a min_xfer bytes of data were written.
N * @retval -EIO Returned without waiting; zero data bytes were written.
N * @retval -EAGAIN Waiting period timed out; between zero and @a min_xfer
N *                 minus one data bytes were written.
N */
Nextern int k_pipe_put(struct k_pipe *pipe, void *data,
N		      size_t bytes_to_write, size_t *bytes_written,
N		      size_t min_xfer, s32_t timeout);
N
N/**
N * @brief Read data from a pipe.
N *
N * This routine reads up to @a bytes_to_read bytes of data from @a pipe.
N *
N * @param pipe Address of the pipe.
N * @param data Address to place the data read from pipe.
N * @param bytes_to_read Maximum number of data bytes to read.
N * @param bytes_read Address of area to hold the number of bytes read.
N * @param min_xfer Minimum number of data bytes to read.
N * @param timeout Waiting period to wait for the data to be read (in
N *                milliseconds), or one of the special values K_NO_WAIT
N *                and K_FOREVER.
N *
N * @retval 0 At least @a min_xfer bytes of data were read.
N * @retval -EIO Returned without waiting; zero data bytes were read.
N * @retval -EAGAIN Waiting period timed out; between zero and @a min_xfer
N *                 minus one data bytes were read.
N */
Nextern int k_pipe_get(struct k_pipe *pipe, void *data,
N		      size_t bytes_to_read, size_t *bytes_read,
N		      size_t min_xfer, s32_t timeout);
N
N/**
N * @brief Write memory block to a pipe.
N *
N * This routine writes the data contained in a memory block to @a pipe.
N * Once all of the data in the block has been written to the pipe, it will
N * free the memory block @a block and give the semaphore @a sem (if specified).
N *
N * @param pipe Address of the pipe.
N * @param block Memory block containing data to send
N * @param size Number of data bytes in memory block to send
N * @param sem Semaphore to signal upon completion (else NULL)
N *
N * @return N/A
N */
Nextern void k_pipe_block_put(struct k_pipe *pipe, struct k_mem_block *block,
N			     size_t size, struct k_sem *sem);
N
N/**
N * @} end defgroup pipe_apis
N */
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
Nstruct k_mem_slab {
N	_wait_q_t wait_q;
N	u32_t num_blocks;
N	size_t block_size;
N	char *buffer;
N	char *free_list;
N	u32_t num_used;
N
N//	_OBJECT_TRACING_NEXT_PTR(k_mem_slab);
N};
N
N#define _K_MEM_SLAB_INITIALIZER(obj, slab_buffer, slab_block_size, \
N			       slab_num_blocks) \
N	{ \
N	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), \
N	.num_blocks = slab_num_blocks, \
N	.block_size = slab_block_size, \
N	.buffer = slab_buffer, \
N	.free_list = NULL, \
N	.num_used = 0, \
N	_OBJECT_TRACING_INIT \
N	}
X#define _K_MEM_SLAB_INITIALIZER(obj, slab_buffer, slab_block_size, 			       slab_num_blocks) 	{ 	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), 	.num_blocks = slab_num_blocks, 	.block_size = slab_block_size, 	.buffer = slab_buffer, 	.free_list = NULL, 	.num_used = 0, 	_OBJECT_TRACING_INIT 	}
N
N#define K_MEM_SLAB_INITIALIZER DEPRECATED_MACRO _K_MEM_SLAB_INITIALIZER
N
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @defgroup mem_slab_apis Memory Slab APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @brief Statically define and initialize a memory slab.
N *
N * The memory slab's buffer contains @a slab_num_blocks memory blocks
N * that are @a slab_block_size bytes long. The buffer is aligned to a
N * @a slab_align -byte boundary. To ensure that each memory block is similarly
N * aligned to this boundary, @a slab_block_size must also be a multiple of
N * @a slab_align.
N *
N * The memory slab can be accessed outside the module where it is defined
N * using:
N *
N * @code extern struct k_mem_slab <name>; @endcode
N *
N * @param name Name of the memory slab.
N * @param slab_block_size Size of each memory block (in bytes).
N * @param slab_num_blocks Number memory blocks.
N * @param slab_align Alignment of the memory slab's buffer (power of 2).
N */
N#define K_MEM_SLAB_DEFINE(name, slab_block_size, slab_num_blocks, slab_align) \
N	char __noinit __aligned(slab_align) \
N		_k_mem_slab_buf_##name[(slab_num_blocks) * (slab_block_size)]; \
N	struct k_mem_slab name \
N		__in_section(_k_mem_slab, static, name) = \
N		_K_MEM_SLAB_INITIALIZER(name, _k_mem_slab_buf_##name, \
N				      slab_block_size, slab_num_blocks)
X#define K_MEM_SLAB_DEFINE(name, slab_block_size, slab_num_blocks, slab_align) 	char __noinit __aligned(slab_align) 		_k_mem_slab_buf_##name[(slab_num_blocks) * (slab_block_size)]; 	struct k_mem_slab name 		__in_section(_k_mem_slab, static, name) = 		_K_MEM_SLAB_INITIALIZER(name, _k_mem_slab_buf_##name, 				      slab_block_size, slab_num_blocks)
N
N/**
N * @brief Initialize a memory slab.
N *
N * Initializes a memory slab, prior to its first use.
N *
N * The memory slab's buffer contains @a slab_num_blocks memory blocks
N * that are @a slab_block_size bytes long. The buffer must be aligned to an
N * N-byte boundary, where N is a power of 2 larger than 2 (i.e. 4, 8, 16, ...).
N * To ensure that each memory block is similarly aligned to this boundary,
N * @a slab_block_size must also be a multiple of N.
N *
N * @param slab Address of the memory slab.
N * @param buffer Pointer to buffer used for the memory blocks.
N * @param block_size Size of each memory block (in bytes).
N * @param num_blocks Number of memory blocks.
N *
N * @return N/A
N */
Nextern void k_mem_slab_init(struct k_mem_slab *slab, void *buffer,
N			   size_t block_size, u32_t num_blocks);
N
N/**
N * @brief Allocate memory from a memory slab.
N *
N * This routine allocates a memory block from a memory slab.
N *
N * @param slab Address of the memory slab.
N * @param mem Pointer to block address area.
N * @param timeout Maximum time to wait for operation to complete
N *        (in milliseconds). Use K_NO_WAIT to return without waiting,
N *        or K_FOREVER to wait as long as necessary.
N *
N * @retval 0 Memory allocated. The block address area pointed at by @a mem
N *         is set to the starting address of the memory block.
N * @retval -ENOMEM Returned without waiting.
N * @retval -EAGAIN Waiting period timed out.
N */
Nextern int k_mem_slab_alloc(struct k_mem_slab *slab, void **mem,
N			    s32_t timeout);
N
N/**
N * @brief Free memory allocated from a memory slab.
N *
N * This routine releases a previously allocated memory block back to its
N * associated memory slab.
N *
N * @param slab Address of the memory slab.
N * @param mem Pointer to block address area (as set by k_mem_slab_alloc()).
N *
N * @return N/A
N */
Nextern void k_mem_slab_free(struct k_mem_slab *slab, void **mem);
N
N/**
N * @brief Get the number of used blocks in a memory slab.
N *
N * This routine gets the number of memory blocks that are currently
N * allocated in @a slab.
N *
N * @param slab Address of the memory slab.
N *
N * @return Number of allocated memory blocks.
N */
Nstatic inline u32_t k_mem_slab_num_used_get(struct k_mem_slab *slab)
N{
N	return slab->num_used;
N}
N
N/**
N * @brief Get the number of unused blocks in a memory slab.
N *
N * This routine gets the number of memory blocks that are currently
N * unallocated in @a slab.
N *
N * @param slab Address of the memory slab.
N *
N * @return Number of unallocated memory blocks.
N */
Nstatic inline u32_t k_mem_slab_num_free_get(struct k_mem_slab *slab)
N{
N	return slab->num_blocks - slab->num_used;
N}
N
N/**
N * @} end defgroup mem_slab_apis
N */
N
N/**
N * @cond INTERNAL_HIDDEN
N */
N
Nstruct k_mem_pool_lvl {
N	union {
N		u32_t *bits_p;
N		u32_t bits;
N	};
N	sys_dlist_t free_list;
N};
N
Nstruct k_mem_pool {
N	void *buf;
N	size_t max_sz;
N	u16_t n_max;
N	u8_t n_levels;
N	u8_t max_inline_level;
N	struct k_mem_pool_lvl *levels;
N	_wait_q_t wait_q;
N};
N
N#define _ALIGN4(n) ((((n)+3)/4)*4)
N
N#define _MPOOL_HAVE_LVL(max, min, l) (((max) >> (2*(l))) >= (min) ? 1 : 0)
N
N#define _MPOOL_LVLS(maxsz, minsz)		\
N	(_MPOOL_HAVE_LVL(maxsz, minsz, 0) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 1) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 2) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 3) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 4) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 5) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 6) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 7) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 8) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 9) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 10) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 11) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 12) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 13) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 14) +	\
N	_MPOOL_HAVE_LVL(maxsz, minsz, 15))
X#define _MPOOL_LVLS(maxsz, minsz)			(_MPOOL_HAVE_LVL(maxsz, minsz, 0) +		_MPOOL_HAVE_LVL(maxsz, minsz, 1) +		_MPOOL_HAVE_LVL(maxsz, minsz, 2) +		_MPOOL_HAVE_LVL(maxsz, minsz, 3) +		_MPOOL_HAVE_LVL(maxsz, minsz, 4) +		_MPOOL_HAVE_LVL(maxsz, minsz, 5) +		_MPOOL_HAVE_LVL(maxsz, minsz, 6) +		_MPOOL_HAVE_LVL(maxsz, minsz, 7) +		_MPOOL_HAVE_LVL(maxsz, minsz, 8) +		_MPOOL_HAVE_LVL(maxsz, minsz, 9) +		_MPOOL_HAVE_LVL(maxsz, minsz, 10) +		_MPOOL_HAVE_LVL(maxsz, minsz, 11) +		_MPOOL_HAVE_LVL(maxsz, minsz, 12) +		_MPOOL_HAVE_LVL(maxsz, minsz, 13) +		_MPOOL_HAVE_LVL(maxsz, minsz, 14) +		_MPOOL_HAVE_LVL(maxsz, minsz, 15))
N
N/* Rounds the needed bits up to integer multiples of u32_t */
N#define _MPOOL_LBIT_WORDS_UNCLAMPED(n_max, l) \
N	((((n_max) << (2*(l))) + 31) / 32)
X#define _MPOOL_LBIT_WORDS_UNCLAMPED(n_max, l) 	((((n_max) << (2*(l))) + 31) / 32)
N
N/* One word gets stored free unioned with the pointer, otherwise the
N * calculated unclamped value
N */
N#define _MPOOL_LBIT_WORDS(n_max, l)			\
N	(_MPOOL_LBIT_WORDS_UNCLAMPED(n_max, l) < 2 ? 0	\
N	 : _MPOOL_LBIT_WORDS_UNCLAMPED(n_max, l))
X#define _MPOOL_LBIT_WORDS(n_max, l)				(_MPOOL_LBIT_WORDS_UNCLAMPED(n_max, l) < 2 ? 0		 : _MPOOL_LBIT_WORDS_UNCLAMPED(n_max, l))
N
N/* How many bytes for the bitfields of a single level? */
N#define _MPOOL_LBIT_BYTES(maxsz, minsz, l, n_max)	\
N	(_MPOOL_LVLS((maxsz), (minsz)) >= (l) ?		\
N	 4 * _MPOOL_LBIT_WORDS((n_max), l) : 0)
X#define _MPOOL_LBIT_BYTES(maxsz, minsz, l, n_max)		(_MPOOL_LVLS((maxsz), (minsz)) >= (l) ?			 4 * _MPOOL_LBIT_WORDS((n_max), l) : 0)
N
N/* Size of the bitmap array that follows the buffer in allocated memory */
N#define _MPOOL_BITS_SIZE(maxsz, minsz, n_max) \
N	(_MPOOL_LBIT_BYTES(maxsz, minsz, 0, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 1, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 2, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 3, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 4, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 5, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 6, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 7, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 8, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 9, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 10, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 11, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 12, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 13, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 14, n_max) +	\
N	_MPOOL_LBIT_BYTES(maxsz, minsz, 15, n_max))
X#define _MPOOL_BITS_SIZE(maxsz, minsz, n_max) 	(_MPOOL_LBIT_BYTES(maxsz, minsz, 0, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 1, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 2, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 3, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 4, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 5, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 6, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 7, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 8, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 9, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 10, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 11, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 12, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 13, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 14, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 15, n_max))
N
N/**
N * INTERNAL_HIDDEN @endcond
N */
N
N/**
N * @addtogroup mem_pool_apis
N * @{
N */
N
N/**
N * @brief Statically define and initialize a memory pool.
N *
N * The memory pool's buffer contains @a n_max blocks that are @a max_size bytes
N * long. The memory pool allows blocks to be repeatedly partitioned into
N * quarters, down to blocks of @a min_size bytes long. The buffer is aligned
N * to a @a align -byte boundary.
N *
N * If the pool is to be accessed outside the module where it is defined, it
N * can be declared via
N *
N * @code extern struct k_mem_pool <name>; @endcode
N *
N * @param name Name of the memory pool.
N * @param minsz Size of the smallest blocks in the pool (in bytes).
N * @param maxsz Size of the largest blocks in the pool (in bytes).
N * @param nmax Number of maximum sized blocks in the pool.
N * @param align Alignment of the pool's buffer (power of 2).
N */
N#define K_MEM_POOL_DEFINE(name, minsz, maxsz, nmax, align)		\
N	char __aligned(align) _mpool_buf_##name[_ALIGN4(maxsz * nmax)	\
N				  + _MPOOL_BITS_SIZE(maxsz, minsz, nmax)]; \
N	struct k_mem_pool_lvl _mpool_lvls_##name[_MPOOL_LVLS(maxsz, minsz)]; \
N	struct k_mem_pool name __in_section(_k_mem_pool, static, name) = { \
N		.buf = _mpool_buf_##name,				\
N		.max_sz = maxsz,					\
N		.n_max = nmax,						\
N		.n_levels = _MPOOL_LVLS(maxsz, minsz),			\
N		.levels = _mpool_lvls_##name,				\
N	}
X#define K_MEM_POOL_DEFINE(name, minsz, maxsz, nmax, align)			char __aligned(align) _mpool_buf_##name[_ALIGN4(maxsz * nmax)					  + _MPOOL_BITS_SIZE(maxsz, minsz, nmax)]; 	struct k_mem_pool_lvl _mpool_lvls_##name[_MPOOL_LVLS(maxsz, minsz)]; 	struct k_mem_pool name __in_section(_k_mem_pool, static, name) = { 		.buf = _mpool_buf_##name,						.max_sz = maxsz,							.n_max = nmax,								.n_levels = _MPOOL_LVLS(maxsz, minsz),					.levels = _mpool_lvls_##name,					}
N
N/**
N * @brief Allocate memory from a memory pool.
N *
N * This routine allocates a memory block from a memory pool.
N *
N * @param pool Address of the memory pool.
N * @param block Pointer to block descriptor for the allocated memory.
N * @param size Amount of memory to allocate (in bytes).
N * @param timeout Maximum time to wait for operation to complete
N *        (in milliseconds). Use K_NO_WAIT to return without waiting,
N *        or K_FOREVER to wait as long as necessary.
N *
N * @retval 0 Memory allocated. The @a data field of the block descriptor
N *         is set to the starting address of the memory block.
N * @retval -ENOMEM Returned without waiting.
N * @retval -EAGAIN Waiting period timed out.
N */
Nextern int k_mem_pool_alloc(struct k_mem_pool *pool, struct k_mem_block *block,
N			    size_t size, s32_t timeout);
N
N/**
N * @brief Free memory allocated from a memory pool.
N *
N * This routine releases a previously allocated memory block back to its
N * memory pool.
N *
N * @param block Pointer to block descriptor for the allocated memory.
N *
N * @return N/A
N */
Nextern void k_mem_pool_free(struct k_mem_block *block);
N
N/**
N * @brief Defragment a memory pool.
N *
N * This is a no-op API preserved for backward compatibility only.
N *
N * @param pool Unused
N *
N * @return N/A
N */
Nstatic inline void k_mem_pool_defrag(struct k_mem_pool *pool) {}
N
N/**
N * @} end addtogroup mem_pool_apis
N */
N
N/**
N * @defgroup heap_apis Heap Memory Pool APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @brief Allocate memory from heap.
N *
N * This routine provides traditional malloc() semantics. Memory is
N * allocated from the heap memory pool.
N *
N * @param size Amount of memory requested (in bytes).
N *
N * @return Address of the allocated memory if successful; otherwise NULL.
N */
Nextern void *k_malloc(size_t size);
N
N/**
N * @brief Free memory allocated from heap.
N *
N * This routine provides traditional free() semantics. The memory being
N * returned must have been allocated from the heap memory pool.
N *
N * If @a ptr is NULL, no operation is performed.
N *
N * @param ptr Pointer to previously allocated memory.
N *
N * @return N/A
N */
Nextern void k_free(void *ptr);
N
N/**
N * @} end defgroup heap_apis
N */
N
N/* polling API - PRIVATE */
N
N#define _INIT_OBJ_POLL_EVENT(obj) do { (obj)->poll_event = NULL; } while ((0))
N
N/* private - implementation data created as needed, per-type */
Nstruct _poller {
N	struct k_thread *thread;
N};
N
N/* private - types bit positions */
Nenum _poll_types_bits {
N	/* can be used to ignore an event */
N	_POLL_TYPE_IGNORE,
N
N	/* to be signaled by k_poll_signal() */
N	_POLL_TYPE_SIGNAL,
N
N	/* semaphore availability */
N	_POLL_TYPE_SEM_AVAILABLE,
N
N	/* queue/fifo/lifo data availability */
N	_POLL_TYPE_DATA_AVAILABLE,
N
N	_POLL_NUM_TYPES
N};
N
N#define _POLL_TYPE_BIT(type) (1 << ((type) - 1))
N
N/* private - states bit positions */
Nenum _poll_states_bits {
N	/* default state when creating event */
N	_POLL_STATE_NOT_READY,
N
N	/* signaled by k_poll_signal() */
N	_POLL_STATE_SIGNALED,
N
N	/* semaphore is available */
N	_POLL_STATE_SEM_AVAILABLE,
N
N	/* data is available to read on queue/fifo/lifo */
N	_POLL_STATE_DATA_AVAILABLE,
N
N	_POLL_NUM_STATES
N};
N
N#define _POLL_STATE_BIT(state) (1 << ((state) - 1))
N
N#define _POLL_EVENT_NUM_UNUSED_BITS \
N	(32 - (0 \
N	       + 8 /* tag */ \
N	       + _POLL_NUM_TYPES \
N	       + _POLL_NUM_STATES \
N	       + 1 /* modes */ \
N	      ))
X#define _POLL_EVENT_NUM_UNUSED_BITS 	(32 - (0 	       + 8   	       + _POLL_NUM_TYPES 	       + _POLL_NUM_STATES 	       + 1   	      ))
N
N#if _POLL_EVENT_NUM_UNUSED_BITS < 0
X#if (32 - (0 + 8 + _POLL_NUM_TYPES + _POLL_NUM_STATES + 1 )) < 0
S#error overflow of 32-bit word in struct k_poll_event
N#endif
N
N/* end of polling API - PRIVATE */
N
N
N/**
N * @defgroup poll_apis Async polling APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/* Public polling API */
N
N/* public - values for k_poll_event.type bitfield */
N#define K_POLL_TYPE_IGNORE 0
N#define K_POLL_TYPE_SIGNAL _POLL_TYPE_BIT(_POLL_TYPE_SIGNAL)
N#define K_POLL_TYPE_SEM_AVAILABLE _POLL_TYPE_BIT(_POLL_TYPE_SEM_AVAILABLE)
N#define K_POLL_TYPE_DATA_AVAILABLE _POLL_TYPE_BIT(_POLL_TYPE_DATA_AVAILABLE)
N#define K_POLL_TYPE_FIFO_DATA_AVAILABLE K_POLL_TYPE_DATA_AVAILABLE
N
N/* public - polling modes */
Nenum k_poll_modes {
N	/* polling thread does not take ownership of objects when available */
N	K_POLL_MODE_NOTIFY_ONLY = 0,
N
N	K_POLL_NUM_MODES
N};
N
N/* public - values for k_poll_event.state bitfield */
N#define K_POLL_STATE_NOT_READY 0
N#define K_POLL_STATE_SIGNALED _POLL_STATE_BIT(_POLL_STATE_SIGNALED)
N#define K_POLL_STATE_SEM_AVAILABLE _POLL_STATE_BIT(_POLL_STATE_SEM_AVAILABLE)
N#define K_POLL_STATE_DATA_AVAILABLE _POLL_STATE_BIT(_POLL_STATE_DATA_AVAILABLE)
N#define K_POLL_STATE_FIFO_DATA_AVAILABLE K_POLL_STATE_DATA_AVAILABLE
N
N/* public - poll signal object */
Nstruct k_poll_signal {
N	/* PRIVATE - DO NOT TOUCH */
N	sys_dlist_t poll_events;
N
N	/*
N	 * 1 if the event has been signaled, 0 otherwise. Stays set to 1 until
N	 * user resets it to 0.
N	 */
N	unsigned int signaled;
N
N	/* custom result value passed to k_poll_signal() if needed */
N	int result;
N};
N
N#define K_POLL_SIGNAL_INITIALIZER(obj) \
N	{ \
N	.poll_events = SYS_DLIST_STATIC_INIT(&obj.poll_events), \
N	.signaled = 0, \
N	.result = 0, \
N	}
X#define K_POLL_SIGNAL_INITIALIZER(obj) 	{ 	.poll_events = SYS_DLIST_STATIC_INIT(&obj.poll_events), 	.signaled = 0, 	.result = 0, 	}
N
Nstruct k_poll_event {
N	/* PRIVATE - DO NOT TOUCH */
N	sys_dnode_t _node;
N
N	/* PRIVATE - DO NOT TOUCH */
N	struct _poller *poller;
N
N	/* optional user-specified tag, opaque, untouched by the API */
N	u32_t tag:8;
N
N	/* bitfield of event types (bitwise-ORed K_POLL_TYPE_xxx values) */
N	u32_t type:_POLL_NUM_TYPES;
N
N	/* bitfield of event states (bitwise-ORed K_POLL_STATE_xxx values) */
N	u32_t state:_POLL_NUM_STATES;
N
N	/* mode of operation, from enum k_poll_modes */
N	u32_t mode:1;
N
N	/* unused bits in 32-bit word */
N	u32_t unused:_POLL_EVENT_NUM_UNUSED_BITS;
X	u32_t unused:(32 - (0 + 8 + _POLL_NUM_TYPES + _POLL_NUM_STATES + 1 ));
N
N	/* per-type data */
N	union {
N		void *obj;
N		struct k_poll_signal *signal;
N		struct k_sem *sem;
N		struct k_fifo *fifo;
N		struct k_queue *queue;
N	};
N};
N
N#define K_POLL_EVENT_INITIALIZER(event_type, event_mode, event_obj) \
N	{ \
N	.poller = NULL, \
N	.type = event_type, \
N	.state = K_POLL_STATE_NOT_READY, \
N	.mode = event_mode, \
N	.unused = 0, \
N	{ .obj = event_obj }, \
N	}
X#define K_POLL_EVENT_INITIALIZER(event_type, event_mode, event_obj) 	{ 	.poller = NULL, 	.type = event_type, 	.state = K_POLL_STATE_NOT_READY, 	.mode = event_mode, 	.unused = 0, 	{ .obj = event_obj }, 	}
N
N#define K_POLL_EVENT_STATIC_INITIALIZER(event_type, event_mode, event_obj, \
N					event_tag) \
N	{ \
N	.type = event_type, \
N	.tag = event_tag, \
N	.state = K_POLL_STATE_NOT_READY, \
N	.mode = event_mode, \
N	.unused = 0, \
N	{ .obj = event_obj }, \
N	}
X#define K_POLL_EVENT_STATIC_INITIALIZER(event_type, event_mode, event_obj, 					event_tag) 	{ 	.type = event_type, 	.tag = event_tag, 	.state = K_POLL_STATE_NOT_READY, 	.mode = event_mode, 	.unused = 0, 	{ .obj = event_obj }, 	}
N
N/**
N * @brief Initialize one struct k_poll_event instance
N *
N * After this routine is called on a poll event, the event it ready to be
N * placed in an event array to be passed to k_poll().
N *
N * @param event The event to initialize.
N * @param type A bitfield of the types of event, from the K_POLL_TYPE_xxx
N *             values. Only values that apply to the same object being polled
N *             can be used together. Choosing K_POLL_TYPE_IGNORE disables the
N *             event.
N * @param mode Future. Use K_POLL_MODE_NOTIFY_ONLY.
N * @param obj Kernel object or poll signal.
N *
N * @return N/A
N */
N
Nextern void k_poll_event_init(struct k_poll_event *event, u32_t type,
N			      int mode, void *obj);
N
N/**
N * @brief Wait for one or many of multiple poll events to occur
N *
N * This routine allows a thread to wait concurrently for one or many of
N * multiple poll events to have occurred. Such events can be a kernel object
N * being available, like a semaphore, or a poll signal event.
N *
N * When an event notifies that a kernel object is available, the kernel object
N * is not "given" to the thread calling k_poll(): it merely signals the fact
N * that the object was available when the k_poll() call was in effect. Also,
N * all threads trying to acquire an object the regular way, i.e. by pending on
N * the object, have precedence over the thread polling on the object. This
N * means that the polling thread will never get the poll event on an object
N * until the object becomes available and its pend queue is empty. For this
N * reason, the k_poll() call is more effective when the objects being polled
N * only have one thread, the polling thread, trying to acquire them.
N *
N * When k_poll() returns 0, the caller should loop on all the events that were
N * passed to k_poll() and check the state field for the values that were
N * expected and take the associated actions.
N *
N * Before being reused for another call to k_poll(), the user has to reset the
N * state field to K_POLL_STATE_NOT_READY.
N *
N * @param events An array of pointers to events to be polled for.
N * @param num_events The number of events in the array.
N * @param timeout Waiting period for an event to be ready (in milliseconds),
N *                or one of the special values K_NO_WAIT and K_FOREVER.
N *
N * @retval 0 One or more events are ready.
N * @retval -EAGAIN Waiting period timed out.
N */
N
Nextern int k_poll(struct k_poll_event *events, int num_events,
N		  s32_t timeout);
N
N/**
N * @brief Initialize a poll signal object.
N *
N * Ready a poll signal object to be signaled via k_poll_signal().
N *
N * @param signal A poll signal.
N *
N * @return N/A
N */
N
Nextern void k_poll_signal_init(struct k_poll_signal *signal);
N
N/**
N * @brief Signal a poll signal object.
N *
N * This routine makes ready a poll signal, which is basically a poll event of
N * type K_POLL_TYPE_SIGNAL. If a thread was polling on that event, it will be
N * made ready to run. A @a result value can be specified.
N *
N * The poll signal contains a 'signaled' field that, when set by
N * k_poll_signal(), stays set until the user sets it back to 0. It thus has to
N * be reset by the user before being passed again to k_poll() or k_poll() will
N * consider it being signaled, and will return immediately.
N *
N * @param signal A poll signal.
N * @param result The value to store in the result field of the signal.
N *
N * @retval 0 The signal was delivered successfully.
N * @retval -EAGAIN The polling thread's timeout is in the process of expiring.
N */
N
Nextern int k_poll_signal(struct k_poll_signal *signal, int result);
N
N/* private internal function */
Nextern int _handle_obj_poll_events(sys_dlist_t *events, u32_t state);
N
N/**
N * @} end defgroup poll_apis
N */
N
N/**
N * @brief Make the CPU idle.
N *
N * This function makes the CPU idle until an event wakes it up.
N *
N * In a regular system, the idle thread should be the only thread responsible
N * for making the CPU idle and triggering any type of power management.
N * However, in some more constrained systems, such as a single-threaded system,
N * the only thread would be responsible for this if needed.
N *
N * @return N/A
N */
Nextern void k_cpu_idle(void);
N
N/**
N * @brief Make the CPU idle in an atomic fashion.
N *
N * Similar to k_cpu_idle(), but called with interrupts locked if operations
N * must be done atomically before making the CPU idle.
N *
N * @param key Interrupt locking key obtained from irq_lock().
N *
N * @return N/A
N */
Nextern void k_cpu_atomic_idle(unsigned int key);
N
Nextern void _sys_power_save_idle_exit(s32_t ticks);
N
N#include <arch/cpu.h>
L 1 "..\..\..\..\include\arch/cpu.h" 1
N/* cpu.h - automatically selects the correct arch.h file to include */
N
N/*
N * Copyright (c) 1997-2014 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N#ifndef __ARCHCPU_H__
N#define __ARCHCPU_H__
N
N#include <arch/arm/arch.h>
L 1 "..\..\..\..\include\arch/arm/arch.h" 1
N/*
N * Copyright (c) 2013-2014 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief ARM specific kernel interface header
N *
N * This header contains the ARM specific kernel interface.  It is
N * included by the kernel interface architecture-abstraction header
N * (include/arc/cpu.h)
N */
N
N#ifndef _ARM_ARCH__H_
N#define _ARM_ARCH__H_
N
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N/* ARM GPRs are often designated by two different names */
N#define sys_define_gpr_with_alias(name1, name2) union { u32_t name1, name2; }
N
N/* APIs need to support non-byte addressable architectures */
N
N#define OCTET_TO_SIZEOFUNIT(X) (X)
N#define SIZEOFUNIT_TO_OCTET(X) (X)
N
N#include <arch/arm/cortex_m/irq.h>
L 1 "..\..\..\..\include\arch/arm/cortex_m/irq.h" 1
N/*
N * Copyright (c) 2013-2014 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Cortex-M public interrupt handling
N *
N * ARM-specific kernel interrupt handling interface. Included by arm/arch.h.
N */
N
N#ifndef _ARCH_ARM_CORTEXM_IRQ_H_
N#define _ARCH_ARM_CORTEXM_IRQ_H_
N
N#include <irq.h>
L 1 "..\..\..\..\include\irq.h" 1
N/*
N * Copyright (c) 2015 Intel corporation
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Public interface for configuring interrupts
N */
N#ifndef _IRQ_H_
N#define _IRQ_H_
N
N/* Pull in the arch-specific implementations */
N#include <arch/cpu.h>
L 1 "..\..\..\..\include\arch/cpu.h" 1
N/* cpu.h - automatically selects the correct arch.h file to include */
N
N/*
N * Copyright (c) 1997-2014 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N#ifndef __ARCHCPU_H__
S#define __ARCHCPU_H__
S
S#include <arch/arm/arch.h>
S
N#endif /* __ARCHCPU_H__ */
L 16 "..\..\..\..\include\irq.h" 2
N
N#ifndef _ASMLANGUAGE
N#include <toolchain.h>
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N/**
N * @defgroup isr_apis Interrupt Service Routine APIs
N * @ingroup kernel_apis
N * @{
N */
N
N/**
N * @brief Initialize an interrupt handler.
N *
N * This routine initializes an interrupt handler for an IRQ. The IRQ must be
N * subsequently enabled before the interrupt handler begins servicing
N * interrupts.
N *
N * @warning
N * Although this routine is invoked at run-time, all of its arguments must be
N * computable by the compiler at build time.
N *
N * @param irq_p IRQ line number.
N * @param priority_p Interrupt priority.
N * @param isr_p Address of interrupt service routine.
N * @param isr_param_p Parameter passed to interrupt service routine.
N * @param flags_p Architecture-specific IRQ configuration flags..
N *
N * @return Interrupt vector assigned to this interrupt.
N */
N#define IRQ_CONNECT(irq_p, priority_p, isr_p, isr_param_p, flags_p) \
N	_ARCH_IRQ_CONNECT(irq_p, priority_p, isr_p, isr_param_p, flags_p)
X#define IRQ_CONNECT(irq_p, priority_p, isr_p, isr_param_p, flags_p) 	_ARCH_IRQ_CONNECT(irq_p, priority_p, isr_p, isr_param_p, flags_p)
N
N/**
N * @brief Initialize a 'direct' interrupt handler.
N *
N * This routine initializes an interrupt handler for an IRQ. The IRQ must be
N * subsequently enabled via irq_enable() before the interrupt handler begins
N * servicing interrupts.
N *
N * These ISRs are designed for performance-critical interrupt handling and do
N * not go through common interrupt handling code. They must be implemented in
N * such a way that it is safe to put them directly in the vector table.  For
N * ISRs written in C, The ISR_DIRECT_DECLARE() macro will do this
N * automatically. For ISRs written in assembly it is entirely up to the
N * developer to ensure that the right steps are taken.
N *
N * This type of interrupt currently has a few limitations compared to normal
N * Zephyr interrupts:
N * - No parameters are passed to the ISR.
N * - No stack switch is done, the ISR will run on the interrupted context's
N *   stack, unless the architecture automatically does the stack switch in HW.
N * - Interrupt locking state is unchanged from how the HW sets it when the ISR
N *   runs. On arches that enter ISRs with interrupts locked, they will remain
N *   locked.
N * - Scheduling decisions are now optional, controlled by the return value of
N *   ISRs implemented with the ISR_DIRECT_DECLARE() macro
N * - The call into the OS to exit power management idle state is now optional.
N *   Normal interrupts always do this before the ISR is run, but when it runs
N *   is now controlled by the placement of a ISR_DIRECT_PM() macro, or omitted
N *   entirely.
N *
N * @warning
N * Although this routine is invoked at run-time, all of its arguments must be
N * computable by the compiler at build time.
N *
N * @param irq_p IRQ line number.
N * @param priority_p Interrupt priority.
N * @param isr_p Address of interrupt service routine.
N * @param flags_p Architecture-specific IRQ configuration flags.
N *
N * @return Interrupt vector assigned to this interrupt.
N */
N#define IRQ_DIRECT_CONNECT(irq_p, priority_p, isr_p, flags_p) \
N	_ARCH_IRQ_DIRECT_CONNECT(irq_p, priority_p, isr_p, flags_p)
X#define IRQ_DIRECT_CONNECT(irq_p, priority_p, isr_p, flags_p) 	_ARCH_IRQ_DIRECT_CONNECT(irq_p, priority_p, isr_p, flags_p)
N
N/**
N * @brief Common tasks before executing the body of an ISR
N *
N * This macro must be at the beginning of all direct interrupts and performs
N * minimal architecture-specific tasks before the ISR itself can run. It takes
N * no arguments and has no return value.
N */
N#define ISR_DIRECT_HEADER() _ARCH_ISR_DIRECT_HEADER()
N
N/**
N * @brief Common tasks before exiting the body of an ISR
N *
N * This macro must be at the end of all direct interrupts and performs
N * minimal architecture-specific tasks like EOI. It has no return value.
N *
N * In a normal interrupt, a check is done at end of interrupt to invoke
N * _Swap() logic if the current thread is preemptible and there is another
N * thread ready to run in the kernel's ready queue cache. This is now optional
N * and controlled by the check_reschedule argument. If unsure, set to nonzero.
N * On systems that do stack switching and nested interrupt tracking in software,
N * _Swap() should only be called if this was a non-nested interrupt.
N *
N * @param check_reschedule If nonzero, additionally invoke scheduling logic
N */
N#define ISR_DIRECT_FOOTER(check_reschedule) \
N	_ARCH_ISR_DIRECT_FOOTER(check_reschedule)
X#define ISR_DIRECT_FOOTER(check_reschedule) 	_ARCH_ISR_DIRECT_FOOTER(check_reschedule)
N
N/**
N * @brief Perform power management idle exit logic
N *
N * This macro may optionally be invoked somewhere in between IRQ_DIRECT_HEADER()
N * and IRQ_DIRECT_FOOTER() invocations. It performs tasks necessary to
N * exit power management idle state. It takes no parameters and returns no
N * arguments. It may be omitted, but be careful!
N */
N#define ISR_DIRECT_PM() _ARCH_ISR_DIRECT_PM()
N
N/**
N * @brief Helper macro to declare a direct interrupt service routine.
N *
N * This will declare the function in a proper way and automatically include
N * the ISR_DIRECT_FOOTER() and ISR_DIRECT_HEADER() macros. The function should
N * return nonzero status if a scheduling decision should potentially be made.
N * See ISR_DIRECT_FOOTER() for more details on the scheduling decision.
N *
N * For architectures that support 'regular' and 'fast' interrupt types, where
N * these interrupt types require different assembly language handling of
N * registers by the ISR, this will always generate code for the 'fast'
N * interrupt type.
N *
N * Example usage:
N *
N * ISR_DIRECT_DECLARE(my_isr)
N * {
N *	bool done = do_stuff();
N *	ISR_DIRECT_PM(); <-- done after do_stuff() due to latency concerns
N *	if (!done) {
N *		return 0;  <-- Don't bother checking if we have to _Swap()
N *	}
N *	k_sem_give(some_sem);
N *	return 1;
N * }
N *
N * @param name symbol name of the ISR
N */
N#define ISR_DIRECT_DECLARE(name) _ARCH_ISR_DIRECT_DECLARE(name)
N
N/**
N * @brief Lock interrupts.
N *
N * This routine disables all interrupts on the CPU. It returns an unsigned
N * integer "lock-out key", which is an architecture-dependent indicator of
N * whether interrupts were locked prior to the call. The lock-out key must be
N * passed to irq_unlock() to re-enable interrupts.
N *
N * This routine can be called recursively, as long as the caller keeps track
N * of each lock-out key that is generated. Interrupts are re-enabled by
N * passing each of the keys to irq_unlock() in the reverse order they were
N * acquired. (That is, each call to irq_lock() must be balanced by
N * a corresponding call to irq_unlock().)
N *
N * @note
N * This routine can be called by ISRs or by threads. If it is called by a
N * thread, the interrupt lock is thread-specific; this means that interrupts
N * remain disabled only while the thread is running. If the thread performs an
N * operation that allows another thread to run (for example, giving a semaphore
N * or sleeping for N milliseconds), the interrupt lock no longer applies and
N * interrupts may be re-enabled while other processing occurs. When the thread
N * once again becomes the current thread, the kernel re-establishes its
N * interrupt lock; this ensures the thread won't be interrupted until it has
N * explicitly released the interrupt lock it established.
N *
N * @warning
N * The lock-out key should never be used to manually re-enable interrupts
N * or to inspect or manipulate the contents of the CPU's interrupt bits.
N *
N * @return Lock-out key.
N */
N#define irq_lock() _arch_irq_lock()
N
N/**
N * @brief Unlock interrupts.
N *
N * This routine reverses the effect of a previous call to irq_lock() using
N * the associated lock-out key. The caller must call the routine once for
N * each time it called irq_lock(), supplying the keys in the reverse order
N * they were acquired, before interrupts are enabled.
N *
N * @note Can be called by ISRs.
N *
N * @param key Lock-out key generated by irq_lock().
N *
N * @return N/A
N */
N#define irq_unlock(key) _arch_irq_unlock(key)
N
N/**
N * @brief Enable an IRQ.
N *
N * This routine enables interrupts from source @a irq.
N *
N * @param irq IRQ line.
N *
N * @return N/A
N */
N#define irq_enable(irq) _arch_irq_enable(irq)
N
N/**
N * @brief Disable an IRQ.
N *
N * This routine disables interrupts from source @a irq.
N *
N * @param irq IRQ line.
N *
N * @return N/A
N */
N#define irq_disable(irq) _arch_irq_disable(irq)
N
N/**
N * @brief Get IRQ enable state.
N *
N * This routine indicates if interrupts from source @a irq are enabled.
N *
N * @param irq IRQ line.
N *
N * @return interrupt enable state, true or false
N */
N#define irq_is_enabled(irq) _arch_irq_is_enabled(irq)
N
N/**
N * @}
N */
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* ASMLANGUAGE */
N#endif /* _IRQ_H_ */
L 18 "..\..\..\..\include\arch/arm/cortex_m/irq.h" 2
N#include <sw_isr_table.h>
L 1 "..\..\..\..\include\sw_isr_table.h" 1
N/*
N * Copyright (c) 2014, Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Software-managed ISR table
N *
N * Data types for a software-managed ISR table, with a parameter per-ISR.
N */
N
N#ifndef _SW_ISR_TABLE__H_
N#define _SW_ISR_TABLE__H_
N
N#include <arch/cpu.h>
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N#if !defined(_ASMLANGUAGE)
X#if !0L
N#include <zephyr/types.h>
N#include <toolchain.h>
N
N/*
N * Note the order: arg first, then ISR. This allows a table entry to be
N * loaded arg -> r0, isr -> r3 in _isr_wrapper with one ldmia instruction,
N * on ARM Cortex-M (Thumb2).
N */
Nstruct _isr_table_entry {
N	void *arg;
N	void (*isr)(void *);
N};
N
N/* The software ISR table itself, an array of these structures indexed by the
N * irq line
N */
Nextern struct _isr_table_entry _sw_isr_table[];
N
N/*
N * Data structure created in a special binary .intlist section for each
N * configured interrupt. gen_irq_tables.py pulls this out of the binary and
N * uses it to create the IRQ vector table and the _sw_isr_table.
N *
N * More discussion in include/linker/intlist.ld
N */
Nstruct _isr_list {
N	/** IRQ line number */
N	s32_t irq;
N	/** Flags for this IRQ, see ISR_FLAG_* definitions */
N	s32_t flags;
N	/** ISR to call */
N	void *func;
N	/** Parameter for non-direct IRQs */
N	void *param;
N};
N
N/** This interrupt gets put directly in the vector table */
N#define ISR_FLAG_DIRECT (1 << 0)
N
N#define _MK_ISR_NAME(x, y) __isr_ ## x ## _irq_ ## y
N
N/* Create an instance of struct _isr_list which gets put in the .intList
N * section. This gets consumed by gen_isr_tables.py which creates the vector
N * and/or SW ISR tables.
N */
N#define _ISR_DECLARE(irq, flags, func, param) \
N    _sw_isr_table[irq].arg = (void *)param; \
N    _sw_isr_table[irq].isr = (void (*)(void *))func;     
X#define _ISR_DECLARE(irq, flags, func, param)     _sw_isr_table[irq].arg = (void *)param;     _sw_isr_table[irq].isr = (void (*)(void *))func;     
N
N
N#endif /* _ASMLANGUAGE */
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* _SW_ISR_TABLE__H_ */
L 19 "..\..\..\..\include\arch/arm/cortex_m/irq.h" 2
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N#ifdef _ASMLANGUAGE
SGTEXT(_IntExit);
SGTEXT(_arch_irq_enable)
SGTEXT(_arch_irq_disable)
SGTEXT(_arch_irq_is_enabled)
N#else
Nextern void _arch_irq_enable(unsigned int irq);
Nextern void _arch_irq_disable(unsigned int irq);
Nextern int _arch_irq_is_enabled(unsigned int irq);
N
Nextern void _IntExit(void);
N
N/* macros convert value of it's argument to a string */
N#define DO_TOSTR(s) #s
N#define TOSTR(s) DO_TOSTR(s)
N
N/* concatenate the values of the arguments into one */
N#define DO_CONCAT(x, y) x ## y
N#define CONCAT(x, y) DO_CONCAT(x, y)
N
N/* internal routine documented in C file, needed by IRQ_CONNECT() macro */
Nextern void _irq_priority_set(unsigned int irq, unsigned int prio,
N			      u32_t flags);
N
N
N/* Flags for use with IRQ_CONNECT() */
N#if CONFIG_ZERO_LATENCY_IRQS
S/**
S * Set this interrupt up as a zero-latency IRQ. It has a fixed hardware
S * priority level (discarding what was supplied in the interrupt's priority
S * argument), and will run even if irq_lock() is active. Be careful!
S */
S#define IRQ_ZERO_LATENCY	(1 << 0)
N#endif
N
N
N/**
N * Configure a static interrupt.
N *
N * All arguments must be computable by the compiler at build time.
N *
N * _ISR_DECLARE will populate the .intList section with the interrupt's
N * parameters, which will then be used by gen_irq_tables.py to create
N * the vector table and the software ISR table. This is all done at
N * build-time.
N *
N * We additionally set the priority in the interrupt controller at
N * runtime.
N *
N * @param irq_p IRQ line number
N * @param priority_p Interrupt priority
N * @param isr_p Interrupt service routine
N * @param isr_param_p ISR parameter
N * @param flags_p IRQ options
N *
N * @return The vector assigned to this interrupt
N */
N#define _ARCH_IRQ_CONNECT(irq_p, priority_p, isr_p, isr_param_p, flags_p) \
N({ \
N	_ISR_DECLARE(irq_p, 0, isr_p, isr_param_p); \
N	_irq_priority_set(irq_p, priority_p, flags_p); \
N	irq_p; \
N})
X#define _ARCH_IRQ_CONNECT(irq_p, priority_p, isr_p, isr_param_p, flags_p) ({ 	_ISR_DECLARE(irq_p, 0, isr_p, isr_param_p); 	_irq_priority_set(irq_p, priority_p, flags_p); 	irq_p; })
N
N
N/**
N * Configure a 'direct' static interrupt.
N *
N * See include/irq.h for details.
N * All arguments must be computable at build time.
N */
N#define _ARCH_IRQ_DIRECT_CONNECT(irq_p, priority_p, isr_p, flags_p) \
N({ \
N	_ISR_DECLARE(irq_p, ISR_FLAG_DIRECT, isr_p, NULL); \
N	_irq_priority_set(irq_p, priority_p, flags_p); \
N	irq_p; \
N})
X#define _ARCH_IRQ_DIRECT_CONNECT(irq_p, priority_p, isr_p, flags_p) ({ 	_ISR_DECLARE(irq_p, ISR_FLAG_DIRECT, isr_p, NULL); 	_irq_priority_set(irq_p, priority_p, flags_p); 	irq_p; })
N
N/* FIXME prefer these inline, but see ZEP-1595 */
Nextern void _arch_isr_direct_pm(void);
N#define _ARCH_ISR_DIRECT_PM() _arch_isr_direct_pm()
N
N#if defined(CONFIG_KERNEL_EVENT_LOGGER_SLEEP) || \
N	defined(CONFIG_KERNEL_EVENT_LOGGER_INTERRUPT)
X#if 0L || 	0L
S#define _ARCH_ISR_DIRECT_HEADER() _arch_isr_direct_header()
Sextern void _arch_isr_direct_header(void);
N#else
N#define _ARCH_ISR_DIRECT_HEADER() do { } while (0)
N#endif
N
N#define _ARCH_ISR_DIRECT_FOOTER(swap) _arch_isr_direct_footer(swap)
N
N/* arch/arm/core/exc_exit.S */
Nextern void _IntExit(void);
N
Nstatic inline void _arch_isr_direct_footer(int maybe_swap)
N{
N	if (maybe_swap) {
N		_IntExit();
N	}
N}
N
N#define _ARCH_ISR_DIRECT_DECLARE(name) \
N	static inline int name##_body(void); \
N	__attribute__ ((interrupt ("IRQ"))) void name(void) \
N	{ \
N		int check_reschedule; \
N		ISR_DIRECT_HEADER(); \
N		check_reschedule = name##_body(); \
N		ISR_DIRECT_FOOTER(check_reschedule); \
N	} \
N	static inline int name##_body(void)
X#define _ARCH_ISR_DIRECT_DECLARE(name) 	static inline int name##_body(void); 	__attribute__ ((interrupt ("IRQ"))) void name(void) 	{ 		int check_reschedule; 		ISR_DIRECT_HEADER(); 		check_reschedule = name##_body(); 		ISR_DIRECT_FOOTER(check_reschedule); 	} 	static inline int name##_body(void)
N
N/* Spurious interrupt handler. Throws an error if called */
Nextern void _irq_spurious(void *unused);
N
N/* Architecture-specific common entry point for interrupts from the vector
N * table. Most likely implemented in assembly. Looks up the correct handler
N * and parameter from the _sw_isr_table and executes it.
N */
Nextern void _isr_wrapper(void);
N
N#endif /* _ASMLANGUAGE */
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* _ARCH_ARM_CORTEXM_IRQ_H_ */
L 33 "..\..\..\..\include\arch/arm/arch.h" 2
N#include <arch/arm/cortex_m/error.h>
L 1 "..\..\..\..\include\arch/arm/cortex_m/error.h" 1
N/*
N * Copyright (c) 2013-2014 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Cortex-M public error handling
N *
N * ARM-specific kernel error handling interface. Included by arm/arch.h.
N */
N
N#ifndef _ARCH_ARM_CORTEXM_ERROR_H_
N#define _ARCH_ARM_CORTEXM_ERROR_H_
N
N#include <arch/arm/cortex_m/exc.h>
L 1 "..\..\..\..\include\arch/arm/cortex_m/exc.h" 1
N/*
N * Copyright (c) 2013-2014 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Cortex-M public exception handling
N *
N * ARM-specific kernel exception handling interface. Included by arm/arch.h.
N */
N
N#ifndef _ARCH_ARM_CORTEXM_EXC_H_
N#define _ARCH_ARM_CORTEXM_EXC_H_
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N/* for assembler, only works with constants */
N#define _EXC_PRIO(pri) (((pri) << (8 - CONFIG_NUM_IRQ_PRIO_BITS)) & 0xff)
N
N#ifdef CONFIG_ZERO_LATENCY_IRQS
S#define _ZERO_LATENCY_IRQS_RESERVED_PRIO 1
N#else
N#define _ZERO_LATENCY_IRQS_RESERVED_PRIO 0
N#endif
N
N#if defined(CONFIG_CPU_CORTEX_M_HAS_PROGRAMMABLE_FAULT_PRIOS) || \
N	defined(CONFIG_CPU_CORTEX_M_HAS_BASEPRI)
X#if 0L || 	0L
S#define _EXCEPTION_RESERVED_PRIO 1
N#else
N#define _EXCEPTION_RESERVED_PRIO 0
N#endif
N
N#define _IRQ_PRIO_OFFSET \
N	(_ZERO_LATENCY_IRQS_RESERVED_PRIO + \
N	 _EXCEPTION_RESERVED_PRIO)
X#define _IRQ_PRIO_OFFSET 	(_ZERO_LATENCY_IRQS_RESERVED_PRIO + 	 _EXCEPTION_RESERVED_PRIO)
N
N#define _EXC_IRQ_DEFAULT_PRIO _EXC_PRIO(_IRQ_PRIO_OFFSET)
N
N#ifdef _ASMLANGUAGE
SGTEXT(_ExcExit);
N#else
N#include <zephyr/types.h>
N
Nstruct __esf {
N	sys_define_gpr_with_alias(a1, r0);
X	union { u32_t a1, r0; };
N	sys_define_gpr_with_alias(a2, r1);
X	union { u32_t a2, r1; };
N	sys_define_gpr_with_alias(a3, r2);
X	union { u32_t a3, r2; };
N	sys_define_gpr_with_alias(a4, r3);
X	union { u32_t a4, r3; };
N	sys_define_gpr_with_alias(ip, r12);
X	union { u32_t ip, r12; };
N	sys_define_gpr_with_alias(lr, r14);
X	union { u32_t lr, r14; };
N	sys_define_gpr_with_alias(pc, r15);
X	union { u32_t pc, r15; };
N	u32_t xpsr;
N#ifdef CONFIG_FLOAT
S	float s[16];
S	u32_t fpscr;
S	u32_t undefined;
N#endif
N};
N
Ntypedef struct __esf NANO_ESF;
N
Nextern void _ExcExit(void);
N
N/**
N * @brief display the contents of a exception stack frame
N *
N * @return N/A
N */
N
Nextern void sys_exc_esf_dump(NANO_ESF *esf);
N
N#endif /* _ASMLANGUAGE */
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* _ARCH_ARM_CORTEXM_EXC_H_ */
L 18 "..\..\..\..\include\arch/arm/cortex_m/error.h" 2
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N#ifndef _ASMLANGUAGE
Nextern void _NanoFatalErrorHandler(unsigned int reason, const NANO_ESF *esf);
Nextern void _SysFatalErrorHandler(unsigned int reason, const NANO_ESF *esf);
N#endif
N
N#define _NANO_ERR_HW_EXCEPTION (0)      /* MPU/Bus/Usage fault */
N#define _NANO_ERR_INVALID_TASK_EXIT (1) /* Invalid task exit */
N#define _NANO_ERR_STACK_CHK_FAIL (2)    /* Stack corruption detected */
N#define _NANO_ERR_ALLOCATION_FAIL (3)   /* Kernel Allocation Failure */
N#define _NANO_ERR_KERNEL_OOPS (4)       /* Kernel oops (fatal to thread) */
N#define _NANO_ERR_KERNEL_PANIC (5)	/* Kernel panic (fatal to system) */
N
N#define _SVC_CALL_IRQ_OFFLOAD		1
N#define _SVC_CALL_RUNTIME_EXCEPT	2
N
N/* ARMv6 will hard-fault if SVC is called with interrupts locked. Just
N * force them unlocked, the thread is in an undefined state anyway
N *
N * On ARMv7m we won't get a hardfault, but if interrupts were locked the
N * thread will continue executing after the exception and forbid PendSV to
N * schedule a new thread until they are unlocked which is not what we want.
N * Force them unlocked as well.
N */
N#define _ARCH_EXCEPT(reason_p) do { \
N	__asm__ volatile ( \
N		"cpsie i\n\t" \
N		"mov r0, %[reason]\n\t" \
N		"svc %[id]\n\t" \
N		: \
N		: [reason] "i" (reason_p), [id] "i" (_SVC_CALL_RUNTIME_EXCEPT) \
N		: "memory"); \
N	CODE_UNREACHABLE; \
N} while (0)
X#define _ARCH_EXCEPT(reason_p) do { 	__asm__ volatile ( 		"cpsie i\n\t" 		"mov r0, %[reason]\n\t" 		"svc %[id]\n\t" 		: 		: [reason] "i" (reason_p), [id] "i" (_SVC_CALL_RUNTIME_EXCEPT) 		: "memory"); 	CODE_UNREACHABLE; } while (0)
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* _ARCH_ARM_CORTEXM_ERROR_H_ */
L 34 "..\..\..\..\include\arch/arm/arch.h" 2
N#include <arch/arm/cortex_m/misc.h>
L 1 "..\..\..\..\include\arch/arm/cortex_m/misc.h" 1
N/*
N * Copyright (c) 2013-2014 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Cortex-M public kernel miscellaneous
N *
N * ARM-specific kernel miscellaneous interface. Included by arm/arch.h.
N */
N
N#ifndef _ARCH_ARM_CORTEXM_MISC_H_
N#define _ARCH_ARM_CORTEXM_MISC_H_
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N#ifndef _ASMLANGUAGE
Nextern void k_cpu_idle(void);
N
Nextern u32_t _timer_cycle_get_32(void);
N#define _arch_k_cycle_get_32()	_timer_cycle_get_32()
N#endif
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* _ARCH_ARM_CORTEXM_MISC_H_ */
L 35 "..\..\..\..\include\arch/arm/arch.h" 2
N#include <arch/arm/cortex_m/asm_inline.h>
L 1 "..\..\..\..\include\arch/arm/cortex_m/asm_inline.h" 1
N/* Intel ARM inline assembler functions and macros for public functions */
N
N/*
N * Copyright (c) 2015, Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N#ifndef _ASM_INLINE_PUBLIC_H
N#define _ASM_INLINE_PUBLIC_H
N
N/*
N * The file must not be included directly
N * Include kernel.h instead
N */
N
N#include <arch/arm/cortex_m/asm_inline_gcc.h>
L 1 "..\..\..\..\include\arch/arm/cortex_m/asm_inline_gcc.h" 1
N/* ARM Cortex-M GCC specific public inline assembler functions and macros */
N
N/*
N * Copyright (c) 2015, Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/* Either public functions or macros or invoked by public functions */
N
N#ifndef _ASM_INLINE_GCC_PUBLIC_GCC_H
N#define _ASM_INLINE_GCC_PUBLIC_GCC_H
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N/*
N * The file must not be included directly
N * Include arch/cpu.h instead
N */
N
N#ifdef _ASMLANGUAGE
S
S#define _SCS_BASE_ADDR _PPB_INT_SCS
S#define _SCS_ICSR (_SCS_BASE_ADDR + 0xd04)
S#define _SCS_ICSR_PENDSV (1 << 28)
S#define _SCS_ICSR_UNPENDSV (1 << 27)
S#define _SCS_ICSR_RETTOBASE (1 << 11)
S
N#else /* !_ASMLANGUAGE */
N#include <zephyr/types.h>
N#include <irq.h>
N
N/**
N *
N * @brief find most significant bit set in a 32-bit word
N *
N * This routine finds the first bit set starting from the most significant bit
N * in the argument passed in and returns the index of that bit.  Bits are
N * numbered starting at 1 from the least significant bit.  A return value of
N * zero indicates that the value passed is zero.
N *
N * @return most significant bit set, 0 if @a op is 0
N */
N
Nstatic ALWAYS_INLINE unsigned int find_msb_set(u32_t op)
Xstatic inline __attribute__((always_inline)) unsigned int find_msb_set(u32_t op)
N{
N	if (!op) {
N		return 0;
N	}
N
N	return 32 - __builtin_clz(op);
N}
N
N
N/**
N *
N * @brief find least significant bit set in a 32-bit word
N *
N * This routine finds the first bit set starting from the least significant bit
N * in the argument passed in and returns the index of that bit.  Bits are
N * numbered starting at 1 from the least significant bit.  A return value of
N * zero indicates that the value passed is zero.
N *
N * @return least significant bit set, 0 if @a op is 0
N */
N
Nstatic ALWAYS_INLINE unsigned int find_lsb_set(u32_t op)
Xstatic inline __attribute__((always_inline)) unsigned int find_lsb_set(u32_t op)
N{
N	return __builtin_ffs(op);
N}
N
N
N/**
N *
N * @brief Disable all interrupts on the CPU
N *
N * This routine disables interrupts.  It can be called from either interrupt,
N * task or fiber level.  This routine returns an architecture-dependent
N * lock-out key representing the "interrupt disable state" prior to the call;
N * this key can be passed to irq_unlock() to re-enable interrupts.
N *
N * The lock-out key should only be used as the argument to the irq_unlock()
N * API.  It should never be used to manually re-enable interrupts or to inspect
N * or manipulate the contents of the source register.
N *
N * This function can be called recursively: it will return a key to return the
N * state of interrupt locking to the previous level.
N *
N * WARNINGS
N * Invoking a kernel routine with interrupts locked may result in
N * interrupts being re-enabled for an unspecified period of time.  If the
N * called routine blocks, interrupts will be re-enabled while another
N * thread executes, or while the system is idle.
N *
N * The "interrupt disable state" is an attribute of a thread.  Thus, if a
N * fiber or task disables interrupts and subsequently invokes a kernel
N * routine that causes the calling thread to block, the interrupt
N * disable state will be restored when the thread is later rescheduled
N * for execution.
N *
N * @return An architecture-dependent lock-out key representing the
N * "interrupt disable state" prior to the call.
N *
N * @internal
N *
N * On Cortex-M3/M4, this function prevents exceptions of priority lower than
N * the two highest priorities from interrupting the CPU.
N *
N * On Cortex-M0/M0+, this function reads the value of PRIMASK which shows
N * if interrupts are enabled, then disables all interrupts except NMI.
N *
N */
N
Nextern unsigned int _arch_irq_lock_s(void);
Nstatic ALWAYS_INLINE unsigned int _arch_irq_lock(void)
Xstatic inline __attribute__((always_inline)) unsigned int _arch_irq_lock(void)
N{
N	return _arch_irq_lock_s();
N}
N
N
N/**
N *
N * @brief Enable all interrupts on the CPU (inline)
N *
N * This routine re-enables interrupts on the CPU.  The @a key parameter is an
N * architecture-dependent lock-out key that is returned by a previous
N * invocation of irq_lock().
N *
N * This routine can be called from either interrupt, task or fiber level.
N *
N * @param key architecture-dependent lock-out key
N *
N * @return N/A
N *
N * On Cortex-M0/M0+, this enables all interrupts if they were not
N * previously disabled.
N *
N */
Nextern void _arch_irq_unlock_s(unsigned int key);
Nstatic ALWAYS_INLINE void _arch_irq_unlock(unsigned int key)
Xstatic inline __attribute__((always_inline)) void _arch_irq_unlock(unsigned int key)
N{
N	return _arch_irq_unlock_s(key);
N}
N
N
N#endif /* _ASMLANGUAGE */
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* _ASM_INLINE_GCC_PUBLIC_GCC_H */
L 18 "..\..\..\..\include\arch/arm/cortex_m/asm_inline.h" 2
N
N#endif /* _ASM_INLINE_PUBLIC_H */
L 36 "..\..\..\..\include\arch/arm/arch.h" 2
N#include <arch/arm/cortex_m/sys_io.h>
L 1 "..\..\..\..\include\arch/arm/cortex_m/sys_io.h" 1
N/*
N * Copyright (c) 2016 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @brief ARM CORTEX-M Series memory mapped register I/O operations
N */
N
N#ifndef _CORTEX_M_SYS_IO_H_
N#define _CORTEX_M_SYS_IO_H_
N
N#if !defined(_ASMLANGUAGE)
X#if !0L
N
N#include <sys_io.h>
L 1 "..\..\..\..\include\sys_io.h" 1
N/* Port and memory mapped registers I/O operations */
N
N/*
N * Copyright (c) 2015 Intel Corporation.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N#ifndef __SYS_IO_H__
N#define __SYS_IO_H__
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N#include <kernel.h>
L 1 "..\..\..\..\include\kernel.h" 1
N/*
N * Copyright (c) 2016, Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N *
N * @brief Public kernel APIs.
N */
N
N#ifndef _kernel__h_
S#define _kernel__h_
S
S#if !defined(_ASMLANGUAGE)
S#include <stddef.h>
S#include <zephyr/types.h>
S#include <limits.h>
S#include <toolchain.h>
S#include <linker/sections.h>
S#include <atomic.h>
S#include <misc/__assert.h>
S#include <misc/dlist.h>
S#include <misc/slist.h>
S#include <misc/util.h>
S#include <drivers/rand32.h>
S#include <kernel_arch_thread.h>
S
S#ifdef __cplusplus
Sextern "C" {
S#endif
S
S/**
S * @brief Kernel APIs
S * @defgroup kernel_apis Kernel APIs
S * @{
S * @}
S */
S
S#ifdef CONFIG_KERNEL_DEBUG
S#include <misc/printk.h>
S#define K_DEBUG(fmt, ...) printk("[%s]  " fmt, __func__, ##__VA_ARGS__)
S#else
S#define K_DEBUG(fmt, ...)
S#endif
S
S#define _NUM_COOP_PRIO (16)
S#define _NUM_PREEMPT_PRIO (15 + 1)
S
S#define K_PRIO_COOP(x) (-(_NUM_COOP_PRIO - (x)))
S#define K_PRIO_PREEMPT(x) (x)
S
S#define K_ANY NULL
S#define K_END NULL
S
S#define K_HIGHEST_THREAD_PRIO (-16)
S
S#define K_LOWEST_THREAD_PRIO 15
S
S#define K_IDLE_PRIO K_LOWEST_THREAD_PRIO
S
S#define K_HIGHEST_APPLICATION_THREAD_PRIO (K_HIGHEST_THREAD_PRIO)
S#define K_LOWEST_APPLICATION_THREAD_PRIO (K_LOWEST_THREAD_PRIO - 1)
S
Stypedef sys_dlist_t _wait_q_t;
S
S#ifdef CONFIG_OBJECT_TRACING
S#define _OBJECT_TRACING_NEXT_PTR(type) struct type *__next
S#define _OBJECT_TRACING_INIT .__next = NULL,
S#else
S#define _OBJECT_TRACING_INIT
S#define _OBJECT_TRACING_NEXT_PTR(type)
S#endif
S
S#define _POLL_EVENT_OBJ_INIT(obj) \
S	.poll_events = SYS_DLIST_STATIC_INIT(&obj.poll_events),
X#define _POLL_EVENT_OBJ_INIT(obj) 	.poll_events = SYS_DLIST_STATIC_INIT(&obj.poll_events),
S#define _POLL_EVENT sys_dlist_t poll_events
S
Sstruct k_thread;
Sstruct k_mutex;
Sstruct k_sem;
Sstruct k_alert;
Sstruct k_msgq;
Sstruct k_mbox;
Sstruct k_pipe;
Sstruct k_queue;
Sstruct k_fifo;
Sstruct k_lifo;
Sstruct k_stack;
Sstruct k_mem_slab;
Sstruct k_mem_pool;
Sstruct k_timer;
Sstruct k_poll_event;
Sstruct k_poll_signal;
S
S/* timeouts */
S
Sstruct _timeout;
Stypedef void (*_timeout_func_t)(struct _timeout *t);
S
Sstruct _timeout {
S	sys_dnode_t node;
S	struct k_thread *thread;
S	sys_dlist_t *wait_q;
S	s32_t delta_ticks_from_prev;
S	_timeout_func_t func;
S};
S
Sextern s32_t _timeout_remaining_get(struct _timeout *timeout);
S
S/* Threads */
Stypedef void (*_thread_entry_t)(void *, void *, void *);
S
S#ifdef CONFIG_THREAD_MONITOR
Sstruct __thread_entry {
S	_thread_entry_t pEntry;
S	void *parameter1;
S	void *parameter2;
S	void *parameter3;
S};
S#endif
S
S/* can be used for creating 'dummy' threads, e.g. for pending on objects */
Sstruct _thread_base {
S
S	/* this thread's entry in a ready/wait queue */
S	sys_dnode_t k_q_node;
S
S	/* user facing 'thread options'; values defined in include/kernel.h */
S	u8_t user_options;
S
S	/* thread state */
S	u8_t thread_state;
S
S	/*
S	 * scheduler lock count and thread priority
S	 *
S	 * These two fields control the preemptibility of a thread.
S	 *
S	 * When the scheduler is locked, sched_locked is decremented, which
S	 * means that the scheduler is locked for values from 0xff to 0x01. A
S	 * thread is coop if its prio is negative, thus 0x80 to 0xff when
S	 * looked at the value as unsigned.
S	 *
S	 * By putting them end-to-end, this means that a thread is
S	 * non-preemptible if the bundled value is greater than or equal to
S	 * 0x0080.
S	 */
S	union {
S		struct {
S#if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
S			u8_t sched_locked;
S			s8_t prio;
S#else /* LITTLE and PDP */
S			s8_t prio;
S			u8_t sched_locked;
S#endif
S		};
S		u16_t preempt;
S	};
S
S	/* data returned by APIs */
S	void *swap_data;
S
S	/* this thread's entry in a timeout queue */
S	struct _timeout timeout;
S
S};
S
Stypedef struct _thread_base _thread_base_t;
S
S/* Contains the stack information of a thread */
Sstruct _thread_stack_info {
S	/* Stack Start */
S	u32_t start;
S	/* Stack Size */
S	u32_t size;
S};
S
Stypedef struct _thread_stack_info _thread_stack_info_t;
S
Sstruct k_thread {
S
S	struct _thread_base base;
S
S	/* defined by the architecture, but all archs need these */
S	struct _caller_saved caller_saved;
S	struct _callee_saved callee_saved;
S
S	/* static thread init data */
S	void *init_data;
S
S	/* abort function */
S	void (*fn_abort)(void);
S
S#if defined(CONFIG_THREAD_MONITOR)
S	/* thread entry and parameters description */
S	struct __thread_entry *entry;
S
S	/* next item in list of all threads */
S	struct k_thread *next_thread;
S#endif
S
S#ifdef CONFIG_THREAD_CUSTOM_DATA
S	/* crude thread-local storage */
S	void *custom_data;
S#endif
S
S	/* per-thread errno variable */
S	int errno_var;
S
S	/* Stack Info */
S	struct _thread_stack_info stack_info;
S
S	/* arch-specifics: must always be at the end */
S	struct _thread_arch arch;
S};
S
Stypedef struct k_thread _thread_t;
Stypedef struct k_thread *k_tid_t;
S#define tcs k_thread
S
Senum execution_context_types {
S	K_ISR = 0,
S	K_COOP_THREAD,
S	K_PREEMPT_THREAD,
S};
S
S/**
S * @defgroup profiling_apis Profiling APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @brief Analyze the main, idle, interrupt and system workqueue call stacks
S *
S * This routine calls @ref STACK_ANALYZE on the 4 call stacks declared and
S * maintained by the kernel. The sizes of those 4 call stacks are defined by:
S *
S * CONFIG_MAIN_STACK_SIZE
S * CONFIG_IDLE_STACK_SIZE
S * CONFIG_ISR_STACK_SIZE
S * CONFIG_SYSTEM_WORKQUEUE_STACK_SIZE
S *
S * @note CONFIG_INIT_STACKS and CONFIG_PRINTK must be set for this function to
S * produce output.
S *
S * @return N/A
S */
Sextern void k_call_stacks_analyze(void);
S
S/**
S * @} end defgroup profiling_apis
S */
S
S/**
S * @defgroup thread_apis Thread APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @typedef k_thread_entry_t
S * @brief Thread entry point function type.
S *
S * A thread's entry point function is invoked when the thread starts executing.
S * Up to 3 argument values can be passed to the function.
S *
S * The thread terminates execution permanently if the entry point function
S * returns. The thread is responsible for releasing any shared resources
S * it may own (such as mutexes and dynamically allocated memory), prior to
S * returning.
S *
S * @param p1 First argument.
S * @param p2 Second argument.
S * @param p3 Third argument.
S *
S * @return N/A
S */
Stypedef void (*k_thread_entry_t)(void *p1, void *p2, void *p3);
S
S#endif /* !_ASMLANGUAGE */
S
S
S/*
S * Thread user options. May be needed by assembly code. Common part uses low
S * bits, arch-specific use high bits.
S */
S
S/* system thread that must not abort */
S#define K_ESSENTIAL (1 << 0)
S
S#if defined(CONFIG_FP_SHARING)
S/* thread uses floating point registers */
S#define K_FP_REGS (1 << 1)
S#endif
S
S#ifdef CONFIG_X86
S/* x86 Bitmask definitions for threads user options */
S
S#if defined(CONFIG_FP_SHARING) && defined(CONFIG_SSE)
S/* thread uses SSEx (and also FP) registers */
S#define K_SSE_REGS (1 << 7)
S#endif
S#endif
S
S/* end - thread options */
S
S#if !defined(_ASMLANGUAGE)
S
S/* Using typedef deliberately here, this is quite intended to be an opaque
S * type. K_THREAD_STACK_BUFFER() should be used to access the data within.
S *
S * The purpose of this data type is to clearly distinguish between the
S * declared symbol for a stack (of type k_thread_stack_t) and the underlying
S * buffer which composes the stack data actually used by the underlying
S * thread; they cannot be used interchangably as some arches precede the
S * stack buffer region with guard areas that trigger a MPU or MMU fault
S * if written to.
S *
S * APIs that want to work with the buffer inside should continue to use
S * char *.
S *
S * Stacks should always be created with K_THREAD_STACK_DEFINE().
S */
Sstruct __packed _k_thread_stack_element {
S	char data;
S};
Stypedef struct _k_thread_stack_element *k_thread_stack_t;
S
S/**
S * @brief Spawn a thread.
S *
S * This routine initializes a thread, then schedules it for execution.
S *
S * The new thread may be scheduled for immediate execution or a delayed start.
S * If the newly spawned thread does not have a delayed start the kernel
S * scheduler may preempt the current thread to allow the new thread to
S * execute.
S *
S * Kernel data structures for bookkeeping and context storage for this thread
S * will be placed at the beginning of the thread's stack memory region and may
S * become corrupted if too much of the stack is used. This function has been
S * deprecated in favor of k_thread_create() to give the user more control on
S * where these data structures reside.
S *
S * Thread options are architecture-specific, and can include K_ESSENTIAL,
S * K_FP_REGS, and K_SSE_REGS. Multiple options may be specified by separating
S * them using "|" (the logical OR operator).
S *
S * The stack itself should be declared with K_THREAD_STACK_DEFINE or variant
S * macros. The stack size parameter should either be a defined constant
S * also passed to K_THREAD_STACK_DEFINE, or the value of K_THREAD_STACK_SIZEOF.
S * Do not use regular C sizeof().
S *
S * @param stack Pointer to the stack space.
S * @param stack_size Stack size in bytes.
S * @param entry Thread entry function.
S * @param p1 1st entry point parameter.
S * @param p2 2nd entry point parameter.
S * @param p3 3rd entry point parameter.
S * @param prio Thread priority.
S * @param options Thread options.
S * @param delay Scheduling delay (in milliseconds), or K_NO_WAIT (for no delay).
S *
S * @return ID of new thread.
S */
Sextern __deprecated k_tid_t k_thread_spawn(k_thread_stack_t stack,
S			size_t stack_size, k_thread_entry_t entry,
S			void *p1, void *p2, void *p3,
S			int prio, u32_t options, s32_t delay);
S
S/**
S * @brief Create a thread.
S *
S * This routine initializes a thread, then schedules it for execution.
S *
S * The new thread may be scheduled for immediate execution or a delayed start.
S * If the newly spawned thread does not have a delayed start the kernel
S * scheduler may preempt the current thread to allow the new thread to
S * execute.
S *
S * Thread options are architecture-specific, and can include K_ESSENTIAL,
S * K_FP_REGS, and K_SSE_REGS. Multiple options may be specified by separating
S * them using "|" (the logical OR operator).
S *
S * Historically, users often would use the beginning of the stack memory region
S * to store the struct k_thread data, although corruption will occur if the
S * stack overflows this region and stack protection features may not detect this
S * situation.
S *
S * @param new_thread Pointer to uninitialized struct k_thread
S * @param stack Pointer to the stack space.
S * @param stack_size Stack size in bytes.
S * @param entry Thread entry function.
S * @param p1 1st entry point parameter.
S * @param p2 2nd entry point parameter.
S * @param p3 3rd entry point parameter.
S * @param prio Thread priority.
S * @param options Thread options.
S * @param delay Scheduling delay (in milliseconds), or K_NO_WAIT (for no delay).
S *
S * @return ID of new thread.
S */
Sextern k_tid_t k_thread_create(struct k_thread *new_thread,
S			       k_thread_stack_t stack,
S			       size_t stack_size,
S			       void (*entry)(void *, void *, void*),
S			       void *p1, void *p2, void *p3,
S			       int prio, u32_t options, s32_t delay);
S
S/**
S * @brief Put the current thread to sleep.
S *
S * This routine puts the current thread to sleep for @a duration
S * milliseconds.
S *
S * @param duration Number of milliseconds to sleep.
S *
S * @return N/A
S */
Sextern void k_sleep(s32_t duration);
S
S/**
S * @brief Cause the current thread to busy wait.
S *
S * This routine causes the current thread to execute a "do nothing" loop for
S * @a usec_to_wait microseconds.
S *
S * @return N/A
S */
Sextern void k_busy_wait(u32_t usec_to_wait);
S
S/**
S * @brief Yield the current thread.
S *
S * This routine causes the current thread to yield execution to another
S * thread of the same or higher priority. If there are no other ready threads
S * of the same or higher priority, the routine returns immediately.
S *
S * @return N/A
S */
Sextern void k_yield(void);
S
S/**
S * @brief Wake up a sleeping thread.
S *
S * This routine prematurely wakes up @a thread from sleeping.
S *
S * If @a thread is not currently sleeping, the routine has no effect.
S *
S * @param thread ID of thread to wake.
S *
S * @return N/A
S */
Sextern void k_wakeup(k_tid_t thread);
S
S/**
S * @brief Get thread ID of the current thread.
S *
S * @return ID of current thread.
S */
Sextern k_tid_t k_current_get(void);
S
S/**
S * @brief Cancel thread performing a delayed start.
S *
S * This routine prevents @a thread from executing if it has not yet started
S * execution. The thread must be re-spawned before it will execute.
S *
S * @param thread ID of thread to cancel.
S *
S * @retval 0 Thread spawning canceled.
S * @retval -EINVAL Thread has already started executing.
S */
Sextern int k_thread_cancel(k_tid_t thread);
S
S/**
S * @brief Abort a thread.
S *
S * This routine permanently stops execution of @a thread. The thread is taken
S * off all kernel queues it is part of (i.e. the ready queue, the timeout
S * queue, or a kernel object wait queue). However, any kernel resources the
S * thread might currently own (such as mutexes or memory blocks) are not
S * released. It is the responsibility of the caller of this routine to ensure
S * all necessary cleanup is performed.
S *
S * @param thread ID of thread to abort.
S *
S * @return N/A
S */
Sextern void k_thread_abort(k_tid_t thread);
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
S/* timeout has timed out and is not on _timeout_q anymore */
S#define _EXPIRED (-2)
S
S/* timeout is not in use */
S#define _INACTIVE (-1)
S
Sstruct _static_thread_data {
S	struct k_thread *init_thread;
S	k_thread_stack_t init_stack;
S	unsigned int init_stack_size;
S	void (*init_entry)(void *, void *, void *);
S	void *init_p1;
S	void *init_p2;
S	void *init_p3;
S	int init_prio;
S	u32_t init_options;
S	s32_t init_delay;
S	void (*init_abort)(void);
S	u32_t init_groups;
S};
S
S#define _THREAD_INITIALIZER(thread, stack, stack_size,           \
S			    entry, p1, p2, p3,                   \
S			    prio, options, delay, abort, groups) \
S	{                                                        \
S	.init_thread = (thread),				 \
S	.init_stack = (stack),					 \
S	.init_stack_size = (stack_size),                         \
S	.init_entry = (void (*)(void *, void *, void *))entry,   \
S	.init_p1 = (void *)p1,                                   \
S	.init_p2 = (void *)p2,                                   \
S	.init_p3 = (void *)p3,                                   \
S	.init_prio = (prio),                                     \
S	.init_options = (options),                               \
S	.init_delay = (delay),                                   \
S	.init_abort = (abort),                                   \
S	.init_groups = (groups),                                 \
S	}
X#define _THREAD_INITIALIZER(thread, stack, stack_size,           			    entry, p1, p2, p3,                   			    prio, options, delay, abort, groups) 	{                                                        	.init_thread = (thread),				 	.init_stack = (stack),					 	.init_stack_size = (stack_size),                         	.init_entry = (void (*)(void *, void *, void *))entry,   	.init_p1 = (void *)p1,                                   	.init_p2 = (void *)p2,                                   	.init_p3 = (void *)p3,                                   	.init_prio = (prio),                                     	.init_options = (options),                               	.init_delay = (delay),                                   	.init_abort = (abort),                                   	.init_groups = (groups),                                 	}
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S/**
S * @brief Statically define and initialize a thread.
S *
S * The thread may be scheduled for immediate execution or a delayed start.
S *
S * Thread options are architecture-specific, and can include K_ESSENTIAL,
S * K_FP_REGS, and K_SSE_REGS. Multiple options may be specified by separating
S * them using "|" (the logical OR operator).
S *
S * The ID of the thread can be accessed using:
S *
S * @code extern const k_tid_t <name>; @endcode
S *
S * @param name Name of the thread.
S * @param stack_size Stack size in bytes.
S * @param entry Thread entry function.
S * @param p1 1st entry point parameter.
S * @param p2 2nd entry point parameter.
S * @param p3 3rd entry point parameter.
S * @param prio Thread priority.
S * @param options Thread options.
S * @param delay Scheduling delay (in milliseconds), or K_NO_WAIT (for no delay).
S *
S * @internal It has been observed that the x86 compiler by default aligns
S * these _static_thread_data structures to 32-byte boundaries, thereby
S * wasting space. To work around this, force a 4-byte alignment.
S */
S#define K_THREAD_DEFINE(name, stack_size,                                \
S			entry, p1, p2, p3,                               \
S			prio, options, delay)                            \
S	K_THREAD_STACK_DEFINE(_k_thread_stack_##name, stack_size);	 \
S	struct k_thread _k_thread_obj_##name;				 \
S	struct _static_thread_data _k_thread_data_##name __aligned(4)    \
S		__in_section(_static_thread_data, static, name) =        \
S		_THREAD_INITIALIZER(&_k_thread_obj_##name,		 \
S				    _k_thread_stack_##name, stack_size,  \
S				entry, p1, p2, p3, prio, options, delay, \
S				NULL, 0);				 \
S	const k_tid_t name = (k_tid_t)&_k_thread_obj_##name
X#define K_THREAD_DEFINE(name, stack_size,                                			entry, p1, p2, p3,                               			prio, options, delay)                            	K_THREAD_STACK_DEFINE(_k_thread_stack_##name, stack_size);	 	struct k_thread _k_thread_obj_##name;				 	struct _static_thread_data _k_thread_data_##name __aligned(4)    		__in_section(_static_thread_data, static, name) =        		_THREAD_INITIALIZER(&_k_thread_obj_##name,		 				    _k_thread_stack_##name, stack_size,  				entry, p1, p2, p3, prio, options, delay, 				NULL, 0);				 	const k_tid_t name = (k_tid_t)&_k_thread_obj_##name
S
S/**
S * @brief Get a thread's priority.
S *
S * This routine gets the priority of @a thread.
S *
S * @param thread ID of thread whose priority is needed.
S *
S * @return Priority of @a thread.
S */
Sextern int  k_thread_priority_get(k_tid_t thread);
S
S/**
S * @brief Set a thread's priority.
S *
S * This routine immediately changes the priority of @a thread.
S *
S * Rescheduling can occur immediately depending on the priority @a thread is
S * set to:
S *
S * - If its priority is raised above the priority of the caller of this
S * function, and the caller is preemptible, @a thread will be scheduled in.
S *
S * - If the caller operates on itself, it lowers its priority below that of
S * other threads in the system, and the caller is preemptible, the thread of
S * highest priority will be scheduled in.
S *
S * Priority can be assigned in the range of -CONFIG_NUM_COOP_PRIORITIES to
S * CONFIG_NUM_PREEMPT_PRIORITIES-1, where -CONFIG_NUM_COOP_PRIORITIES is the
S * highest priority.
S *
S * @param thread ID of thread whose priority is to be set.
S * @param prio New priority.
S *
S * @warning Changing the priority of a thread currently involved in mutex
S * priority inheritance may result in undefined behavior.
S *
S * @return N/A
S */
Sextern void k_thread_priority_set(k_tid_t thread, int prio);
S
S/**
S * @brief Suspend a thread.
S *
S * This routine prevents the kernel scheduler from making @a thread the
S * current thread. All other internal operations on @a thread are still
S * performed; for example, any timeout it is waiting on keeps ticking,
S * kernel objects it is waiting on are still handed to it, etc.
S *
S * If @a thread is already suspended, the routine has no effect.
S *
S * @param thread ID of thread to suspend.
S *
S * @return N/A
S */
Sextern void k_thread_suspend(k_tid_t thread);
S
S/**
S * @brief Resume a suspended thread.
S *
S * This routine allows the kernel scheduler to make @a thread the current
S * thread, when it is next eligible for that role.
S *
S * If @a thread is not currently suspended, the routine has no effect.
S *
S * @param thread ID of thread to resume.
S *
S * @return N/A
S */
Sextern void k_thread_resume(k_tid_t thread);
S
S/**
S * @brief Set time-slicing period and scope.
S *
S * This routine specifies how the scheduler will perform time slicing of
S * preemptible threads.
S *
S * To enable time slicing, @a slice must be non-zero. The scheduler
S * ensures that no thread runs for more than the specified time limit
S * before other threads of that priority are given a chance to execute.
S * Any thread whose priority is higher than @a prio is exempted, and may
S * execute as long as desired without being preempted due to time slicing.
S *
S * Time slicing only limits the maximum amount of time a thread may continuously
S * execute. Once the scheduler selects a thread for execution, there is no
S * minimum guaranteed time the thread will execute before threads of greater or
S * equal priority are scheduled.
S *
S * When the current thread is the only one of that priority eligible
S * for execution, this routine has no effect; the thread is immediately
S * rescheduled after the slice period expires.
S *
S * To disable timeslicing, set both @a slice and @a prio to zero.
S *
S * @param slice Maximum time slice length (in milliseconds).
S * @param prio Highest thread priority level eligible for time slicing.
S *
S * @return N/A
S */
Sextern void k_sched_time_slice_set(s32_t slice, int prio);
S
S/**
S * @} end defgroup thread_apis
S */
S
S/**
S * @addtogroup isr_apis
S * @{
S */
S
S/**
S * @brief Determine if code is running at interrupt level.
S *
S * This routine allows the caller to customize its actions, depending on
S * whether it is a thread or an ISR.
S *
S * @note Can be called by ISRs.
S *
S * @return 0 if invoked by a thread.
S * @return Non-zero if invoked by an ISR.
S */
Sextern int k_is_in_isr(void);
S
S/**
S * @brief Determine if code is running in a preemptible thread.
S *
S * This routine allows the caller to customize its actions, depending on
S * whether it can be preempted by another thread. The routine returns a 'true'
S * value if all of the following conditions are met:
S *
S * - The code is running in a thread, not at ISR.
S * - The thread's priority is in the preemptible range.
S * - The thread has not locked the scheduler.
S *
S * @note Can be called by ISRs.
S *
S * @return 0 if invoked by an ISR or by a cooperative thread.
S * @return Non-zero if invoked by a preemptible thread.
S */
Sextern int k_is_preempt_thread(void);
S
S/**
S * @} end addtogroup isr_apis
S */
S
S/**
S * @addtogroup thread_apis
S * @{
S */
S
S/**
S * @brief Lock the scheduler.
S *
S * This routine prevents the current thread from being preempted by another
S * thread by instructing the scheduler to treat it as a cooperative thread.
S * If the thread subsequently performs an operation that makes it unready,
S * it will be context switched out in the normal manner. When the thread
S * again becomes the current thread, its non-preemptible status is maintained.
S *
S * This routine can be called recursively.
S *
S * @note k_sched_lock() and k_sched_unlock() should normally be used
S * when the operation being performed can be safely interrupted by ISRs.
S * However, if the amount of processing involved is very small, better
S * performance may be obtained by using irq_lock() and irq_unlock().
S *
S * @return N/A
S */
Sextern void k_sched_lock(void);
S
S/**
S * @brief Unlock the scheduler.
S *
S * This routine reverses the effect of a previous call to k_sched_lock().
S * A thread must call the routine once for each time it called k_sched_lock()
S * before the thread becomes preemptible.
S *
S * @return N/A
S */
Sextern void k_sched_unlock(void);
S
S/**
S * @brief Set current thread's custom data.
S *
S * This routine sets the custom data for the current thread to @ value.
S *
S * Custom data is not used by the kernel itself, and is freely available
S * for a thread to use as it sees fit. It can be used as a framework
S * upon which to build thread-local storage.
S *
S * @param value New custom data value.
S *
S * @return N/A
S */
Sextern void k_thread_custom_data_set(void *value);
S
S/**
S * @brief Get current thread's custom data.
S *
S * This routine returns the custom data for the current thread.
S *
S * @return Current custom data value.
S */
Sextern void *k_thread_custom_data_get(void);
S
S/**
S * @} end addtogroup thread_apis
S */
S
S#include <sys_clock.h>
S
S/**
S * @addtogroup clock_apis
S * @{
S */
S
S/**
S * @brief Generate null timeout delay.
S *
S * This macro generates a timeout delay that that instructs a kernel API
S * not to wait if the requested operation cannot be performed immediately.
S *
S * @return Timeout delay value.
S */
S#define K_NO_WAIT 0
S
S/**
S * @brief Generate timeout delay from milliseconds.
S *
S * This macro generates a timeout delay that that instructs a kernel API
S * to wait up to @a ms milliseconds to perform the requested operation.
S *
S * @param ms Duration in milliseconds.
S *
S * @return Timeout delay value.
S */
S#define K_MSEC(ms)     (ms)
S
S/**
S * @brief Generate timeout delay from seconds.
S *
S * This macro generates a timeout delay that that instructs a kernel API
S * to wait up to @a s seconds to perform the requested operation.
S *
S * @param s Duration in seconds.
S *
S * @return Timeout delay value.
S */
S#define K_SECONDS(s)   K_MSEC((s) * MSEC_PER_SEC)
S
S/**
S * @brief Generate timeout delay from minutes.
S *
S * This macro generates a timeout delay that that instructs a kernel API
S * to wait up to @a m minutes to perform the requested operation.
S *
S * @param m Duration in minutes.
S *
S * @return Timeout delay value.
S */
S#define K_MINUTES(m)   K_SECONDS((m) * 60)
S
S/**
S * @brief Generate timeout delay from hours.
S *
S * This macro generates a timeout delay that that instructs a kernel API
S * to wait up to @a h hours to perform the requested operation.
S *
S * @param h Duration in hours.
S *
S * @return Timeout delay value.
S */
S#define K_HOURS(h)     K_MINUTES((h) * 60)
S
S/**
S * @brief Generate infinite timeout delay.
S *
S * This macro generates a timeout delay that that instructs a kernel API
S * to wait as long as necessary to perform the requested operation.
S *
S * @return Timeout delay value.
S */
S#define K_FOREVER (-1)
S
S/**
S * @} end addtogroup clock_apis
S */
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
S/* kernel clocks */
S
S#if	(sys_clock_ticks_per_sec == 1000) || \
S	(sys_clock_ticks_per_sec == 500)  || \
S	(sys_clock_ticks_per_sec == 250)  || \
S	(sys_clock_ticks_per_sec == 125)  || \
S	(sys_clock_ticks_per_sec == 100)  || \
S	(sys_clock_ticks_per_sec == 50)   || \
S	(sys_clock_ticks_per_sec == 25)   || \
S	(sys_clock_ticks_per_sec == 20)   || \
S	(sys_clock_ticks_per_sec == 10)   || \
S	(sys_clock_ticks_per_sec == 1)
X#if	(sys_clock_ticks_per_sec == 1000) || 	(sys_clock_ticks_per_sec == 500)  || 	(sys_clock_ticks_per_sec == 250)  || 	(sys_clock_ticks_per_sec == 125)  || 	(sys_clock_ticks_per_sec == 100)  || 	(sys_clock_ticks_per_sec == 50)   || 	(sys_clock_ticks_per_sec == 25)   || 	(sys_clock_ticks_per_sec == 20)   || 	(sys_clock_ticks_per_sec == 10)   || 	(sys_clock_ticks_per_sec == 1)
S
S	#define _ms_per_tick (MSEC_PER_SEC / sys_clock_ticks_per_sec)
S#else
S	/* yields horrible 64-bit math on many architectures: try to avoid */
S	#define _NON_OPTIMIZED_TICKS_PER_SEC
S#endif
S
S#ifdef _NON_OPTIMIZED_TICKS_PER_SEC
Sextern s32_t _ms_to_ticks(s32_t ms);
S#else
Sstatic ALWAYS_INLINE s32_t _ms_to_ticks(s32_t ms)
S{
S	return (s32_t)ceiling_fraction((u32_t)ms, _ms_per_tick);
S}
S#endif
S
S/* added tick needed to account for tick in progress */
S#define _TICK_ALIGN 1
S
Sstatic inline s64_t __ticks_to_ms(s64_t ticks)
S{
S#ifdef _NON_OPTIMIZED_TICKS_PER_SEC
S	return (MSEC_PER_SEC * (u64_t)ticks) / sys_clock_ticks_per_sec;
S#else
S	return (u64_t)ticks * _ms_per_tick;
S#endif
S}
S
Sstruct k_timer {
S	/*
S	 * _timeout structure must be first here if we want to use
S	 * dynamic timer allocation. timeout.node is used in the double-linked
S	 * list of free timers
S	 */
S	struct _timeout timeout;
S
S	/* wait queue for the (single) thread waiting on this timer */
S	_wait_q_t wait_q;
S
S	/* runs in ISR context */
S	void (*expiry_fn)(struct k_timer *);
S
S	/* runs in the context of the thread that calls k_timer_stop() */
S	void (*stop_fn)(struct k_timer *);
S
S	/* timer period */
S	s32_t period;
S
S	/* timer status */
S	u32_t status;
S
S	/* user-specific data, also used to support legacy features */
S	void *user_data;
S
S//	_OBJECT_TRACING_NEXT_PTR(k_timer);
S};
S
S#define _K_TIMER_INITIALIZER(obj, expiry, stop) \
S	{ \
S	.timeout.delta_ticks_from_prev = _INACTIVE, \
S	.timeout.wait_q = NULL, \
S	.timeout.thread = NULL, \
S	.timeout.func = _timer_expiration_handler, \
S	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), \
S	.expiry_fn = expiry, \
S	.stop_fn = stop, \
S	.status = 0, \
S	.user_data = 0, \
S	_OBJECT_TRACING_INIT \
S	}
X#define _K_TIMER_INITIALIZER(obj, expiry, stop) 	{ 	.timeout.delta_ticks_from_prev = _INACTIVE, 	.timeout.wait_q = NULL, 	.timeout.thread = NULL, 	.timeout.func = _timer_expiration_handler, 	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), 	.expiry_fn = expiry, 	.stop_fn = stop, 	.status = 0, 	.user_data = 0, 	_OBJECT_TRACING_INIT 	}
S
S#define K_TIMER_INITIALIZER DEPRECATED_MACRO _K_TIMER_INITIALIZER
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S/**
S * @defgroup timer_apis Timer APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @typedef k_timer_expiry_t
S * @brief Timer expiry function type.
S *
S * A timer's expiry function is executed by the system clock interrupt handler
S * each time the timer expires. The expiry function is optional, and is only
S * invoked if the timer has been initialized with one.
S *
S * @param timer     Address of timer.
S *
S * @return N/A
S */
Stypedef void (*k_timer_expiry_t)(struct k_timer *timer);
S
S/**
S * @typedef k_timer_stop_t
S * @brief Timer stop function type.
S *
S * A timer's stop function is executed if the timer is stopped prematurely.
S * The function runs in the context of the thread that stops the timer.
S * The stop function is optional, and is only invoked if the timer has been
S * initialized with one.
S *
S * @param timer     Address of timer.
S *
S * @return N/A
S */
Stypedef void (*k_timer_stop_t)(struct k_timer *timer);
S
S/**
S * @brief Statically define and initialize a timer.
S *
S * The timer can be accessed outside the module where it is defined using:
S *
S * @code extern struct k_timer <name>; @endcode
S *
S * @param name Name of the timer variable.
S * @param expiry_fn Function to invoke each time the timer expires.
S * @param stop_fn   Function to invoke if the timer is stopped while running.
S */
S#define K_TIMER_DEFINE(name, expiry_fn, stop_fn) \
S	struct k_timer name \
S		__in_section(_k_timer, static, name) = \
S		_K_TIMER_INITIALIZER(name, expiry_fn, stop_fn)
X#define K_TIMER_DEFINE(name, expiry_fn, stop_fn) 	struct k_timer name 		__in_section(_k_timer, static, name) = 		_K_TIMER_INITIALIZER(name, expiry_fn, stop_fn)
S
S/**
S * @brief Initialize a timer.
S *
S * This routine initializes a timer, prior to its first use.
S *
S * @param timer     Address of timer.
S * @param expiry_fn Function to invoke each time the timer expires.
S * @param stop_fn   Function to invoke if the timer is stopped while running.
S *
S * @return N/A
S */
Sextern void k_timer_init(struct k_timer *timer,
S			 k_timer_expiry_t expiry_fn,
S			 k_timer_stop_t stop_fn);
S
S/**
S * @brief Start a timer.
S *
S * This routine starts a timer, and resets its status to zero. The timer
S * begins counting down using the specified duration and period values.
S *
S * Attempting to start a timer that is already running is permitted.
S * The timer's status is reset to zero and the timer begins counting down
S * using the new duration and period values.
S *
S * @param timer     Address of timer.
S * @param duration  Initial timer duration (in milliseconds).
S * @param period    Timer period (in milliseconds).
S *
S * @return N/A
S */
Sextern void k_timer_start(struct k_timer *timer,
S			  s32_t duration, s32_t period);
S
S/**
S * @brief Stop a timer.
S *
S * This routine stops a running timer prematurely. The timer's stop function,
S * if one exists, is invoked by the caller.
S *
S * Attempting to stop a timer that is not running is permitted, but has no
S * effect on the timer.
S *
S * @note Can be called by ISRs.  The stop handler has to be callable from ISRs
S * if @a k_timer_stop is to be called from ISRs.
S *
S * @param timer     Address of timer.
S *
S * @return N/A
S */
Sextern void k_timer_stop(struct k_timer *timer);
S
S/**
S * @brief Read timer status.
S *
S * This routine reads the timer's status, which indicates the number of times
S * it has expired since its status was last read.
S *
S * Calling this routine resets the timer's status to zero.
S *
S * @param timer     Address of timer.
S *
S * @return Timer status.
S */
Sextern u32_t k_timer_status_get(struct k_timer *timer);
S
S/**
S * @brief Synchronize thread to timer expiration.
S *
S * This routine blocks the calling thread until the timer's status is non-zero
S * (indicating that it has expired at least once since it was last examined)
S * or the timer is stopped. If the timer status is already non-zero,
S * or the timer is already stopped, the caller continues without waiting.
S *
S * Calling this routine resets the timer's status to zero.
S *
S * This routine must not be used by interrupt handlers, since they are not
S * allowed to block.
S *
S * @param timer     Address of timer.
S *
S * @return Timer status.
S */
Sextern u32_t k_timer_status_sync(struct k_timer *timer);
S
S/**
S * @brief Get time remaining before a timer next expires.
S *
S * This routine computes the (approximate) time remaining before a running
S * timer next expires. If the timer is not running, it returns zero.
S *
S * @param timer     Address of timer.
S *
S * @return Remaining time (in milliseconds).
S */
Sstatic inline s32_t k_timer_remaining_get(struct k_timer *timer)
S{
S	return _timeout_remaining_get(&timer->timeout);
S}
S
S/**
S * @brief Associate user-specific data with a timer.
S *
S * This routine records the @a user_data with the @a timer, to be retrieved
S * later.
S *
S * It can be used e.g. in a timer handler shared across multiple subsystems to
S * retrieve data specific to the subsystem this timer is associated with.
S *
S * @param timer     Address of timer.
S * @param user_data User data to associate with the timer.
S *
S * @return N/A
S */
Sstatic inline void k_timer_user_data_set(struct k_timer *timer,
S					 void *user_data)
S{
S	timer->user_data = user_data;
S}
S
S/**
S * @brief Retrieve the user-specific data from a timer.
S *
S * @param timer     Address of timer.
S *
S * @return The user data.
S */
Sstatic inline void *k_timer_user_data_get(struct k_timer *timer)
S{
S	return timer->user_data;
S}
S
S/**
S * @} end defgroup timer_apis
S */
S
S/**
S * @addtogroup clock_apis
S * @{
S */
S
S/**
S * @brief Get system uptime.
S *
S * This routine returns the elapsed time since the system booted,
S * in milliseconds.
S *
S * @return Current uptime.
S */
Sextern s64_t k_uptime_get(void);
S
S#define k_enable_sys_clock_always_on() do { } while ((0))
S#define k_disable_sys_clock_always_on() do { } while ((0))
S
S/**
S * @brief Get system uptime (32-bit version).
S *
S * This routine returns the lower 32-bits of the elapsed time since the system
S * booted, in milliseconds.
S *
S * This routine can be more efficient than k_uptime_get(), as it reduces the
S * need for interrupt locking and 64-bit math. However, the 32-bit result
S * cannot hold a system uptime time larger than approximately 50 days, so the
S * caller must handle possible rollovers.
S *
S * @return Current uptime.
S */
Sextern u32_t k_uptime_get_32(void);
S
S/**
S * @brief Get elapsed time.
S *
S * This routine computes the elapsed time between the current system uptime
S * and an earlier reference time, in milliseconds.
S *
S * @param reftime Pointer to a reference time, which is updated to the current
S *                uptime upon return.
S *
S * @return Elapsed time.
S */
Sextern s64_t k_uptime_delta(s64_t *reftime);
S
S/**
S * @brief Get elapsed time (32-bit version).
S *
S * This routine computes the elapsed time between the current system uptime
S * and an earlier reference time, in milliseconds.
S *
S * This routine can be more efficient than k_uptime_delta(), as it reduces the
S * need for interrupt locking and 64-bit math. However, the 32-bit result
S * cannot hold an elapsed time larger than approximately 50 days, so the
S * caller must handle possible rollovers.
S *
S * @param reftime Pointer to a reference time, which is updated to the current
S *                uptime upon return.
S *
S * @return Elapsed time.
S */
Sextern u32_t k_uptime_delta_32(s64_t *reftime);
S
S/**
S * @brief Read the hardware clock.
S *
S * This routine returns the current time, as measured by the system's hardware
S * clock.
S *
S * @return Current hardware clock up-counter (in cycles).
S */
S#define k_cycle_get_32()	_arch_k_cycle_get_32()
S
S/**
S * @} end addtogroup clock_apis
S */
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
Sstruct k_queue {
S	sys_slist_t data_q;
S	union {
S		_wait_q_t wait_q;
S
S		_POLL_EVENT;
S	};
S
S//	_OBJECT_TRACING_NEXT_PTR(k_queue);
S};
S
S#define _K_QUEUE_INITIALIZER(obj) \
S	{ \
S	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), \
S	.data_q = SYS_SLIST_STATIC_INIT(&obj.data_q), \
S	_POLL_EVENT_OBJ_INIT(obj) \
S	_OBJECT_TRACING_INIT \
S	}
X#define _K_QUEUE_INITIALIZER(obj) 	{ 	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), 	.data_q = SYS_SLIST_STATIC_INIT(&obj.data_q), 	_POLL_EVENT_OBJ_INIT(obj) 	_OBJECT_TRACING_INIT 	}
S
S#define K_QUEUE_INITIALIZER DEPRECATED_MACRO _K_QUEUE_INITIALIZER
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S/**
S * @defgroup queue_apis Queue APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @brief Initialize a queue.
S *
S * This routine initializes a queue object, prior to its first use.
S *
S * @param queue Address of the queue.
S *
S * @return N/A
S */
Sextern void k_queue_init(struct k_queue *queue);
S
S/**
S * @brief Cancel waiting on a queue.
S *
S * This routine causes first thread pending on @a queue, if any, to
S * return from k_queue_get() call with NULL value (as if timeout expired).
S *
S * @note Can be called by ISRs.
S *
S * @param queue Address of the queue.
S *
S * @return N/A
S */
Sextern void k_queue_cancel_wait(struct k_queue *queue);
S
S/**
S * @brief Append an element to the end of a queue.
S *
S * This routine appends a data item to @a queue. A queue data item must be
S * aligned on a 4-byte boundary, and the first 32 bits of the item are
S * reserved for the kernel's use.
S *
S * @note Can be called by ISRs.
S *
S * @param queue Address of the queue.
S * @param data Address of the data item.
S *
S * @return N/A
S */
Sextern void k_queue_append(struct k_queue *queue, void *data);
S
S/**
S * @brief Prepend an element to a queue.
S *
S * This routine prepends a data item to @a queue. A queue data item must be
S * aligned on a 4-byte boundary, and the first 32 bits of the item are
S * reserved for the kernel's use.
S *
S * @note Can be called by ISRs.
S *
S * @param queue Address of the queue.
S * @param data Address of the data item.
S *
S * @return N/A
S */
Sextern void k_queue_prepend(struct k_queue *queue, void *data);
S
S/**
S * @brief Inserts an element to a queue.
S *
S * This routine inserts a data item to @a queue after previous item. A queue
S * data item must be aligned on a 4-byte boundary, and the first 32 bits of the
S * item are reserved for the kernel's use.
S *
S * @note Can be called by ISRs.
S *
S * @param queue Address of the queue.
S * @param prev Address of the previous data item.
S * @param data Address of the data item.
S *
S * @return N/A
S */
Sextern void k_queue_insert(struct k_queue *queue, void *prev, void *data);
S
S/**
S * @brief Atomically append a list of elements to a queue.
S *
S * This routine adds a list of data items to @a queue in one operation.
S * The data items must be in a singly-linked list, with the first 32 bits
S * in each data item pointing to the next data item; the list must be
S * NULL-terminated.
S *
S * @note Can be called by ISRs.
S *
S * @param queue Address of the queue.
S * @param head Pointer to first node in singly-linked list.
S * @param tail Pointer to last node in singly-linked list.
S *
S * @return N/A
S */
Sextern void k_queue_append_list(struct k_queue *queue, void *head, void *tail);
S
S/**
S * @brief Atomically add a list of elements to a queue.
S *
S * This routine adds a list of data items to @a queue in one operation.
S * The data items must be in a singly-linked list implemented using a
S * sys_slist_t object. Upon completion, the original list is empty.
S *
S * @note Can be called by ISRs.
S *
S * @param queue Address of the queue.
S * @param list Pointer to sys_slist_t object.
S *
S * @return N/A
S */
Sextern void k_queue_merge_slist(struct k_queue *queue, sys_slist_t *list);
S
S/**
S * @brief Get an element from a queue.
S *
S * This routine removes first data item from @a queue. The first 32 bits of the
S * data item are reserved for the kernel's use.
S *
S * @note Can be called by ISRs, but @a timeout must be set to K_NO_WAIT.
S *
S * @param queue Address of the queue.
S * @param timeout Waiting period to obtain a data item (in milliseconds),
S *                or one of the special values K_NO_WAIT and K_FOREVER.
S *
S * @return Address of the data item if successful; NULL if returned
S * without waiting, or waiting period timed out.
S */
Sextern void *k_queue_get(struct k_queue *queue, s32_t timeout);
S
S/**
S * @brief Remove an element from a queue.
S *
S * This routine removes data item from @a queue. The first 32 bits of the
S * data item are reserved for the kernel's use. Removing elements from k_queue
S * rely on sys_slist_find_and_remove which is not a constant time operation.
S *
S * @note Can be called by ISRs
S *
S * @param queue Address of the queue.
S * @param data Address of the data item.
S *
S * @return true if data item was removed
S */
Sstatic inline bool k_queue_remove(struct k_queue *queue, void *data)
S{
S	return sys_slist_find_and_remove(&queue->data_q, (sys_snode_t *)data);
S}
S
S/**
S * @brief Query a queue to see if it has data available.
S *
S * Note that the data might be already gone by the time this function returns
S * if other threads are also trying to read from the queue.
S *
S * @note Can be called by ISRs.
S *
S * @param queue Address of the queue.
S *
S * @return Non-zero if the queue is empty.
S * @return 0 if data is available.
S */
Sstatic inline int k_queue_is_empty(struct k_queue *queue)
S{
S	return (int)sys_slist_is_empty(&queue->data_q);
S}
S
S/**
S * @brief Peek element at the head of queue.
S *
S * Return element from the head of queue without removing it.
S *
S * @param queue Address of the queue.
S *
S * @return Head element, or NULL if queue is empty.
S */
Sstatic inline void *k_queue_peek_head(struct k_queue *queue)
S{
S	return sys_slist_peek_head(&queue->data_q);
S}
S
S/**
S * @brief Peek element at the tail of queue.
S *
S * Return element from the tail of queue without removing it.
S *
S * @param queue Address of the queue.
S *
S * @return Tail element, or NULL if queue is empty.
S */
Sstatic inline void *k_queue_peek_tail(struct k_queue *queue)
S{
S	return sys_slist_peek_tail(&queue->data_q);
S}
S
S/**
S * @brief Statically define and initialize a queue.
S *
S * The queue can be accessed outside the module where it is defined using:
S *
S * @code extern struct k_queue <name>; @endcode
S *
S * @param name Name of the queue.
S */
S#define K_QUEUE_DEFINE(name) \
S	struct k_queue name \
S		__in_section(_k_queue, static, name) = \
S		_K_QUEUE_INITIALIZER(name)
X#define K_QUEUE_DEFINE(name) 	struct k_queue name 		__in_section(_k_queue, static, name) = 		_K_QUEUE_INITIALIZER(name)
S
S/**
S * @} end defgroup queue_apis
S */
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
Sstruct k_fifo {
S	struct k_queue _queue;
S};
S
S#define _K_FIFO_INITIALIZER(obj) \
S	{ \
S	._queue = _K_QUEUE_INITIALIZER(obj._queue) \
S	}
X#define _K_FIFO_INITIALIZER(obj) 	{ 	._queue = _K_QUEUE_INITIALIZER(obj._queue) 	}
S
S#define K_FIFO_INITIALIZER DEPRECATED_MACRO _K_FIFO_INITIALIZER
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S/**
S * @defgroup fifo_apis Fifo APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @brief Initialize a fifo.
S *
S * This routine initializes a fifo object, prior to its first use.
S *
S * @param fifo Address of the fifo.
S *
S * @return N/A
S */
S#define k_fifo_init(fifo) \
S	k_queue_init((struct k_queue *) fifo)
X#define k_fifo_init(fifo) 	k_queue_init((struct k_queue *) fifo)
S
S/**
S * @brief Cancel waiting on a fifo.
S *
S * This routine causes first thread pending on @a fifo, if any, to
S * return from k_fifo_get() call with NULL value (as if timeout
S * expired).
S *
S * @note Can be called by ISRs.
S *
S * @param fifo Address of the fifo.
S *
S * @return N/A
S */
S#define k_fifo_cancel_wait(fifo) \
S	k_queue_cancel_wait((struct k_queue *) fifo)
X#define k_fifo_cancel_wait(fifo) 	k_queue_cancel_wait((struct k_queue *) fifo)
S
S/**
S * @brief Add an element to a fifo.
S *
S * This routine adds a data item to @a fifo. A fifo data item must be
S * aligned on a 4-byte boundary, and the first 32 bits of the item are
S * reserved for the kernel's use.
S *
S * @note Can be called by ISRs.
S *
S * @param fifo Address of the fifo.
S * @param data Address of the data item.
S *
S * @return N/A
S */
S#define k_fifo_put(fifo, data) \
S	k_queue_append((struct k_queue *) fifo, data)
X#define k_fifo_put(fifo, data) 	k_queue_append((struct k_queue *) fifo, data)
S
S/**
S * @brief Atomically add a list of elements to a fifo.
S *
S * This routine adds a list of data items to @a fifo in one operation.
S * The data items must be in a singly-linked list, with the first 32 bits
S * each data item pointing to the next data item; the list must be
S * NULL-terminated.
S *
S * @note Can be called by ISRs.
S *
S * @param fifo Address of the fifo.
S * @param head Pointer to first node in singly-linked list.
S * @param tail Pointer to last node in singly-linked list.
S *
S * @return N/A
S */
S#define k_fifo_put_list(fifo, head, tail) \
S	k_queue_append_list((struct k_queue *) fifo, head, tail)
X#define k_fifo_put_list(fifo, head, tail) 	k_queue_append_list((struct k_queue *) fifo, head, tail)
S
S/**
S * @brief Atomically add a list of elements to a fifo.
S *
S * This routine adds a list of data items to @a fifo in one operation.
S * The data items must be in a singly-linked list implemented using a
S * sys_slist_t object. Upon completion, the sys_slist_t object is invalid
S * and must be re-initialized via sys_slist_init().
S *
S * @note Can be called by ISRs.
S *
S * @param fifo Address of the fifo.
S * @param list Pointer to sys_slist_t object.
S *
S * @return N/A
S */
S#define k_fifo_put_slist(fifo, list) \
S	k_queue_merge_slist((struct k_queue *) fifo, list)
X#define k_fifo_put_slist(fifo, list) 	k_queue_merge_slist((struct k_queue *) fifo, list)
S
S/**
S * @brief Get an element from a fifo.
S *
S * This routine removes a data item from @a fifo in a "first in, first out"
S * manner. The first 32 bits of the data item are reserved for the kernel's use.
S *
S * @note Can be called by ISRs, but @a timeout must be set to K_NO_WAIT.
S *
S * @param fifo Address of the fifo.
S * @param timeout Waiting period to obtain a data item (in milliseconds),
S *                or one of the special values K_NO_WAIT and K_FOREVER.
S *
S * @return Address of the data item if successful; NULL if returned
S * without waiting, or waiting period timed out.
S */
S#define k_fifo_get(fifo, timeout) \
S	k_queue_get((struct k_queue *) fifo, timeout)
X#define k_fifo_get(fifo, timeout) 	k_queue_get((struct k_queue *) fifo, timeout)
S
S/**
S * @brief Query a fifo to see if it has data available.
S *
S * Note that the data might be already gone by the time this function returns
S * if other threads is also trying to read from the fifo.
S *
S * @note Can be called by ISRs.
S *
S * @param fifo Address of the fifo.
S *
S * @return Non-zero if the fifo is empty.
S * @return 0 if data is available.
S */
S#define k_fifo_is_empty(fifo) \
S	k_queue_is_empty((struct k_queue *) fifo)
X#define k_fifo_is_empty(fifo) 	k_queue_is_empty((struct k_queue *) fifo)
S
S/**
S * @brief Peek element at the head of fifo.
S *
S * Return element from the head of fifo without removing it. A usecase
S * for this is if elements of the fifo are themselves containers. Then
S * on each iteration of processing, a head container will be peeked,
S * and some data processed out of it, and only if the container is empty,
S * it will be completely remove from the fifo.
S *
S * @param fifo Address of the fifo.
S *
S * @return Head element, or NULL if the fifo is empty.
S */
S#define k_fifo_peek_head(fifo) \
S	k_queue_peek_head((struct k_queue *) fifo)
X#define k_fifo_peek_head(fifo) 	k_queue_peek_head((struct k_queue *) fifo)
S
S/**
S * @brief Peek element at the tail of fifo.
S *
S * Return element from the tail of fifo (without removing it). A usecase
S * for this is if elements of the fifo are themselves containers. Then
S * it may be useful to add more data to the last container in fifo.
S *
S * @param fifo Address of the fifo.
S *
S * @return Tail element, or NULL if fifo is empty.
S */
S#define k_fifo_peek_tail(fifo) \
S	k_queue_peek_tail((struct k_queue *) fifo)
X#define k_fifo_peek_tail(fifo) 	k_queue_peek_tail((struct k_queue *) fifo)
S
S/**
S * @brief Statically define and initialize a fifo.
S *
S * The fifo can be accessed outside the module where it is defined using:
S *
S * @code extern struct k_fifo <name>; @endcode
S *
S * @param name Name of the fifo.
S */
S#define K_FIFO_DEFINE(name) \
S	struct k_fifo name \
S		__in_section(_k_queue, static, name) = \
S		_K_FIFO_INITIALIZER(name)
X#define K_FIFO_DEFINE(name) 	struct k_fifo name 		__in_section(_k_queue, static, name) = 		_K_FIFO_INITIALIZER(name)
S
S/**
S * @} end defgroup fifo_apis
S */
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
Sstruct k_lifo {
S	struct k_queue _queue;
S};
S
S#define _K_LIFO_INITIALIZER(obj) \
S	{ \
S	._queue = _K_QUEUE_INITIALIZER(obj._queue) \
S	}
X#define _K_LIFO_INITIALIZER(obj) 	{ 	._queue = _K_QUEUE_INITIALIZER(obj._queue) 	}
S
S#define K_LIFO_INITIALIZER DEPRECATED_MACRO _K_LIFO_INITIALIZER
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S/**
S * @defgroup lifo_apis Lifo APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @brief Initialize a lifo.
S *
S * This routine initializes a lifo object, prior to its first use.
S *
S * @param lifo Address of the lifo.
S *
S * @return N/A
S */
S#define k_lifo_init(lifo) \
S	k_queue_init((struct k_queue *) lifo)
X#define k_lifo_init(lifo) 	k_queue_init((struct k_queue *) lifo)
S
S/**
S * @brief Add an element to a lifo.
S *
S * This routine adds a data item to @a lifo. A lifo data item must be
S * aligned on a 4-byte boundary, and the first 32 bits of the item are
S * reserved for the kernel's use.
S *
S * @note Can be called by ISRs.
S *
S * @param lifo Address of the lifo.
S * @param data Address of the data item.
S *
S * @return N/A
S */
S#define k_lifo_put(lifo, data) \
S	k_queue_prepend((struct k_queue *) lifo, data)
X#define k_lifo_put(lifo, data) 	k_queue_prepend((struct k_queue *) lifo, data)
S
S/**
S * @brief Get an element from a lifo.
S *
S * This routine removes a data item from @a lifo in a "last in, first out"
S * manner. The first 32 bits of the data item are reserved for the kernel's use.
S *
S * @note Can be called by ISRs, but @a timeout must be set to K_NO_WAIT.
S *
S * @param lifo Address of the lifo.
S * @param timeout Waiting period to obtain a data item (in milliseconds),
S *                or one of the special values K_NO_WAIT and K_FOREVER.
S *
S * @return Address of the data item if successful; NULL if returned
S * without waiting, or waiting period timed out.
S */
S#define k_lifo_get(lifo, timeout) \
S	k_queue_get((struct k_queue *) lifo, timeout)
X#define k_lifo_get(lifo, timeout) 	k_queue_get((struct k_queue *) lifo, timeout)
S
S/**
S * @brief Statically define and initialize a lifo.
S *
S * The lifo can be accessed outside the module where it is defined using:
S *
S * @code extern struct k_lifo <name>; @endcode
S *
S * @param name Name of the fifo.
S */
S#define K_LIFO_DEFINE(name) \
S	struct k_lifo name \
S		__in_section(_k_queue, static, name) = \
S		_K_LIFO_INITIALIZER(name)
X#define K_LIFO_DEFINE(name) 	struct k_lifo name 		__in_section(_k_queue, static, name) = 		_K_LIFO_INITIALIZER(name)
S
S/**
S * @} end defgroup lifo_apis
S */
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
Sstruct k_stack {
S	_wait_q_t wait_q;
S	u32_t *base, *next, *top;
S
S//	_OBJECT_TRACING_NEXT_PTR(k_stack);
S};
S
S#define _K_STACK_INITIALIZER(obj, stack_buffer, stack_num_entries) \
S	{ \
S	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), \
S	.base = stack_buffer, \
S	.next = stack_buffer, \
S	.top = stack_buffer + stack_num_entries, \
S	_OBJECT_TRACING_INIT \
S	}
X#define _K_STACK_INITIALIZER(obj, stack_buffer, stack_num_entries) 	{ 	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), 	.base = stack_buffer, 	.next = stack_buffer, 	.top = stack_buffer + stack_num_entries, 	_OBJECT_TRACING_INIT 	}
S
S#define K_STACK_INITIALIZER DEPRECATED_MACRO _K_STACK_INITIALIZER
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S/**
S * @defgroup stack_apis Stack APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @brief Initialize a stack.
S *
S * This routine initializes a stack object, prior to its first use.
S *
S * @param stack Address of the stack.
S * @param buffer Address of array used to hold stacked values.
S * @param num_entries Maximum number of values that can be stacked.
S *
S * @return N/A
S */
Sextern void k_stack_init(struct k_stack *stack,
S			 u32_t *buffer, int num_entries);
S
S/**
S * @brief Push an element onto a stack.
S *
S * This routine adds a 32-bit value @a data to @a stack.
S *
S * @note Can be called by ISRs.
S *
S * @param stack Address of the stack.
S * @param data Value to push onto the stack.
S *
S * @return N/A
S */
Sextern void k_stack_push(struct k_stack *stack, u32_t data);
S
S/**
S * @brief Pop an element from a stack.
S *
S * This routine removes a 32-bit value from @a stack in a "last in, first out"
S * manner and stores the value in @a data.
S *
S * @note Can be called by ISRs, but @a timeout must be set to K_NO_WAIT.
S *
S * @param stack Address of the stack.
S * @param data Address of area to hold the value popped from the stack.
S * @param timeout Waiting period to obtain a value (in milliseconds),
S *                or one of the special values K_NO_WAIT and K_FOREVER.
S *
S * @retval 0 Element popped from stack.
S * @retval -EBUSY Returned without waiting.
S * @retval -EAGAIN Waiting period timed out.
S */
Sextern int k_stack_pop(struct k_stack *stack, u32_t *data, s32_t timeout);
S
S/**
S * @brief Statically define and initialize a stack
S *
S * The stack can be accessed outside the module where it is defined using:
S *
S * @code extern struct k_stack <name>; @endcode
S *
S * @param name Name of the stack.
S * @param stack_num_entries Maximum number of values that can be stacked.
S */
S#define K_STACK_DEFINE(name, stack_num_entries)                \
S	u32_t __noinit                                      \
S		_k_stack_buf_##name[stack_num_entries];        \
S	struct k_stack name                                    \
S		__in_section(_k_stack, static, name) =    \
S		_K_STACK_INITIALIZER(name, _k_stack_buf_##name, \
S				    stack_num_entries)
X#define K_STACK_DEFINE(name, stack_num_entries)                	u32_t __noinit                                      		_k_stack_buf_##name[stack_num_entries];        	struct k_stack name                                    		__in_section(_k_stack, static, name) =    		_K_STACK_INITIALIZER(name, _k_stack_buf_##name, 				    stack_num_entries)
S
S/**
S * @} end defgroup stack_apis
S */
S
Sstruct k_work;
S
S/**
S * @defgroup workqueue_apis Workqueue Thread APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @typedef k_work_handler_t
S * @brief Work item handler function type.
S *
S * A work item's handler function is executed by a workqueue's thread
S * when the work item is processed by the workqueue.
S *
S * @param work Address of the work item.
S *
S * @return N/A
S */
Stypedef void (*k_work_handler_t)(struct k_work *work);
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
Sstruct k_work_q {
S	struct k_queue queue;
S	struct k_thread thread;
S};
S
Senum {
S	K_WORK_STATE_PENDING,	/* Work item pending state */
S};
S
Sstruct k_work {
S	void *_reserved;		/* Used by k_queue implementation. */
S	k_work_handler_t handler;
S	atomic_t flags[1];
S};
S
Sstruct k_delayed_work {
S	struct k_work work;
S	struct _timeout timeout;
S	struct k_work_q *work_q;
S};
S
Sextern struct k_work_q k_sys_work_q;
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S#define _K_WORK_INITIALIZER(work_handler) \
S	{ \
S	._reserved = NULL, \
S	.handler = work_handler, \
S	.flags = { 0 } \
S	}
X#define _K_WORK_INITIALIZER(work_handler) 	{ 	._reserved = NULL, 	.handler = work_handler, 	.flags = { 0 } 	}
S
S#define K_WORK_INITIALIZER DEPRECATED_MACRO _K_WORK_INITIALIZER
S
S/**
S * @brief Initialize a statically-defined work item.
S *
S * This macro can be used to initialize a statically-defined workqueue work
S * item, prior to its first use. For example,
S *
S * @code static K_WORK_DEFINE(<work>, <work_handler>); @endcode
S *
S * @param work Symbol name for work item object
S * @param work_handler Function to invoke each time work item is processed.
S */
S#define K_WORK_DEFINE(work, work_handler) \
S	struct k_work work \
S		__in_section(_k_work, static, work) = \
S		_K_WORK_INITIALIZER(work_handler)
X#define K_WORK_DEFINE(work, work_handler) 	struct k_work work 		__in_section(_k_work, static, work) = 		_K_WORK_INITIALIZER(work_handler)
S
S/**
S * @brief Initialize a work item.
S *
S * This routine initializes a workqueue work item, prior to its first use.
S *
S * @param work Address of work item.
S * @param handler Function to invoke each time work item is processed.
S *
S * @return N/A
S */
Sstatic inline void k_work_init(struct k_work *work, k_work_handler_t handler)
S{
S	atomic_clear_bit(work->flags, K_WORK_STATE_PENDING);
S	work->handler = handler;
S}
S
S/**
S * @brief Submit a work item.
S *
S * This routine submits work item @a work to be processed by workqueue
S * @a work_q. If the work item is already pending in the workqueue's queue
S * as a result of an earlier submission, this routine has no effect on the
S * work item. If the work item has already been processed, or is currently
S * being processed, its work is considered complete and the work item can be
S * resubmitted.
S *
S * @warning
S * A submitted work item must not be modified until it has been processed
S * by the workqueue.
S *
S * @note Can be called by ISRs.
S *
S * @param work_q Address of workqueue.
S * @param work Address of work item.
S *
S * @return N/A
S */
Sstatic inline void k_work_submit_to_queue(struct k_work_q *work_q,
S					  struct k_work *work)
S{
S	if (!atomic_test_and_set_bit(work->flags, K_WORK_STATE_PENDING)) {
S		k_queue_append(&work_q->queue, work);
S	}
S}
S
S/**
S * @brief Check if a work item is pending.
S *
S * This routine indicates if work item @a work is pending in a workqueue's
S * queue.
S *
S * @note Can be called by ISRs.
S *
S * @param work Address of work item.
S *
S * @return 1 if work item is pending, or 0 if it is not pending.
S */
Sstatic inline int k_work_pending(struct k_work *work)
S{
S	return atomic_test_bit(work->flags, K_WORK_STATE_PENDING);
S}
S
S/**
S * @brief Start a workqueue.
S *
S * This routine starts workqueue @a work_q. The workqueue spawns its work
S * processing thread, which runs forever.
S *
S * @param work_q Address of workqueue.
S * @param stack Pointer to work queue thread's stack space, as defined by
S *		K_THREAD_STACK_DEFINE()
S * @param stack_size Size of the work queue thread's stack (in bytes), which
S *		should either be the same constant passed to
S *		K_THREAD_STACK_DEFINE() or the value of K_THREAD_STACK_SIZEOF().
S * @param prio Priority of the work queue's thread.
S *
S * @return N/A
S */
Sextern void k_work_q_start(struct k_work_q *work_q,
S			   k_thread_stack_t stack,
S			   size_t stack_size, int prio);
S
S/**
S * @brief Initialize a delayed work item.
S *
S * This routine initializes a workqueue delayed work item, prior to
S * its first use.
S *
S * @param work Address of delayed work item.
S * @param handler Function to invoke each time work item is processed.
S *
S * @return N/A
S */
Sextern void k_delayed_work_init(struct k_delayed_work *work,
S				k_work_handler_t handler);
S
S/**
S * @brief Submit a delayed work item.
S *
S * This routine schedules work item @a work to be processed by workqueue
S * @a work_q after a delay of @a delay milliseconds. The routine initiates
S * an asynchronous countdown for the work item and then returns to the caller.
S * Only when the countdown completes is the work item actually submitted to
S * the workqueue and becomes pending.
S *
S * Submitting a previously submitted delayed work item that is still
S * counting down cancels the existing submission and restarts the countdown
S * using the new delay. If the work item is currently pending on the
S * workqueue's queue because the countdown has completed it is too late to
S * resubmit the item, and resubmission fails without impacting the work item.
S * If the work item has already been processed, or is currently being processed,
S * its work is considered complete and the work item can be resubmitted.
S *
S * @warning
S * A delayed work item must not be modified until it has been processed
S * by the workqueue.
S *
S * @note Can be called by ISRs.
S *
S * @param work_q Address of workqueue.
S * @param work Address of delayed work item.
S * @param delay Delay before submitting the work item (in milliseconds).
S *
S * @retval 0 Work item countdown started.
S * @retval -EINPROGRESS Work item is already pending.
S * @retval -EINVAL Work item is being processed or has completed its work.
S * @retval -EADDRINUSE Work item is pending on a different workqueue.
S */
Sextern int k_delayed_work_submit_to_queue(struct k_work_q *work_q,
S					  struct k_delayed_work *work,
S					  s32_t delay);
S
S/**
S * @brief Cancel a delayed work item.
S *
S * This routine cancels the submission of delayed work item @a work.
S * A delayed work item can only be canceled while its countdown is still
S * underway.
S *
S * @note Can be called by ISRs.
S *
S * @param work Address of delayed work item.
S *
S * @retval 0 Work item countdown canceled.
S * @retval -EINPROGRESS Work item is already pending.
S * @retval -EINVAL Work item is being processed or has completed its work.
S */
Sextern int k_delayed_work_cancel(struct k_delayed_work *work);
S
S/**
S * @brief Submit a work item to the system workqueue.
S *
S * This routine submits work item @a work to be processed by the system
S * workqueue. If the work item is already pending in the workqueue's queue
S * as a result of an earlier submission, this routine has no effect on the
S * work item. If the work item has already been processed, or is currently
S * being processed, its work is considered complete and the work item can be
S * resubmitted.
S *
S * @warning
S * Work items submitted to the system workqueue should avoid using handlers
S * that block or yield since this may prevent the system workqueue from
S * processing other work items in a timely manner.
S *
S * @note Can be called by ISRs.
S *
S * @param work Address of work item.
S *
S * @return N/A
S */
Sstatic inline void k_work_submit(struct k_work *work)
S{
S	k_work_submit_to_queue(&k_sys_work_q, work);
S}
S
S/**
S * @brief Submit a delayed work item to the system workqueue.
S *
S * This routine schedules work item @a work to be processed by the system
S * workqueue after a delay of @a delay milliseconds. The routine initiates
S * an asynchronous countdown for the work item and then returns to the caller.
S * Only when the countdown completes is the work item actually submitted to
S * the workqueue and becomes pending.
S *
S * Submitting a previously submitted delayed work item that is still
S * counting down cancels the existing submission and restarts the countdown
S * using the new delay. If the work item is currently pending on the
S * workqueue's queue because the countdown has completed it is too late to
S * resubmit the item, and resubmission fails without impacting the work item.
S * If the work item has already been processed, or is currently being processed,
S * its work is considered complete and the work item can be resubmitted.
S *
S * @warning
S * Work items submitted to the system workqueue should avoid using handlers
S * that block or yield since this may prevent the system workqueue from
S * processing other work items in a timely manner.
S *
S * @note Can be called by ISRs.
S *
S * @param work Address of delayed work item.
S * @param delay Delay before submitting the work item (in milliseconds).
S *
S * @retval 0 Work item countdown started.
S * @retval -EINPROGRESS Work item is already pending.
S * @retval -EINVAL Work item is being processed or has completed its work.
S * @retval -EADDRINUSE Work item is pending on a different workqueue.
S */
Sstatic inline int k_delayed_work_submit(struct k_delayed_work *work,
S					s32_t delay)
S{
S	return k_delayed_work_submit_to_queue(&k_sys_work_q, work, delay);
S}
S
S/**
S * @brief Get time remaining before a delayed work gets scheduled.
S *
S * This routine computes the (approximate) time remaining before a
S * delayed work gets executed. If the delayed work is not waiting to be
S * schedules, it returns zero.
S *
S * @param work     Delayed work item.
S *
S * @return Remaining time (in milliseconds).
S */
Sstatic inline s32_t k_delayed_work_remaining_get(struct k_delayed_work *work)
S{
S	return _timeout_remaining_get(&work->timeout);
S}
S
S/**
S * @} end defgroup workqueue_apis
S */
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
Sstruct k_mutex {
S	_wait_q_t wait_q;
S	struct k_thread *owner;
S	u32_t lock_count;
S	int owner_orig_prio;
S
S//	_OBJECT_TRACING_NEXT_PTR(k_mutex);
S};
S
S#define _K_MUTEX_INITIALIZER(obj) \
S	{ \
S	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), \
S	.owner = NULL, \
S	.lock_count = 0, \
S	.owner_orig_prio = K_LOWEST_THREAD_PRIO, \
S	_OBJECT_TRACING_INIT \
S	}
X#define _K_MUTEX_INITIALIZER(obj) 	{ 	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), 	.owner = NULL, 	.lock_count = 0, 	.owner_orig_prio = K_LOWEST_THREAD_PRIO, 	_OBJECT_TRACING_INIT 	}
S
S#define K_MUTEX_INITIALIZER DEPRECATED_MACRO _K_MUTEX_INITIALIZER
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S/**
S * @defgroup mutex_apis Mutex APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @brief Statically define and initialize a mutex.
S *
S * The mutex can be accessed outside the module where it is defined using:
S *
S * @code extern struct k_mutex <name>; @endcode
S *
S * @param name Name of the mutex.
S */
S#define K_MUTEX_DEFINE(name) \
S	struct k_mutex name \
S		__in_section(_k_mutex, static, name) = \
S		_K_MUTEX_INITIALIZER(name)
X#define K_MUTEX_DEFINE(name) 	struct k_mutex name 		__in_section(_k_mutex, static, name) = 		_K_MUTEX_INITIALIZER(name)
S
S/**
S * @brief Initialize a mutex.
S *
S * This routine initializes a mutex object, prior to its first use.
S *
S * Upon completion, the mutex is available and does not have an owner.
S *
S * @param mutex Address of the mutex.
S *
S * @return N/A
S */
Sextern void k_mutex_init(struct k_mutex *mutex);
S
S/**
S * @brief Lock a mutex.
S *
S * This routine locks @a mutex. If the mutex is locked by another thread,
S * the calling thread waits until the mutex becomes available or until
S * a timeout occurs.
S *
S * A thread is permitted to lock a mutex it has already locked. The operation
S * completes immediately and the lock count is increased by 1.
S *
S * @param mutex Address of the mutex.
S * @param timeout Waiting period to lock the mutex (in milliseconds),
S *                or one of the special values K_NO_WAIT and K_FOREVER.
S *
S * @retval 0 Mutex locked.
S * @retval -EBUSY Returned without waiting.
S * @retval -EAGAIN Waiting period timed out.
S */
Sextern int k_mutex_lock(struct k_mutex *mutex, s32_t timeout);
S
S/**
S * @brief Unlock a mutex.
S *
S * This routine unlocks @a mutex. The mutex must already be locked by the
S * calling thread.
S *
S * The mutex cannot be claimed by another thread until it has been unlocked by
S * the calling thread as many times as it was previously locked by that
S * thread.
S *
S * @param mutex Address of the mutex.
S *
S * @return N/A
S */
Sextern void k_mutex_unlock(struct k_mutex *mutex);
S
S/**
S * @} end defgroup mutex_apis
S */
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
Sstruct k_sem {
S	_wait_q_t wait_q;
S	unsigned int count;
S	unsigned int limit;
S	_POLL_EVENT;
S
S//	_OBJECT_TRACING_NEXT_PTR(k_sem);
S};
S
S#define _K_SEM_INITIALIZER(obj, initial_count, count_limit) \
S	{ \
S	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), \
S	.count = initial_count, \
S	.limit = count_limit, \
S	_POLL_EVENT_OBJ_INIT(obj) \
S	_OBJECT_TRACING_INIT \
S	}
X#define _K_SEM_INITIALIZER(obj, initial_count, count_limit) 	{ 	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), 	.count = initial_count, 	.limit = count_limit, 	_POLL_EVENT_OBJ_INIT(obj) 	_OBJECT_TRACING_INIT 	}
S
S#define K_SEM_INITIALIZER DEPRECATED_MACRO _K_SEM_INITIALIZER
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S/**
S * @defgroup semaphore_apis Semaphore APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @brief Initialize a semaphore.
S *
S * This routine initializes a semaphore object, prior to its first use.
S *
S * @param sem Address of the semaphore.
S * @param initial_count Initial semaphore count.
S * @param limit Maximum permitted semaphore count.
S *
S * @return N/A
S */
Sextern void k_sem_init(struct k_sem *sem, unsigned int initial_count,
S			unsigned int limit);
S
S/**
S * @brief Take a semaphore.
S *
S * This routine takes @a sem.
S *
S * @note Can be called by ISRs, but @a timeout must be set to K_NO_WAIT.
S *
S * @param sem Address of the semaphore.
S * @param timeout Waiting period to take the semaphore (in milliseconds),
S *                or one of the special values K_NO_WAIT and K_FOREVER.
S *
S * @note When porting code from the nanokernel legacy API to the new API, be
S * careful with the return value of this function. The return value is the
S * reverse of the one of nano_sem_take family of APIs: 0 means success, and
S * non-zero means failure, while the nano_sem_take family returns 1 for success
S * and 0 for failure.
S *
S * @retval 0 Semaphore taken.
S * @retval -EBUSY Returned without waiting.
S * @retval -EAGAIN Waiting period timed out.
S */
Sextern int k_sem_take(struct k_sem *sem, s32_t timeout);
S
S/**
S * @brief Give a semaphore.
S *
S * This routine gives @a sem, unless the semaphore is already at its maximum
S * permitted count.
S *
S * @note Can be called by ISRs.
S *
S * @param sem Address of the semaphore.
S *
S * @return N/A
S */
Sextern void k_sem_give(struct k_sem *sem);
S
S/**
S * @brief Reset a semaphore's count to zero.
S *
S * This routine sets the count of @a sem to zero.
S *
S * @param sem Address of the semaphore.
S *
S * @return N/A
S */
Sstatic inline void k_sem_reset(struct k_sem *sem)
S{
S	sem->count = 0;
S}
S
S/**
S * @brief Get a semaphore's count.
S *
S * This routine returns the current count of @a sem.
S *
S * @param sem Address of the semaphore.
S *
S * @return Current semaphore count.
S */
Sstatic inline unsigned int k_sem_count_get(struct k_sem *sem)
S{
S	return sem->count;
S}
S
S/**
S * @brief Statically define and initialize a semaphore.
S *
S * The semaphore can be accessed outside the module where it is defined using:
S *
S * @code extern struct k_sem <name>; @endcode
S *
S * @param name Name of the semaphore.
S * @param initial_count Initial semaphore count.
S * @param count_limit Maximum permitted semaphore count.
S */
S#define K_SEM_DEFINE(name, initial_count, count_limit) \
S	struct k_sem name \
S		__in_section(_k_sem, static, name) = \
S		_K_SEM_INITIALIZER(name, initial_count, count_limit)
X#define K_SEM_DEFINE(name, initial_count, count_limit) 	struct k_sem name 		__in_section(_k_sem, static, name) = 		_K_SEM_INITIALIZER(name, initial_count, count_limit)
S
S/**
S * @} end defgroup semaphore_apis
S */
S
S/**
S * @defgroup alert_apis Alert APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @typedef k_alert_handler_t
S * @brief Alert handler function type.
S *
S * An alert's alert handler function is invoked by the system workqueue
S * when the alert is signaled. The alert handler function is optional,
S * and is only invoked if the alert has been initialized with one.
S *
S * @param alert Address of the alert.
S *
S * @return 0 if alert has been consumed; non-zero if alert should pend.
S */
Stypedef int (*k_alert_handler_t)(struct k_alert *alert);
S
S/**
S * @} end defgroup alert_apis
S */
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
S#define K_ALERT_DEFAULT NULL
S#define K_ALERT_IGNORE ((void *)(-1))
S
Sstruct k_alert {
S	k_alert_handler_t handler;
S	atomic_t send_count;
S	struct k_work work_item;
S	struct k_sem sem;
S
S//	_OBJECT_TRACING_NEXT_PTR(k_alert);
S};
S
Sextern void _alert_deliver(struct k_work *work);
S
S#define _K_ALERT_INITIALIZER(obj, alert_handler, max_num_pending_alerts) \
S	{ \
S	.handler = (k_alert_handler_t)alert_handler, \
S	.send_count = ATOMIC_INIT(0), \
S	.work_item = _K_WORK_INITIALIZER(_alert_deliver), \
S	.sem = _K_SEM_INITIALIZER(obj.sem, 0, max_num_pending_alerts), \
S	_OBJECT_TRACING_INIT \
S	}
X#define _K_ALERT_INITIALIZER(obj, alert_handler, max_num_pending_alerts) 	{ 	.handler = (k_alert_handler_t)alert_handler, 	.send_count = ATOMIC_INIT(0), 	.work_item = _K_WORK_INITIALIZER(_alert_deliver), 	.sem = _K_SEM_INITIALIZER(obj.sem, 0, max_num_pending_alerts), 	_OBJECT_TRACING_INIT 	}
S
S#define K_ALERT_INITIALIZER DEPRECATED_MACRO _K_ALERT_INITIALIZER
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S/**
S * @addtogroup alert_apis
S * @{
S */
S
S/**
S * @brief Statically define and initialize an alert.
S *
S * The alert can be accessed outside the module where it is defined using:
S *
S * @code extern struct k_alert <name>; @endcode
S *
S * @param name Name of the alert.
S * @param alert_handler Action to take when alert is sent. Specify either
S *        the address of a function to be invoked by the system workqueue
S *        thread, K_ALERT_IGNORE (which causes the alert to be ignored), or
S *        K_ALERT_DEFAULT (which causes the alert to pend).
S * @param max_num_pending_alerts Maximum number of pending alerts.
S */
S#define K_ALERT_DEFINE(name, alert_handler, max_num_pending_alerts) \
S	struct k_alert name \
S		__in_section(_k_alert, static, name) = \
S		_K_ALERT_INITIALIZER(name, alert_handler, \
S				    max_num_pending_alerts)
X#define K_ALERT_DEFINE(name, alert_handler, max_num_pending_alerts) 	struct k_alert name 		__in_section(_k_alert, static, name) = 		_K_ALERT_INITIALIZER(name, alert_handler, 				    max_num_pending_alerts)
S
S/**
S * @brief Initialize an alert.
S *
S * This routine initializes an alert object, prior to its first use.
S *
S * @param alert Address of the alert.
S * @param handler Action to take when alert is sent. Specify either the address
S *                of a function to be invoked by the system workqueue thread,
S *                K_ALERT_IGNORE (which causes the alert to be ignored), or
S *                K_ALERT_DEFAULT (which causes the alert to pend).
S * @param max_num_pending_alerts Maximum number of pending alerts.
S *
S * @return N/A
S */
Sextern void k_alert_init(struct k_alert *alert, k_alert_handler_t handler,
S			 unsigned int max_num_pending_alerts);
S
S/**
S * @brief Receive an alert.
S *
S * This routine receives a pending alert for @a alert.
S *
S * @note Can be called by ISRs, but @a timeout must be set to K_NO_WAIT.
S *
S * @param alert Address of the alert.
S * @param timeout Waiting period to receive the alert (in milliseconds),
S *                or one of the special values K_NO_WAIT and K_FOREVER.
S *
S * @retval 0 Alert received.
S * @retval -EBUSY Returned without waiting.
S * @retval -EAGAIN Waiting period timed out.
S */
Sextern int k_alert_recv(struct k_alert *alert, s32_t timeout);
S
S/**
S * @brief Signal an alert.
S *
S * This routine signals @a alert. The action specified for @a alert will
S * be taken, which may trigger the execution of an alert handler function
S * and/or cause the alert to pend (assuming the alert has not reached its
S * maximum number of pending alerts).
S *
S * @note Can be called by ISRs.
S *
S * @param alert Address of the alert.
S *
S * @return N/A
S */
Sextern void k_alert_send(struct k_alert *alert);
S
S/**
S * @} end addtogroup alert_apis
S */
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
Sstruct k_msgq {
S	_wait_q_t wait_q;
S	size_t msg_size;
S	u32_t max_msgs;
S	char *buffer_start;
S	char *buffer_end;
S	char *read_ptr;
S	char *write_ptr;
S	u32_t used_msgs;
S
S//	_OBJECT_TRACING_NEXT_PTR(k_msgq);
S};
S
S#define _K_MSGQ_INITIALIZER(obj, q_buffer, q_msg_size, q_max_msgs) \
S	{ \
S	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), \
S	.max_msgs = q_max_msgs, \
S	.msg_size = q_msg_size, \
S	.buffer_start = q_buffer, \
S	.buffer_end = q_buffer + (q_max_msgs * q_msg_size), \
S	.read_ptr = q_buffer, \
S	.write_ptr = q_buffer, \
S	.used_msgs = 0, \
S	_OBJECT_TRACING_INIT \
S	}
X#define _K_MSGQ_INITIALIZER(obj, q_buffer, q_msg_size, q_max_msgs) 	{ 	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), 	.max_msgs = q_max_msgs, 	.msg_size = q_msg_size, 	.buffer_start = q_buffer, 	.buffer_end = q_buffer + (q_max_msgs * q_msg_size), 	.read_ptr = q_buffer, 	.write_ptr = q_buffer, 	.used_msgs = 0, 	_OBJECT_TRACING_INIT 	}
S
S#define K_MSGQ_INITIALIZER DEPRECATED_MACRO _K_MSGQ_INITIALIZER
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S/**
S * @defgroup msgq_apis Message Queue APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @brief Statically define and initialize a message queue.
S *
S * The message queue's ring buffer contains space for @a q_max_msgs messages,
S * each of which is @a q_msg_size bytes long. The buffer is aligned to a
S * @a q_align -byte boundary, which must be a power of 2. To ensure that each
S * message is similarly aligned to this boundary, @a q_msg_size must also be
S * a multiple of @a q_align.
S *
S * The message queue can be accessed outside the module where it is defined
S * using:
S *
S * @code extern struct k_msgq <name>; @endcode
S *
S * @param q_name Name of the message queue.
S * @param q_msg_size Message size (in bytes).
S * @param q_max_msgs Maximum number of messages that can be queued.
S * @param q_align Alignment of the message queue's ring buffer.
S */
S#define K_MSGQ_DEFINE(q_name, q_msg_size, q_max_msgs, q_align)      \
S	static char __noinit __aligned(q_align)                     \
S		_k_fifo_buf_##q_name[(q_max_msgs) * (q_msg_size)];  \
S	struct k_msgq q_name                                        \
S		__in_section(_k_msgq, static, q_name) =        \
S	       _K_MSGQ_INITIALIZER(q_name, _k_fifo_buf_##q_name,     \
S				  q_msg_size, q_max_msgs)
X#define K_MSGQ_DEFINE(q_name, q_msg_size, q_max_msgs, q_align)      	static char __noinit __aligned(q_align)                     		_k_fifo_buf_##q_name[(q_max_msgs) * (q_msg_size)];  	struct k_msgq q_name                                        		__in_section(_k_msgq, static, q_name) =        	       _K_MSGQ_INITIALIZER(q_name, _k_fifo_buf_##q_name,     				  q_msg_size, q_max_msgs)
S
S/**
S * @brief Initialize a message queue.
S *
S * This routine initializes a message queue object, prior to its first use.
S *
S * The message queue's ring buffer must contain space for @a max_msgs messages,
S * each of which is @a msg_size bytes long. The buffer must be aligned to an
S * N-byte boundary, where N is a power of 2 (i.e. 1, 2, 4, ...). To ensure
S * that each message is similarly aligned to this boundary, @a q_msg_size
S * must also be a multiple of N.
S *
S * @param q Address of the message queue.
S * @param buffer Pointer to ring buffer that holds queued messages.
S * @param msg_size Message size (in bytes).
S * @param max_msgs Maximum number of messages that can be queued.
S *
S * @return N/A
S */
Sextern void k_msgq_init(struct k_msgq *q, char *buffer,
S			size_t msg_size, u32_t max_msgs);
S
S/**
S * @brief Send a message to a message queue.
S *
S * This routine sends a message to message queue @a q.
S *
S * @note Can be called by ISRs.
S *
S * @param q Address of the message queue.
S * @param data Pointer to the message.
S * @param timeout Waiting period to add the message (in milliseconds),
S *                or one of the special values K_NO_WAIT and K_FOREVER.
S *
S * @retval 0 Message sent.
S * @retval -ENOMSG Returned without waiting or queue purged.
S * @retval -EAGAIN Waiting period timed out.
S */
Sextern int k_msgq_put(struct k_msgq *q, void *data, s32_t timeout);
S
S/**
S * @brief Receive a message from a message queue.
S *
S * This routine receives a message from message queue @a q in a "first in,
S * first out" manner.
S *
S * @note Can be called by ISRs, but @a timeout must be set to K_NO_WAIT.
S *
S * @param q Address of the message queue.
S * @param data Address of area to hold the received message.
S * @param timeout Waiting period to receive the message (in milliseconds),
S *                or one of the special values K_NO_WAIT and K_FOREVER.
S *
S * @retval 0 Message received.
S * @retval -ENOMSG Returned without waiting.
S * @retval -EAGAIN Waiting period timed out.
S */
Sextern int k_msgq_get(struct k_msgq *q, void *data, s32_t timeout);
S
S/**
S * @brief Purge a message queue.
S *
S * This routine discards all unreceived messages in a message queue's ring
S * buffer. Any threads that are blocked waiting to send a message to the
S * message queue are unblocked and see an -ENOMSG error code.
S *
S * @param q Address of the message queue.
S *
S * @return N/A
S */
Sextern void k_msgq_purge(struct k_msgq *q);
S
S/**
S * @brief Get the amount of free space in a message queue.
S *
S * This routine returns the number of unused entries in a message queue's
S * ring buffer.
S *
S * @param q Address of the message queue.
S *
S * @return Number of unused ring buffer entries.
S */
Sstatic inline u32_t k_msgq_num_free_get(struct k_msgq *q)
S{
S	return q->max_msgs - q->used_msgs;
S}
S
S/**
S * @brief Get the number of messages in a message queue.
S *
S * This routine returns the number of messages in a message queue's ring buffer.
S *
S * @param q Address of the message queue.
S *
S * @return Number of messages.
S */
Sstatic inline u32_t k_msgq_num_used_get(struct k_msgq *q)
S{
S	return q->used_msgs;
S}
S
S/**
S * @} end defgroup msgq_apis
S */
S
S/**
S * @defgroup mem_pool_apis Memory Pool APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/* Note on sizing: the use of a 20 bit field for block means that,
S * assuming a reasonable minimum block size of 16 bytes, we're limited
S * to 16M of memory managed by a single pool.  Long term it would be
S * good to move to a variable bit size based on configuration.
S */
Sstruct k_mem_block_id {
S	u32_t pool : 8;
S	u32_t level : 4;
S	u32_t block : 20;
S};
S
Sstruct k_mem_block {
S	void *data;
S	struct k_mem_block_id id;
S};
S
S/**
S * @} end defgroup mem_pool_apis
S */
S
S/**
S * @defgroup mailbox_apis Mailbox APIs
S * @ingroup kernel_apis
S * @{
S */
S
Sstruct k_mbox_msg {
S	/** internal use only - needed for legacy API support */
S	u32_t _mailbox;
S	/** size of message (in bytes) */
S	size_t size;
S	/** application-defined information value */
S	u32_t info;
S	/** sender's message data buffer */
S	void *tx_data;
S	/** internal use only - needed for legacy API support */
S	void *_rx_data;
S	/** message data block descriptor */
S	struct k_mem_block tx_block;
S	/** source thread id */
S	k_tid_t rx_source_thread;
S	/** target thread id */
S	k_tid_t tx_target_thread;
S	/** internal use only - thread waiting on send (may be a dummy) */
S	k_tid_t _syncing_thread;
S#if (CONFIG_NUM_MBOX_ASYNC_MSGS > 0)
S	/** internal use only - semaphore used during asynchronous send */
S	struct k_sem *_async_sem;
S#endif
S};
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
Sstruct k_mbox {
S	_wait_q_t tx_msg_queue;
S	_wait_q_t rx_msg_queue;
S
S//	_OBJECT_TRACING_NEXT_PTR(k_mbox);
S};
S
S#define _K_MBOX_INITIALIZER(obj) \
S	{ \
S	.tx_msg_queue = SYS_DLIST_STATIC_INIT(&obj.tx_msg_queue), \
S	.rx_msg_queue = SYS_DLIST_STATIC_INIT(&obj.rx_msg_queue), \
S	_OBJECT_TRACING_INIT \
S	}
X#define _K_MBOX_INITIALIZER(obj) 	{ 	.tx_msg_queue = SYS_DLIST_STATIC_INIT(&obj.tx_msg_queue), 	.rx_msg_queue = SYS_DLIST_STATIC_INIT(&obj.rx_msg_queue), 	_OBJECT_TRACING_INIT 	}
S
S#define K_MBOX_INITIALIZER DEPRECATED_MACRO _K_MBOX_INITIALIZER
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S/**
S * @brief Statically define and initialize a mailbox.
S *
S * The mailbox is to be accessed outside the module where it is defined using:
S *
S * @code extern struct k_mbox <name>; @endcode
S *
S * @param name Name of the mailbox.
S */
S#define K_MBOX_DEFINE(name) \
S	struct k_mbox name \
S		__in_section(_k_mbox, static, name) = \
S		_K_MBOX_INITIALIZER(name) \
S
X#define K_MBOX_DEFINE(name) 	struct k_mbox name 		__in_section(_k_mbox, static, name) = 		_K_MBOX_INITIALIZER(name) 
S/**
S * @brief Initialize a mailbox.
S *
S * This routine initializes a mailbox object, prior to its first use.
S *
S * @param mbox Address of the mailbox.
S *
S * @return N/A
S */
Sextern void k_mbox_init(struct k_mbox *mbox);
S
S/**
S * @brief Send a mailbox message in a synchronous manner.
S *
S * This routine sends a message to @a mbox and waits for a receiver to both
S * receive and process it. The message data may be in a buffer, in a memory
S * pool block, or non-existent (i.e. an empty message).
S *
S * @param mbox Address of the mailbox.
S * @param tx_msg Address of the transmit message descriptor.
S * @param timeout Waiting period for the message to be received (in
S *                milliseconds), or one of the special values K_NO_WAIT
S *                and K_FOREVER. Once the message has been received,
S *                this routine waits as long as necessary for the message
S *                to be completely processed.
S *
S * @retval 0 Message sent.
S * @retval -ENOMSG Returned without waiting.
S * @retval -EAGAIN Waiting period timed out.
S */
Sextern int k_mbox_put(struct k_mbox *mbox, struct k_mbox_msg *tx_msg,
S		      s32_t timeout);
S
S/**
S * @brief Send a mailbox message in an asynchronous manner.
S *
S * This routine sends a message to @a mbox without waiting for a receiver
S * to process it. The message data may be in a buffer, in a memory pool block,
S * or non-existent (i.e. an empty message). Optionally, the semaphore @a sem
S * will be given when the message has been both received and completely
S * processed by the receiver.
S *
S * @param mbox Address of the mailbox.
S * @param tx_msg Address of the transmit message descriptor.
S * @param sem Address of a semaphore, or NULL if none is needed.
S *
S * @return N/A
S */
Sextern void k_mbox_async_put(struct k_mbox *mbox, struct k_mbox_msg *tx_msg,
S			     struct k_sem *sem);
S
S/**
S * @brief Receive a mailbox message.
S *
S * This routine receives a message from @a mbox, then optionally retrieves
S * its data and disposes of the message.
S *
S * @param mbox Address of the mailbox.
S * @param rx_msg Address of the receive message descriptor.
S * @param buffer Address of the buffer to receive data, or NULL to defer data
S *               retrieval and message disposal until later.
S * @param timeout Waiting period for a message to be received (in
S *                milliseconds), or one of the special values K_NO_WAIT
S *                and K_FOREVER.
S *
S * @retval 0 Message received.
S * @retval -ENOMSG Returned without waiting.
S * @retval -EAGAIN Waiting period timed out.
S */
Sextern int k_mbox_get(struct k_mbox *mbox, struct k_mbox_msg *rx_msg,
S		      void *buffer, s32_t timeout);
S
S/**
S * @brief Retrieve mailbox message data into a buffer.
S *
S * This routine completes the processing of a received message by retrieving
S * its data into a buffer, then disposing of the message.
S *
S * Alternatively, this routine can be used to dispose of a received message
S * without retrieving its data.
S *
S * @param rx_msg Address of the receive message descriptor.
S * @param buffer Address of the buffer to receive data, or NULL to discard
S *               the data.
S *
S * @return N/A
S */
Sextern void k_mbox_data_get(struct k_mbox_msg *rx_msg, void *buffer);
S
S/**
S * @brief Retrieve mailbox message data into a memory pool block.
S *
S * This routine completes the processing of a received message by retrieving
S * its data into a memory pool block, then disposing of the message.
S * The memory pool block that results from successful retrieval must be
S * returned to the pool once the data has been processed, even in cases
S * where zero bytes of data are retrieved.
S *
S * Alternatively, this routine can be used to dispose of a received message
S * without retrieving its data. In this case there is no need to return a
S * memory pool block to the pool.
S *
S * This routine allocates a new memory pool block for the data only if the
S * data is not already in one. If a new block cannot be allocated, the routine
S * returns a failure code and the received message is left unchanged. This
S * permits the caller to reattempt data retrieval at a later time or to dispose
S * of the received message without retrieving its data.
S *
S * @param rx_msg Address of a receive message descriptor.
S * @param pool Address of memory pool, or NULL to discard data.
S * @param block Address of the area to hold memory pool block info.
S * @param timeout Waiting period to wait for a memory pool block (in
S *                milliseconds), or one of the special values K_NO_WAIT
S *                and K_FOREVER.
S *
S * @retval 0 Data retrieved.
S * @retval -ENOMEM Returned without waiting.
S * @retval -EAGAIN Waiting period timed out.
S */
Sextern int k_mbox_data_block_get(struct k_mbox_msg *rx_msg,
S				 struct k_mem_pool *pool,
S				 struct k_mem_block *block, s32_t timeout);
S
S/**
S * @} end defgroup mailbox_apis
S */
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
Sstruct k_pipe {
S	unsigned char *buffer;          /* Pipe buffer: may be NULL */
S	size_t         size;            /* Buffer size */
S	size_t         bytes_used;      /* # bytes used in buffer */
S	size_t         read_index;      /* Where in buffer to read from */
S	size_t         write_index;     /* Where in buffer to write */
S
S	struct {
S		_wait_q_t      readers; /* Reader wait queue */
S		_wait_q_t      writers; /* Writer wait queue */
S	} wait_q;
S
S//	_OBJECT_TRACING_NEXT_PTR(k_pipe);
S};
S
S#define _K_PIPE_INITIALIZER(obj, pipe_buffer, pipe_buffer_size)        \
S	{                                                             \
S	.buffer = pipe_buffer,                                        \
S	.size = pipe_buffer_size,                                     \
S	.bytes_used = 0,                                              \
S	.read_index = 0,                                              \
S	.write_index = 0,                                             \
S	.wait_q.writers = SYS_DLIST_STATIC_INIT(&obj.wait_q.writers), \
S	.wait_q.readers = SYS_DLIST_STATIC_INIT(&obj.wait_q.readers), \
S	_OBJECT_TRACING_INIT                            \
S	}
X#define _K_PIPE_INITIALIZER(obj, pipe_buffer, pipe_buffer_size)        	{                                                             	.buffer = pipe_buffer,                                        	.size = pipe_buffer_size,                                     	.bytes_used = 0,                                              	.read_index = 0,                                              	.write_index = 0,                                             	.wait_q.writers = SYS_DLIST_STATIC_INIT(&obj.wait_q.writers), 	.wait_q.readers = SYS_DLIST_STATIC_INIT(&obj.wait_q.readers), 	_OBJECT_TRACING_INIT                            	}
S
S#define K_PIPE_INITIALIZER DEPRECATED_MACRO _K_PIPE_INITIALIZER
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S/**
S * @defgroup pipe_apis Pipe APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @brief Statically define and initialize a pipe.
S *
S * The pipe can be accessed outside the module where it is defined using:
S *
S * @code extern struct k_pipe <name>; @endcode
S *
S * @param name Name of the pipe.
S * @param pipe_buffer_size Size of the pipe's ring buffer (in bytes),
S *                         or zero if no ring buffer is used.
S * @param pipe_align Alignment of the pipe's ring buffer (power of 2).
S */
S#define K_PIPE_DEFINE(name, pipe_buffer_size, pipe_align)     \
S	static unsigned char __noinit __aligned(pipe_align)   \
S		_k_pipe_buf_##name[pipe_buffer_size];         \
S	struct k_pipe name                                    \
S		__in_section(_k_pipe, static, name) =    \
S		_K_PIPE_INITIALIZER(name, _k_pipe_buf_##name, pipe_buffer_size)
X#define K_PIPE_DEFINE(name, pipe_buffer_size, pipe_align)     	static unsigned char __noinit __aligned(pipe_align)   		_k_pipe_buf_##name[pipe_buffer_size];         	struct k_pipe name                                    		__in_section(_k_pipe, static, name) =    		_K_PIPE_INITIALIZER(name, _k_pipe_buf_##name, pipe_buffer_size)
S
S/**
S * @brief Initialize a pipe.
S *
S * This routine initializes a pipe object, prior to its first use.
S *
S * @param pipe Address of the pipe.
S * @param buffer Address of the pipe's ring buffer, or NULL if no ring buffer
S *               is used.
S * @param size Size of the pipe's ring buffer (in bytes), or zero if no ring
S *             buffer is used.
S *
S * @return N/A
S */
Sextern void k_pipe_init(struct k_pipe *pipe, unsigned char *buffer,
S			size_t size);
S
S/**
S * @brief Write data to a pipe.
S *
S * This routine writes up to @a bytes_to_write bytes of data to @a pipe.
S *
S * @param pipe Address of the pipe.
S * @param data Address of data to write.
S * @param bytes_to_write Size of data (in bytes).
S * @param bytes_written Address of area to hold the number of bytes written.
S * @param min_xfer Minimum number of bytes to write.
S * @param timeout Waiting period to wait for the data to be written (in
S *                milliseconds), or one of the special values K_NO_WAIT
S *                and K_FOREVER.
S *
S * @retval 0 At least @a min_xfer bytes of data were written.
S * @retval -EIO Returned without waiting; zero data bytes were written.
S * @retval -EAGAIN Waiting period timed out; between zero and @a min_xfer
S *                 minus one data bytes were written.
S */
Sextern int k_pipe_put(struct k_pipe *pipe, void *data,
S		      size_t bytes_to_write, size_t *bytes_written,
S		      size_t min_xfer, s32_t timeout);
S
S/**
S * @brief Read data from a pipe.
S *
S * This routine reads up to @a bytes_to_read bytes of data from @a pipe.
S *
S * @param pipe Address of the pipe.
S * @param data Address to place the data read from pipe.
S * @param bytes_to_read Maximum number of data bytes to read.
S * @param bytes_read Address of area to hold the number of bytes read.
S * @param min_xfer Minimum number of data bytes to read.
S * @param timeout Waiting period to wait for the data to be read (in
S *                milliseconds), or one of the special values K_NO_WAIT
S *                and K_FOREVER.
S *
S * @retval 0 At least @a min_xfer bytes of data were read.
S * @retval -EIO Returned without waiting; zero data bytes were read.
S * @retval -EAGAIN Waiting period timed out; between zero and @a min_xfer
S *                 minus one data bytes were read.
S */
Sextern int k_pipe_get(struct k_pipe *pipe, void *data,
S		      size_t bytes_to_read, size_t *bytes_read,
S		      size_t min_xfer, s32_t timeout);
S
S/**
S * @brief Write memory block to a pipe.
S *
S * This routine writes the data contained in a memory block to @a pipe.
S * Once all of the data in the block has been written to the pipe, it will
S * free the memory block @a block and give the semaphore @a sem (if specified).
S *
S * @param pipe Address of the pipe.
S * @param block Memory block containing data to send
S * @param size Number of data bytes in memory block to send
S * @param sem Semaphore to signal upon completion (else NULL)
S *
S * @return N/A
S */
Sextern void k_pipe_block_put(struct k_pipe *pipe, struct k_mem_block *block,
S			     size_t size, struct k_sem *sem);
S
S/**
S * @} end defgroup pipe_apis
S */
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
Sstruct k_mem_slab {
S	_wait_q_t wait_q;
S	u32_t num_blocks;
S	size_t block_size;
S	char *buffer;
S	char *free_list;
S	u32_t num_used;
S
S//	_OBJECT_TRACING_NEXT_PTR(k_mem_slab);
S};
S
S#define _K_MEM_SLAB_INITIALIZER(obj, slab_buffer, slab_block_size, \
S			       slab_num_blocks) \
S	{ \
S	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), \
S	.num_blocks = slab_num_blocks, \
S	.block_size = slab_block_size, \
S	.buffer = slab_buffer, \
S	.free_list = NULL, \
S	.num_used = 0, \
S	_OBJECT_TRACING_INIT \
S	}
X#define _K_MEM_SLAB_INITIALIZER(obj, slab_buffer, slab_block_size, 			       slab_num_blocks) 	{ 	.wait_q = SYS_DLIST_STATIC_INIT(&obj.wait_q), 	.num_blocks = slab_num_blocks, 	.block_size = slab_block_size, 	.buffer = slab_buffer, 	.free_list = NULL, 	.num_used = 0, 	_OBJECT_TRACING_INIT 	}
S
S#define K_MEM_SLAB_INITIALIZER DEPRECATED_MACRO _K_MEM_SLAB_INITIALIZER
S
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S/**
S * @defgroup mem_slab_apis Memory Slab APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @brief Statically define and initialize a memory slab.
S *
S * The memory slab's buffer contains @a slab_num_blocks memory blocks
S * that are @a slab_block_size bytes long. The buffer is aligned to a
S * @a slab_align -byte boundary. To ensure that each memory block is similarly
S * aligned to this boundary, @a slab_block_size must also be a multiple of
S * @a slab_align.
S *
S * The memory slab can be accessed outside the module where it is defined
S * using:
S *
S * @code extern struct k_mem_slab <name>; @endcode
S *
S * @param name Name of the memory slab.
S * @param slab_block_size Size of each memory block (in bytes).
S * @param slab_num_blocks Number memory blocks.
S * @param slab_align Alignment of the memory slab's buffer (power of 2).
S */
S#define K_MEM_SLAB_DEFINE(name, slab_block_size, slab_num_blocks, slab_align) \
S	char __noinit __aligned(slab_align) \
S		_k_mem_slab_buf_##name[(slab_num_blocks) * (slab_block_size)]; \
S	struct k_mem_slab name \
S		__in_section(_k_mem_slab, static, name) = \
S		_K_MEM_SLAB_INITIALIZER(name, _k_mem_slab_buf_##name, \
S				      slab_block_size, slab_num_blocks)
X#define K_MEM_SLAB_DEFINE(name, slab_block_size, slab_num_blocks, slab_align) 	char __noinit __aligned(slab_align) 		_k_mem_slab_buf_##name[(slab_num_blocks) * (slab_block_size)]; 	struct k_mem_slab name 		__in_section(_k_mem_slab, static, name) = 		_K_MEM_SLAB_INITIALIZER(name, _k_mem_slab_buf_##name, 				      slab_block_size, slab_num_blocks)
S
S/**
S * @brief Initialize a memory slab.
S *
S * Initializes a memory slab, prior to its first use.
S *
S * The memory slab's buffer contains @a slab_num_blocks memory blocks
S * that are @a slab_block_size bytes long. The buffer must be aligned to an
S * N-byte boundary, where N is a power of 2 larger than 2 (i.e. 4, 8, 16, ...).
S * To ensure that each memory block is similarly aligned to this boundary,
S * @a slab_block_size must also be a multiple of N.
S *
S * @param slab Address of the memory slab.
S * @param buffer Pointer to buffer used for the memory blocks.
S * @param block_size Size of each memory block (in bytes).
S * @param num_blocks Number of memory blocks.
S *
S * @return N/A
S */
Sextern void k_mem_slab_init(struct k_mem_slab *slab, void *buffer,
S			   size_t block_size, u32_t num_blocks);
S
S/**
S * @brief Allocate memory from a memory slab.
S *
S * This routine allocates a memory block from a memory slab.
S *
S * @param slab Address of the memory slab.
S * @param mem Pointer to block address area.
S * @param timeout Maximum time to wait for operation to complete
S *        (in milliseconds). Use K_NO_WAIT to return without waiting,
S *        or K_FOREVER to wait as long as necessary.
S *
S * @retval 0 Memory allocated. The block address area pointed at by @a mem
S *         is set to the starting address of the memory block.
S * @retval -ENOMEM Returned without waiting.
S * @retval -EAGAIN Waiting period timed out.
S */
Sextern int k_mem_slab_alloc(struct k_mem_slab *slab, void **mem,
S			    s32_t timeout);
S
S/**
S * @brief Free memory allocated from a memory slab.
S *
S * This routine releases a previously allocated memory block back to its
S * associated memory slab.
S *
S * @param slab Address of the memory slab.
S * @param mem Pointer to block address area (as set by k_mem_slab_alloc()).
S *
S * @return N/A
S */
Sextern void k_mem_slab_free(struct k_mem_slab *slab, void **mem);
S
S/**
S * @brief Get the number of used blocks in a memory slab.
S *
S * This routine gets the number of memory blocks that are currently
S * allocated in @a slab.
S *
S * @param slab Address of the memory slab.
S *
S * @return Number of allocated memory blocks.
S */
Sstatic inline u32_t k_mem_slab_num_used_get(struct k_mem_slab *slab)
S{
S	return slab->num_used;
S}
S
S/**
S * @brief Get the number of unused blocks in a memory slab.
S *
S * This routine gets the number of memory blocks that are currently
S * unallocated in @a slab.
S *
S * @param slab Address of the memory slab.
S *
S * @return Number of unallocated memory blocks.
S */
Sstatic inline u32_t k_mem_slab_num_free_get(struct k_mem_slab *slab)
S{
S	return slab->num_blocks - slab->num_used;
S}
S
S/**
S * @} end defgroup mem_slab_apis
S */
S
S/**
S * @cond INTERNAL_HIDDEN
S */
S
Sstruct k_mem_pool_lvl {
S	union {
S		u32_t *bits_p;
S		u32_t bits;
S	};
S	sys_dlist_t free_list;
S};
S
Sstruct k_mem_pool {
S	void *buf;
S	size_t max_sz;
S	u16_t n_max;
S	u8_t n_levels;
S	u8_t max_inline_level;
S	struct k_mem_pool_lvl *levels;
S	_wait_q_t wait_q;
S};
S
S#define _ALIGN4(n) ((((n)+3)/4)*4)
S
S#define _MPOOL_HAVE_LVL(max, min, l) (((max) >> (2*(l))) >= (min) ? 1 : 0)
S
S#define _MPOOL_LVLS(maxsz, minsz)		\
S	(_MPOOL_HAVE_LVL(maxsz, minsz, 0) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 1) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 2) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 3) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 4) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 5) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 6) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 7) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 8) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 9) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 10) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 11) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 12) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 13) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 14) +	\
S	_MPOOL_HAVE_LVL(maxsz, minsz, 15))
X#define _MPOOL_LVLS(maxsz, minsz)			(_MPOOL_HAVE_LVL(maxsz, minsz, 0) +		_MPOOL_HAVE_LVL(maxsz, minsz, 1) +		_MPOOL_HAVE_LVL(maxsz, minsz, 2) +		_MPOOL_HAVE_LVL(maxsz, minsz, 3) +		_MPOOL_HAVE_LVL(maxsz, minsz, 4) +		_MPOOL_HAVE_LVL(maxsz, minsz, 5) +		_MPOOL_HAVE_LVL(maxsz, minsz, 6) +		_MPOOL_HAVE_LVL(maxsz, minsz, 7) +		_MPOOL_HAVE_LVL(maxsz, minsz, 8) +		_MPOOL_HAVE_LVL(maxsz, minsz, 9) +		_MPOOL_HAVE_LVL(maxsz, minsz, 10) +		_MPOOL_HAVE_LVL(maxsz, minsz, 11) +		_MPOOL_HAVE_LVL(maxsz, minsz, 12) +		_MPOOL_HAVE_LVL(maxsz, minsz, 13) +		_MPOOL_HAVE_LVL(maxsz, minsz, 14) +		_MPOOL_HAVE_LVL(maxsz, minsz, 15))
S
S/* Rounds the needed bits up to integer multiples of u32_t */
S#define _MPOOL_LBIT_WORDS_UNCLAMPED(n_max, l) \
S	((((n_max) << (2*(l))) + 31) / 32)
X#define _MPOOL_LBIT_WORDS_UNCLAMPED(n_max, l) 	((((n_max) << (2*(l))) + 31) / 32)
S
S/* One word gets stored free unioned with the pointer, otherwise the
S * calculated unclamped value
S */
S#define _MPOOL_LBIT_WORDS(n_max, l)			\
S	(_MPOOL_LBIT_WORDS_UNCLAMPED(n_max, l) < 2 ? 0	\
S	 : _MPOOL_LBIT_WORDS_UNCLAMPED(n_max, l))
X#define _MPOOL_LBIT_WORDS(n_max, l)				(_MPOOL_LBIT_WORDS_UNCLAMPED(n_max, l) < 2 ? 0		 : _MPOOL_LBIT_WORDS_UNCLAMPED(n_max, l))
S
S/* How many bytes for the bitfields of a single level? */
S#define _MPOOL_LBIT_BYTES(maxsz, minsz, l, n_max)	\
S	(_MPOOL_LVLS((maxsz), (minsz)) >= (l) ?		\
S	 4 * _MPOOL_LBIT_WORDS((n_max), l) : 0)
X#define _MPOOL_LBIT_BYTES(maxsz, minsz, l, n_max)		(_MPOOL_LVLS((maxsz), (minsz)) >= (l) ?			 4 * _MPOOL_LBIT_WORDS((n_max), l) : 0)
S
S/* Size of the bitmap array that follows the buffer in allocated memory */
S#define _MPOOL_BITS_SIZE(maxsz, minsz, n_max) \
S	(_MPOOL_LBIT_BYTES(maxsz, minsz, 0, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 1, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 2, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 3, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 4, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 5, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 6, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 7, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 8, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 9, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 10, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 11, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 12, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 13, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 14, n_max) +	\
S	_MPOOL_LBIT_BYTES(maxsz, minsz, 15, n_max))
X#define _MPOOL_BITS_SIZE(maxsz, minsz, n_max) 	(_MPOOL_LBIT_BYTES(maxsz, minsz, 0, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 1, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 2, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 3, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 4, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 5, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 6, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 7, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 8, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 9, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 10, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 11, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 12, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 13, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 14, n_max) +		_MPOOL_LBIT_BYTES(maxsz, minsz, 15, n_max))
S
S/**
S * INTERNAL_HIDDEN @endcond
S */
S
S/**
S * @addtogroup mem_pool_apis
S * @{
S */
S
S/**
S * @brief Statically define and initialize a memory pool.
S *
S * The memory pool's buffer contains @a n_max blocks that are @a max_size bytes
S * long. The memory pool allows blocks to be repeatedly partitioned into
S * quarters, down to blocks of @a min_size bytes long. The buffer is aligned
S * to a @a align -byte boundary.
S *
S * If the pool is to be accessed outside the module where it is defined, it
S * can be declared via
S *
S * @code extern struct k_mem_pool <name>; @endcode
S *
S * @param name Name of the memory pool.
S * @param minsz Size of the smallest blocks in the pool (in bytes).
S * @param maxsz Size of the largest blocks in the pool (in bytes).
S * @param nmax Number of maximum sized blocks in the pool.
S * @param align Alignment of the pool's buffer (power of 2).
S */
S#define K_MEM_POOL_DEFINE(name, minsz, maxsz, nmax, align)		\
S	char __aligned(align) _mpool_buf_##name[_ALIGN4(maxsz * nmax)	\
S				  + _MPOOL_BITS_SIZE(maxsz, minsz, nmax)]; \
S	struct k_mem_pool_lvl _mpool_lvls_##name[_MPOOL_LVLS(maxsz, minsz)]; \
S	struct k_mem_pool name __in_section(_k_mem_pool, static, name) = { \
S		.buf = _mpool_buf_##name,				\
S		.max_sz = maxsz,					\
S		.n_max = nmax,						\
S		.n_levels = _MPOOL_LVLS(maxsz, minsz),			\
S		.levels = _mpool_lvls_##name,				\
S	}
X#define K_MEM_POOL_DEFINE(name, minsz, maxsz, nmax, align)			char __aligned(align) _mpool_buf_##name[_ALIGN4(maxsz * nmax)					  + _MPOOL_BITS_SIZE(maxsz, minsz, nmax)]; 	struct k_mem_pool_lvl _mpool_lvls_##name[_MPOOL_LVLS(maxsz, minsz)]; 	struct k_mem_pool name __in_section(_k_mem_pool, static, name) = { 		.buf = _mpool_buf_##name,						.max_sz = maxsz,							.n_max = nmax,								.n_levels = _MPOOL_LVLS(maxsz, minsz),					.levels = _mpool_lvls_##name,					}
S
S/**
S * @brief Allocate memory from a memory pool.
S *
S * This routine allocates a memory block from a memory pool.
S *
S * @param pool Address of the memory pool.
S * @param block Pointer to block descriptor for the allocated memory.
S * @param size Amount of memory to allocate (in bytes).
S * @param timeout Maximum time to wait for operation to complete
S *        (in milliseconds). Use K_NO_WAIT to return without waiting,
S *        or K_FOREVER to wait as long as necessary.
S *
S * @retval 0 Memory allocated. The @a data field of the block descriptor
S *         is set to the starting address of the memory block.
S * @retval -ENOMEM Returned without waiting.
S * @retval -EAGAIN Waiting period timed out.
S */
Sextern int k_mem_pool_alloc(struct k_mem_pool *pool, struct k_mem_block *block,
S			    size_t size, s32_t timeout);
S
S/**
S * @brief Free memory allocated from a memory pool.
S *
S * This routine releases a previously allocated memory block back to its
S * memory pool.
S *
S * @param block Pointer to block descriptor for the allocated memory.
S *
S * @return N/A
S */
Sextern void k_mem_pool_free(struct k_mem_block *block);
S
S/**
S * @brief Defragment a memory pool.
S *
S * This is a no-op API preserved for backward compatibility only.
S *
S * @param pool Unused
S *
S * @return N/A
S */
Sstatic inline void k_mem_pool_defrag(struct k_mem_pool *pool) {}
S
S/**
S * @} end addtogroup mem_pool_apis
S */
S
S/**
S * @defgroup heap_apis Heap Memory Pool APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/**
S * @brief Allocate memory from heap.
S *
S * This routine provides traditional malloc() semantics. Memory is
S * allocated from the heap memory pool.
S *
S * @param size Amount of memory requested (in bytes).
S *
S * @return Address of the allocated memory if successful; otherwise NULL.
S */
Sextern void *k_malloc(size_t size);
S
S/**
S * @brief Free memory allocated from heap.
S *
S * This routine provides traditional free() semantics. The memory being
S * returned must have been allocated from the heap memory pool.
S *
S * If @a ptr is NULL, no operation is performed.
S *
S * @param ptr Pointer to previously allocated memory.
S *
S * @return N/A
S */
Sextern void k_free(void *ptr);
S
S/**
S * @} end defgroup heap_apis
S */
S
S/* polling API - PRIVATE */
S
S#define _INIT_OBJ_POLL_EVENT(obj) do { (obj)->poll_event = NULL; } while ((0))
S
S/* private - implementation data created as needed, per-type */
Sstruct _poller {
S	struct k_thread *thread;
S};
S
S/* private - types bit positions */
Senum _poll_types_bits {
S	/* can be used to ignore an event */
S	_POLL_TYPE_IGNORE,
S
S	/* to be signaled by k_poll_signal() */
S	_POLL_TYPE_SIGNAL,
S
S	/* semaphore availability */
S	_POLL_TYPE_SEM_AVAILABLE,
S
S	/* queue/fifo/lifo data availability */
S	_POLL_TYPE_DATA_AVAILABLE,
S
S	_POLL_NUM_TYPES
S};
S
S#define _POLL_TYPE_BIT(type) (1 << ((type) - 1))
S
S/* private - states bit positions */
Senum _poll_states_bits {
S	/* default state when creating event */
S	_POLL_STATE_NOT_READY,
S
S	/* signaled by k_poll_signal() */
S	_POLL_STATE_SIGNALED,
S
S	/* semaphore is available */
S	_POLL_STATE_SEM_AVAILABLE,
S
S	/* data is available to read on queue/fifo/lifo */
S	_POLL_STATE_DATA_AVAILABLE,
S
S	_POLL_NUM_STATES
S};
S
S#define _POLL_STATE_BIT(state) (1 << ((state) - 1))
S
S#define _POLL_EVENT_NUM_UNUSED_BITS \
S	(32 - (0 \
S	       + 8 /* tag */ \
S	       + _POLL_NUM_TYPES \
S	       + _POLL_NUM_STATES \
S	       + 1 /* modes */ \
S	      ))
X#define _POLL_EVENT_NUM_UNUSED_BITS 	(32 - (0 	       + 8   	       + _POLL_NUM_TYPES 	       + _POLL_NUM_STATES 	       + 1   	      ))
S
S#if _POLL_EVENT_NUM_UNUSED_BITS < 0
S#error overflow of 32-bit word in struct k_poll_event
S#endif
S
S/* end of polling API - PRIVATE */
S
S
S/**
S * @defgroup poll_apis Async polling APIs
S * @ingroup kernel_apis
S * @{
S */
S
S/* Public polling API */
S
S/* public - values for k_poll_event.type bitfield */
S#define K_POLL_TYPE_IGNORE 0
S#define K_POLL_TYPE_SIGNAL _POLL_TYPE_BIT(_POLL_TYPE_SIGNAL)
S#define K_POLL_TYPE_SEM_AVAILABLE _POLL_TYPE_BIT(_POLL_TYPE_SEM_AVAILABLE)
S#define K_POLL_TYPE_DATA_AVAILABLE _POLL_TYPE_BIT(_POLL_TYPE_DATA_AVAILABLE)
S#define K_POLL_TYPE_FIFO_DATA_AVAILABLE K_POLL_TYPE_DATA_AVAILABLE
S
S/* public - polling modes */
Senum k_poll_modes {
S	/* polling thread does not take ownership of objects when available */
S	K_POLL_MODE_NOTIFY_ONLY = 0,
S
S	K_POLL_NUM_MODES
S};
S
S/* public - values for k_poll_event.state bitfield */
S#define K_POLL_STATE_NOT_READY 0
S#define K_POLL_STATE_SIGNALED _POLL_STATE_BIT(_POLL_STATE_SIGNALED)
S#define K_POLL_STATE_SEM_AVAILABLE _POLL_STATE_BIT(_POLL_STATE_SEM_AVAILABLE)
S#define K_POLL_STATE_DATA_AVAILABLE _POLL_STATE_BIT(_POLL_STATE_DATA_AVAILABLE)
S#define K_POLL_STATE_FIFO_DATA_AVAILABLE K_POLL_STATE_DATA_AVAILABLE
S
S/* public - poll signal object */
Sstruct k_poll_signal {
S	/* PRIVATE - DO NOT TOUCH */
S	sys_dlist_t poll_events;
S
S	/*
S	 * 1 if the event has been signaled, 0 otherwise. Stays set to 1 until
S	 * user resets it to 0.
S	 */
S	unsigned int signaled;
S
S	/* custom result value passed to k_poll_signal() if needed */
S	int result;
S};
S
S#define K_POLL_SIGNAL_INITIALIZER(obj) \
S	{ \
S	.poll_events = SYS_DLIST_STATIC_INIT(&obj.poll_events), \
S	.signaled = 0, \
S	.result = 0, \
S	}
X#define K_POLL_SIGNAL_INITIALIZER(obj) 	{ 	.poll_events = SYS_DLIST_STATIC_INIT(&obj.poll_events), 	.signaled = 0, 	.result = 0, 	}
S
Sstruct k_poll_event {
S	/* PRIVATE - DO NOT TOUCH */
S	sys_dnode_t _node;
S
S	/* PRIVATE - DO NOT TOUCH */
S	struct _poller *poller;
S
S	/* optional user-specified tag, opaque, untouched by the API */
S	u32_t tag:8;
S
S	/* bitfield of event types (bitwise-ORed K_POLL_TYPE_xxx values) */
S	u32_t type:_POLL_NUM_TYPES;
S
S	/* bitfield of event states (bitwise-ORed K_POLL_STATE_xxx values) */
S	u32_t state:_POLL_NUM_STATES;
S
S	/* mode of operation, from enum k_poll_modes */
S	u32_t mode:1;
S
S	/* unused bits in 32-bit word */
S	u32_t unused:_POLL_EVENT_NUM_UNUSED_BITS;
S
S	/* per-type data */
S	union {
S		void *obj;
S		struct k_poll_signal *signal;
S		struct k_sem *sem;
S		struct k_fifo *fifo;
S		struct k_queue *queue;
S	};
S};
S
S#define K_POLL_EVENT_INITIALIZER(event_type, event_mode, event_obj) \
S	{ \
S	.poller = NULL, \
S	.type = event_type, \
S	.state = K_POLL_STATE_NOT_READY, \
S	.mode = event_mode, \
S	.unused = 0, \
S	{ .obj = event_obj }, \
S	}
X#define K_POLL_EVENT_INITIALIZER(event_type, event_mode, event_obj) 	{ 	.poller = NULL, 	.type = event_type, 	.state = K_POLL_STATE_NOT_READY, 	.mode = event_mode, 	.unused = 0, 	{ .obj = event_obj }, 	}
S
S#define K_POLL_EVENT_STATIC_INITIALIZER(event_type, event_mode, event_obj, \
S					event_tag) \
S	{ \
S	.type = event_type, \
S	.tag = event_tag, \
S	.state = K_POLL_STATE_NOT_READY, \
S	.mode = event_mode, \
S	.unused = 0, \
S	{ .obj = event_obj }, \
S	}
X#define K_POLL_EVENT_STATIC_INITIALIZER(event_type, event_mode, event_obj, 					event_tag) 	{ 	.type = event_type, 	.tag = event_tag, 	.state = K_POLL_STATE_NOT_READY, 	.mode = event_mode, 	.unused = 0, 	{ .obj = event_obj }, 	}
S
S/**
S * @brief Initialize one struct k_poll_event instance
S *
S * After this routine is called on a poll event, the event it ready to be
S * placed in an event array to be passed to k_poll().
S *
S * @param event The event to initialize.
S * @param type A bitfield of the types of event, from the K_POLL_TYPE_xxx
S *             values. Only values that apply to the same object being polled
S *             can be used together. Choosing K_POLL_TYPE_IGNORE disables the
S *             event.
S * @param mode Future. Use K_POLL_MODE_NOTIFY_ONLY.
S * @param obj Kernel object or poll signal.
S *
S * @return N/A
S */
S
Sextern void k_poll_event_init(struct k_poll_event *event, u32_t type,
S			      int mode, void *obj);
S
S/**
S * @brief Wait for one or many of multiple poll events to occur
S *
S * This routine allows a thread to wait concurrently for one or many of
S * multiple poll events to have occurred. Such events can be a kernel object
S * being available, like a semaphore, or a poll signal event.
S *
S * When an event notifies that a kernel object is available, the kernel object
S * is not "given" to the thread calling k_poll(): it merely signals the fact
S * that the object was available when the k_poll() call was in effect. Also,
S * all threads trying to acquire an object the regular way, i.e. by pending on
S * the object, have precedence over the thread polling on the object. This
S * means that the polling thread will never get the poll event on an object
S * until the object becomes available and its pend queue is empty. For this
S * reason, the k_poll() call is more effective when the objects being polled
S * only have one thread, the polling thread, trying to acquire them.
S *
S * When k_poll() returns 0, the caller should loop on all the events that were
S * passed to k_poll() and check the state field for the values that were
S * expected and take the associated actions.
S *
S * Before being reused for another call to k_poll(), the user has to reset the
S * state field to K_POLL_STATE_NOT_READY.
S *
S * @param events An array of pointers to events to be polled for.
S * @param num_events The number of events in the array.
S * @param timeout Waiting period for an event to be ready (in milliseconds),
S *                or one of the special values K_NO_WAIT and K_FOREVER.
S *
S * @retval 0 One or more events are ready.
S * @retval -EAGAIN Waiting period timed out.
S */
S
Sextern int k_poll(struct k_poll_event *events, int num_events,
S		  s32_t timeout);
S
S/**
S * @brief Initialize a poll signal object.
S *
S * Ready a poll signal object to be signaled via k_poll_signal().
S *
S * @param signal A poll signal.
S *
S * @return N/A
S */
S
Sextern void k_poll_signal_init(struct k_poll_signal *signal);
S
S/**
S * @brief Signal a poll signal object.
S *
S * This routine makes ready a poll signal, which is basically a poll event of
S * type K_POLL_TYPE_SIGNAL. If a thread was polling on that event, it will be
S * made ready to run. A @a result value can be specified.
S *
S * The poll signal contains a 'signaled' field that, when set by
S * k_poll_signal(), stays set until the user sets it back to 0. It thus has to
S * be reset by the user before being passed again to k_poll() or k_poll() will
S * consider it being signaled, and will return immediately.
S *
S * @param signal A poll signal.
S * @param result The value to store in the result field of the signal.
S *
S * @retval 0 The signal was delivered successfully.
S * @retval -EAGAIN The polling thread's timeout is in the process of expiring.
S */
S
Sextern int k_poll_signal(struct k_poll_signal *signal, int result);
S
S/* private internal function */
Sextern int _handle_obj_poll_events(sys_dlist_t *events, u32_t state);
S
S/**
S * @} end defgroup poll_apis
S */
S
S/**
S * @brief Make the CPU idle.
S *
S * This function makes the CPU idle until an event wakes it up.
S *
S * In a regular system, the idle thread should be the only thread responsible
S * for making the CPU idle and triggering any type of power management.
S * However, in some more constrained systems, such as a single-threaded system,
S * the only thread would be responsible for this if needed.
S *
S * @return N/A
S */
Sextern void k_cpu_idle(void);
S
S/**
S * @brief Make the CPU idle in an atomic fashion.
S *
S * Similar to k_cpu_idle(), but called with interrupts locked if operations
S * must be done atomically before making the CPU idle.
S *
S * @param key Interrupt locking key obtained from irq_lock().
S *
S * @return N/A
S */
Sextern void k_cpu_atomic_idle(unsigned int key);
S
Sextern void _sys_power_save_idle_exit(s32_t ticks);
S
S#include <arch/cpu.h>
S
S#ifdef _ARCH_EXCEPT
S/* This archtecture has direct support for triggering a CPU exception */
S#define _k_except_reason(reason)	_ARCH_EXCEPT(reason)
S#else
S
S#include <misc/printk.h>
S
S/* NOTE: This is the implementation for arches that do not implement
S * _ARCH_EXCEPT() to generate a real CPU exception.
S *
S * We won't have a real exception frame to determine the PC value when
S * the oops occurred, so print file and line number before we jump into
S * the fatal error handler.
S */
S#define _k_except_reason(reason) do { \
S		printk("@ %s:%d:\n", __FILE__,  __LINE__); \
S		_NanoFatalErrorHandler(reason, &_default_esf); \
S		CODE_UNREACHABLE; \
S	} while (0)
X#define _k_except_reason(reason) do { 		printk("@ %s:%d:\n", __FILE__,  __LINE__); 		_NanoFatalErrorHandler(reason, &_default_esf); 		CODE_UNREACHABLE; 	} while (0)
S
S#endif /* _ARCH__EXCEPT */
S
S/**
S * @brief Fatally terminate a thread
S *
S * This should be called when a thread has encountered an unrecoverable
S * runtime condition and needs to terminate. What this ultimately
S * means is determined by the _fatal_error_handler() implementation, which
S * will be called will reason code _NANO_ERR_KERNEL_OOPS.
S *
S * If this is called from ISR context, the default system fatal error handler
S * will treat it as an unrecoverable system error, just like k_panic().
S */
S#define k_oops()	_k_except_reason(_NANO_ERR_KERNEL_OOPS)
S
S/**
S * @brief Fatally terminate the system
S *
S * This should be called when the Zephyr kernel has encountered an
S * unrecoverable runtime condition and needs to terminate. What this ultimately
S * means is determined by the _fatal_error_handler() implementation, which
S * will be called will reason code _NANO_ERR_KERNEL_PANIC.
S */
S#define k_panic()	_k_except_reason(_NANO_ERR_KERNEL_PANIC)
S
S/*
S * private APIs that are utilized by one or more public APIs
S */
S
Sextern void _init_static_threads(void);
S
S
Sextern int _is_thread_essential(void);
Sextern void _timer_expiration_handler(struct _timeout *t);
S
S/* arch/cpu.h may declare an architecture or platform-specific macro
S * for properly declaring stacks, compatible with MMU/MPU constraints if
S * enabled
S */
S#ifdef _ARCH_THREAD_STACK_DEFINE
S#define K_THREAD_STACK_DEFINE(sym, size) _ARCH_THREAD_STACK_DEFINE(sym, size)
S#define K_THREAD_STACK_ARRAY_DEFINE(sym, nmemb, size) \
S		_ARCH_THREAD_STACK_ARRAY_DEFINE(sym, nmemb, size)
X#define K_THREAD_STACK_ARRAY_DEFINE(sym, nmemb, size) 		_ARCH_THREAD_STACK_ARRAY_DEFINE(sym, nmemb, size)
S#define K_THREAD_STACK_MEMBER(sym, size) _ARCH_THREAD_STACK_MEMBER(sym, size)
S#define K_THREAD_STACK_SIZEOF(sym) _ARCH_THREAD_STACK_SIZEOF(sym)
Sstatic inline char *K_THREAD_STACK_BUFFER(k_thread_stack_t sym)
S{
S	return _ARCH_THREAD_STACK_BUFFER(sym);
S}
S#else
S/**
S * @brief Declare a toplevel thread stack memory region
S *
S * This declares a region of memory suitable for use as a thread's stack.
S *
S * This is the generic, historical definition. Align to STACK_ALIGN and put in
S * 'noinit' section so that it isn't zeroed at boot
S *
S * The declared symbol will always be a k_thread_stack_t which can be passed to
S * k_thread_create, but should otherwise not be manipulated. If the buffer
S * inside needs to be examined, use K_THREAD_STACK_BUFFER().
S *
S * It is legal to precede this definition with the 'static' keyword.
S *
S * It is NOT legal to take the sizeof(sym) and pass that to the stackSize
S * parameter of k_thread_create(), it may not be the same as the
S * 'size' parameter. Use K_THREAD_STACK_SIZEOF() instead.
S *
S * @param sym Thread stack symbol name
S * @param size Size of the stack memory region
S */
S#define K_THREAD_STACK_DEFINE(sym, size) \
S	struct _k_thread_stack_element __noinit __aligned(STACK_ALIGN) sym[size]
X#define K_THREAD_STACK_DEFINE(sym, size) 	struct _k_thread_stack_element __noinit __aligned(STACK_ALIGN) sym[size]
S
S/**
S * @brief Declare a toplevel array of thread stack memory regions
S *
S * Create an array of equally sized stacks. See K_THREAD_STACK_DEFINE
S * definition for additional details and constraints.
S *
S * This is the generic, historical definition. Align to STACK_ALIGN and put in
S * 'noinit' section so that it isn't zeroed at boot
S *
S * @param sym Thread stack symbol name
S * @param nmemb Number of stacks to declare
S * @param size Size of the stack memory region
S */
S
S#define K_THREAD_STACK_ARRAY_DEFINE(sym, nmemb, size) \
S	struct _k_thread_stack_element __noinit \
S		__aligned(STACK_ALIGN) sym[nmemb][size]
X#define K_THREAD_STACK_ARRAY_DEFINE(sym, nmemb, size) 	struct _k_thread_stack_element __noinit 		__aligned(STACK_ALIGN) sym[nmemb][size]
S
S/**
S * @brief Declare an embedded stack memory region
S *
S * Used for stacks embedded within other data structures. Use is highly
S * discouraged but in some cases necessary. For memory protection scenarios,
S * it is very important that any RAM preceding this member not be writable
S * by threads else a stack overflow will lead to silent corruption. In other
S * words, the containing data structure should live in RAM owned by the kernel.
S *
S * @param sym Thread stack symbol name
S * @param size Size of the stack memory region
S */
S#define K_THREAD_STACK_MEMBER(sym, size) \
S	struct _k_thread_stack_element __aligned(STACK_ALIGN) sym[size]
X#define K_THREAD_STACK_MEMBER(sym, size) 	struct _k_thread_stack_element __aligned(STACK_ALIGN) sym[size]
S
S/**
S * @brief Return the size in bytes of a stack memory region
S *
S * Convenience macro for passing the desired stack size to k_thread_create()
S * since the underlying implementation may actually create something larger
S * (for instance a guard area).
S *
S * The value returned here is guaranteed to match the 'size' parameter
S * passed to K_THREAD_STACK_DEFINE.
S *
S * Do not use this for stacks declared with K_THREAD_STACK_ARRAY_DEFINE(),
S * it is not guaranteed to return the original value since each array
S * element must be aligned.
S *
S * @param sym Stack memory symbol
S * @return Size of the stack
S */
S#define K_THREAD_STACK_SIZEOF(sym) sizeof(sym)
S
S/**
S * @brief Get a pointer to the physical stack buffer
S *
S * Convenience macro to get at the real underlying stack buffer used by
S * the CPU. Guaranteed to be a character pointer of size K_THREAD_STACK_SIZEOF.
S * This is really only intended for diagnostic tools which want to examine
S * stack memory contents.
S *
S * @param sym Declared stack symbol name
S * @return The buffer itself, a char *
S */
Sstatic inline char *K_THREAD_STACK_BUFFER(k_thread_stack_t sym)
S{
S	return (char *)sym;
S}
S
S#endif /* _ARCH_DECLARE_STACK */
S
S#ifdef __cplusplus
S}
S#endif
S
S#if defined(CONFIG_CPLUSPLUS) && defined(__cplusplus)
S/*
S * Define new and delete operators.
S * At this moment, the operators do nothing since objects are supposed
S * to be statically allocated.
S */
Sinline void operator delete(void *ptr)
S{
S	(void)ptr;
S}
S
Sinline void operator delete[](void *ptr)
S{
S	(void)ptr;
S}
S
Sinline void *operator new(size_t size)
S{
S	(void)size;
S	return NULL;
S}
S
Sinline void *operator new[](size_t size)
S{
S	(void)size;
S	return NULL;
S}
S
S/* Placement versions of operator new and delete */
Sinline void operator delete(void *ptr1, void *ptr2)
S{
S	(void)ptr1;
S	(void)ptr2;
S}
S
Sinline void operator delete[](void *ptr1, void *ptr2)
S{
S	(void)ptr1;
S	(void)ptr2;
S}
S
Sinline void *operator new(size_t size, void *ptr)
S{
S	(void)size;
S	return ptr;
S}
S
Sinline void *operator new[](size_t size, void *ptr)
S{
S	(void)size;
S	return ptr;
S}
S
S#endif /* defined(CONFIG_CPLUSPLUS) && defined(__cplusplus) */
S
S#endif /* !_ASMLANGUAGE */
S
N#endif /* _kernel__h_ */
L 17 "..\..\..\..\include\sys_io.h" 2
N#include <zephyr/types.h>
N#include <stddef.h>
N
Ntypedef u32_t io_port_t;
Ntypedef u32_t mm_reg_t;
Ntypedef u32_t mem_addr_t;
N
N/* Port I/O functions */
N
N/**
N * @fn static inline void sys_out8(u8_t data, io_port_t port);
N * @brief Output a byte to an I/O port
N *
N * This function writes a byte to the given port.
N *
N * @param data the byte to write
N * @param port the port address where to write the byte
N */
N
N/**
N * @fn static inline u8_t sys_in8(io_port_t port);
N * @brief Input a byte from an I/O port
N *
N * This function reads a byte from the port.
N *
N * @param port the port address from where to read the byte
N *
N * @return the byte read
N */
N
N/**
N * @fn static inline void sys_out16(u16_t data, io_port_t port);
N * @brief Output a 16 bits to an I/O port
N *
N * This function writes a 16 bits to the given port.
N *
N * @param data the 16 bits to write
N * @param port the port address where to write the 16 bits
N */
N
N/**
N * @fn static inline u16_t sys_in16(io_port_t port);
N * @brief Input 16 bits from an I/O port
N *
N * This function reads 16 bits from the port.
N *
N * @param port the port address from where to read the 16 bits
N *
N * @return the 16 bits read
N */
N
N/**
N * @fn static inline void sys_out32(u32_t data, io_port_t port);
N * @brief Output 32 bits to an I/O port
N *
N * This function writes 32 bits to the given port.
N *
N * @param data the 32 bits to write
N * @param port the port address where to write the 32 bits
N */
N
N/**
N * @fn static inline u32_t sys_in32(io_port_t port);
N * @brief Input 32 bits from an I/O port
N *
N * This function reads 32 bits from the port.
N *
N * @param port the port address from where to read the 32 bits
N *
N * @return the 32 bits read
N */
N
N/**
N * @fn static inline void sys_io_set_bit(io_port_t port, unsigned int bit)
N * @brief Set the designated bit from port to 1
N *
N * This functions takes the designated bit starting from port and sets it to 1.
N *
N * @param port the port address from where to look for the bit
N * @param bit the designated bit to set (from 0 to n)
N */
N
N/**
N * @fn static inline void sys_io_clear_bit(io_port_t port, unsigned int bit)
N * @brief Clear the designated bit from port to 0
N *
N * This functions takes the designated bit starting from port and sets it to 0.
N *
N * @param port the port address from where to look for the bit
N * @param bit the designated bit to clear (from 0 to n)
N */
N
N/**
N * @fn static inline int sys_io_test_bit(io_port_t port, unsigned int bit)
N * @brief Test the bit from port if it is set or not
N *
N * This functions takes the designated bit starting from port and tests its
N * current setting. It will return the current setting.
N *
N * @param port the port address from where to look for the bit
N * @param bit the designated bit to test (from 0 to n)
N *
N * @return 1 if it is set, 0 otherwise
N */
N
N/**
N * @fn static inline int sys_io_test_and_set_bit(io_port_t port, unsigned int bit)
N * @brief Test the bit from port and set it
N *
N * This functions takes the designated bit starting from port, tests its
N * current setting and sets it. It will return the previous setting.
N *
N * @param port the port address from where to look for the bit
N * @param bit the designated bit to test and set (from 0 to n)
N *
N * @return 1 if it was set, 0 otherwise
N */
N
N/**
N * @fn static inline int sys_io_test_and_clear_bit(io_port_t port, unsigned int bit)
N * @brief Test the bit from port and clear it
N *
N * This functions takes the designated bit starting from port, tests its
N * current setting and clears it. It will return the previous setting.
N *
N * @param port the port address from where to look for the bit
N * @param bit the designated bit to test and clear (from 0 to n)
N *
N * @return 0 if it was clear, 1 otherwise
N */
N
N/* Memory mapped registers I/O functions */
N
N/**
N * @fn static inline void sys_write8(u8_t data, mm_reg_t addr);
N * @brief Write a byte to a memory mapped register
N *
N * This function writes a byte to the given memory mapped register.
N *
N * @param data the byte to write
N * @param addr the memory mapped register address where to write the byte
N */
N
N/**
N * @fn static inline u8_t sys_read8(mm_reg_t addr);
N * @brief Read a byte from a memory mapped register
N *
N * This function reads a byte from the given memory mapped register.
N *
N * @param addr the memory mapped register address from where to read the byte
N *
N * @return the byte read
N */
N
N/**
N * @fn static inline void sys_write16(u16_t data, mm_reg_t addr);
N * @brief Write 16 bits to a memory mapped register
N *
N * This function writes 16 bits to the given memory mapped register.
N *
N * @param data the 16 bits to write
N * @param addr the memory mapped register address where to write the 16 bits
N */
N
N/**
N * @fn static inline u16_t sys_read16(mm_reg_t addr);
N * @brief Read 16 bits from a memory mapped register
N *
N * This function reads 16 bits from the given memory mapped register.
N *
N * @param addr the memory mapped register address from where to read
N *        the 16 bits
N *
N * @return the 16 bits read
N */
N
N/**
N * @fn static inline void sys_write32(u32_t data, mm_reg_t addr);
N * @brief Write 32 bits to a memory mapped register
N *
N * This function writes 32 bits to the given memory mapped register.
N *
N * @param data the 32 bits to write
N * @param addr the memory mapped register address where to write the 32 bits
N */
N
N/**
N * @fn static inline u32_t sys_read32(mm_reg_t addr);
N * @brief Read 32 bits from a memory mapped register
N *
N * This function reads 32 bits from the given memory mapped register.
N *
N * @param addr the memory mapped register address from where to read
N *        the 32 bits
N *
N * @return the 32 bits read
N */
N
N/* Memory bits manipulation functions */
N
N/**
N * @fn static inline void sys_set_bit(mem_addr_t addr, unsigned int bit)
N * @brief Set the designated bit from addr to 1
N *
N * This functions takes the designated bit starting from addr and sets it to 1.
N *
N * @param addr the memory address from where to look for the bit
N * @param bit the designated bit to set (from 0 to 31)
N */
N
N/**
N * @fn static inline void sys_clear_bit(mem_addr_t addr, unsigned int bit)
N * @brief Clear the designated bit from addr to 0
N *
N * This functions takes the designated bit starting from addr and sets it to 0.
N *
N * @param addr the memory address from where to look for the bit
N * @param bit the designated bit to clear (from 0 to 31)
N */
N
N/**
N * @fn static inline int sys_test_bit(mem_addr_t addr, unsigned int bit)
N * @brief Test the bit if it is set or not
N *
N * This functions takes the designated bit starting from addr and tests its
N * current setting. It will return the current setting.
N *
N * @param addr the memory address from where to look for the bit
N * @param bit the designated bit to test (from 0 to 31)
N *
N * @return 1 if it is set, 0 otherwise
N */
N
N/**
N * @fn static inline int sys_test_and_set_bit(mem_addr_t addr, unsigned int bit)
N * @brief Test the bit and set it
N *
N * This functions takes the designated bit starting from addr, tests its
N * current setting and sets it. It will return the previous setting.
N *
N * @param addr the memory address from where to look for the bit
N * @param bit the designated bit to test and set (from 0 to 31)
N *
N * @return 1 if it was set, 0 otherwise
N */
N
N/**
N * @fn static inline int sys_test_and_clear_bit(mem_addr_t addr, unsigned int bit)
N * @brief Test the bit and clear it
N *
N * This functions takes the designated bit starting from addr, test its
N * current setting and clears it. It will return the previous setting.
N *
N * @param addr the memory address from where to look for the bit
N * @param bit the designated bit to test and clear (from 0 to 31)
N *
N * @return 0 if it was clear, 1 otherwise
N */
N
N/**
N * @fn static inline void sys_bitfield_set_bit(mem_addr_t addr, unsigned int bit)
N * @brief Set the designated bit from addr to 1
N *
N * This functions takes the designated bit starting from addr and sets it to 1.
N *
N * @param addr the memory address from where to look for the bit
N * @param bit the designated bit to set (arbitrary)
N */
N
N/**
N * @fn static inline void sys_bitfield_clear_bit(mem_addr_t addr, unsigned int bit)
N * @brief Clear the designated bit from addr to 0
N *
N * This functions takes the designated bit starting from addr and sets it to 0.
N *
N * @param addr the memory address from where to look for the bit
N * @param bit the designated bit to clear (arbitrary)
N */
N
N/**
N * @fn static inline int sys_bitfield_test_bit(mem_addr_t addr, unsigned int bit)
N * @brief Test the bit if it is set or not
N *
N * This functions takes the designated bit starting from addr and tests its
N * current setting. It will return the current setting.
N *
N * @param addr the memory address from where to look for the bit
N * @param bit the designated bit to test (arbitrary
N *
N * @return 1 if it is set, 0 otherwise
N */
N
N/**
N * @fn static inline int sys_bitfield_test_and_set_bit(mem_addr_t addr, unsigned int bit)
N * @brief Test the bit and set it
N *
N * This functions takes the designated bit starting from addr, tests its
N * current setting and sets it. It will return the previous setting.
N *
N * @param addr the memory address from where to look for the bit
N * @param bit the designated bit to test and set (arbitrary)
N *
N * @return 1 if it was set, 0 otherwise
N */
N
N/**
N * @fn static inline int sys_bitfield_test_and_clear_bit(mem_addr_t addr, unsigned int bit)
N * @brief Test the bit and clear it
N *
N * This functions takes the designated bit starting from addr, test its
N * current setting and clears it. It will return the previous setting.
N *
N * @param addr the memory address from where to look for the bit
N * @param bit the designated bit to test and clear (arbitrary)
N *
N * @return 0 if it was clear, 1 otherwise
N */
N
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* __SYS_IO_H__ */
L 17 "..\..\..\..\include\arch/arm/cortex_m/sys_io.h" 2
N
N/* Memory mapped registers I/O functions */
N
Nstatic inline u32_t sys_read32(mem_addr_t addr)
N{
N	return *(volatile u32_t *)addr;
N}
N
N
Nstatic inline void sys_write32(u32_t data, mem_addr_t addr)
N{
N	*(volatile u32_t *)addr = data;
N}
N
N
N/* Memory bit manipulation functions */
N
Nstatic inline void sys_set_bit(mem_addr_t addr, unsigned int bit)
N{
N	u32_t temp = *(volatile u32_t *)addr;
N
N	*(volatile u32_t *)addr = temp | (1 << bit);
N}
N
Nstatic inline void sys_clear_bit(mem_addr_t addr, unsigned int bit)
N{
N	u32_t temp = *(volatile u32_t *)addr;
N
N	*(volatile u32_t *)addr = temp & ~(1 << bit);
N}
N
N#endif /* !_ASMLANGUAGE */
N
N#endif /* _CORTEX_M_SYS_IO_H_ */
L 37 "..\..\..\..\include\arch/arm/arch.h" 2
N
N
N/**
N * @brief Declare the STACK_ALIGN_SIZE
N *
N * Denotes the required alignment of the stack pointer on public API
N * boundaries
N *
N */
N#define STACK_ALIGN_SIZE 8
N
N/**
N * @brief Declare a minimum MPU guard alignment and size
N *
N * This specifies the minimum MPU guard alignment/size for the MPU.  This
N * will be used to denote the guard section of the stack, if it exists.
N *
N * One key note is that this guard results in extra bytes being added to
N * the stack.  APIs which give the stack ptr and stack size will take this
N * guard size into account.
N *
N * Stack is allocated, but initial stack pointer is at the end
N * (highest address).  Stack grows down to the actual allocation
N * address (lowest address).  Stack guard, if present, will comprise
N * the lowest MPU_GUARD_ALIGN_AND_SIZE bytes of the stack.
N *
N * As the stack grows down, it will reach the end of the stack when it
N * encounters either the stack guard region, or the stack allocation
N * address.
N *
N * ----------------------- <---- Stack allocation address + stack size +
N * |                     |            MPU_GUARD_ALIGN_AND_SIZE
N * |  Some thread data   | <---- Defined when thread is created
N * |        ...          |
N * |---------------------| <---- Actual initial stack ptr
N * |  Initial Stack Ptr  |       aligned to STACK_ALIGN_SIZE
N * |        ...          |
N * |        ...          |
N * |        ...          |
N * |        ...          |
N * |        ...          |
N * |        ...          |
N * |        ...          |
N * |        ...          |
N * |  Stack Ends         |
N * |---------------------- <---- Stack Buffer Ptr from API
N * |  MPU Guard,         |
N * |     if present      |
N * ----------------------- <---- Stack Allocation address
N *
N */
N#define MPU_GUARD_ALIGN_AND_SIZE	0
N
N/**
N * @brief Define alignment of a stack buffer
N *
N * This is used for two different things:
N * 1) Used in checks for stack size to be a multiple of the stack buffer
N *    alignment
N * 2) Used to determine the alignment of a stack buffer
N *
N */
N#define STACK_ALIGN	max(STACK_ALIGN_SIZE, MPU_GUARD_ALIGN_AND_SIZE)
N
N/**
N * @brief Declare a toplevel thread stack memory region
N *
N * This declares a region of memory suitable for use as a thread's stack.
N *
N * This is the generic, historical definition. Align to STACK_ALIGN_SIZE and
N * put in * 'noinit' section so that it isn't zeroed at boot
N *
N * The declared symbol will always be a character array which can be passed to
N * k_thread_create, but should otherwise not be manipulated.
N *
N * It is legal to precede this definition with the 'static' keyword.
N *
N * It is NOT legal to take the sizeof(sym) and pass that to the stackSize
N * parameter of k_thread_create(), it may not be the same as the
N * 'size' parameter. Use K_THREAD_STACK_SIZEOF() instead.
N *
N * @param sym Thread stack symbol name
N * @param size Size of the stack memory region
N */
N#define _ARCH_THREAD_STACK_DEFINE(sym, size) \
N	struct _k_thread_stack_element __noinit __aligned(STACK_ALIGN) \
N		sym[size+MPU_GUARD_ALIGN_AND_SIZE]
X#define _ARCH_THREAD_STACK_DEFINE(sym, size) 	struct _k_thread_stack_element __noinit __aligned(STACK_ALIGN) 		sym[size+MPU_GUARD_ALIGN_AND_SIZE]
N
N/**
N * @brief Declare a toplevel array of thread stack memory regions
N *
N * Create an array of equally sized stacks. See K_THREAD_STACK_DEFINE
N * definition for additional details and constraints.
N *
N * This is the generic, historical definition. Align to STACK_ALIGN_SIZE and
N * put in * 'noinit' section so that it isn't zeroed at boot
N *
N * @param sym Thread stack symbol name
N * @param nmemb Number of stacks to declare
N * @param size Size of the stack memory region
N */
N#define _ARCH_THREAD_STACK_ARRAY_DEFINE(sym, nmemb, size) \
N	struct _k_thread_stack_element __noinit __aligned(STACK_ALIGN) \
N		sym[nmemb][size+MPU_GUARD_ALIGN_AND_SIZE]
X#define _ARCH_THREAD_STACK_ARRAY_DEFINE(sym, nmemb, size) 	struct _k_thread_stack_element __noinit __aligned(STACK_ALIGN) 		sym[nmemb][size+MPU_GUARD_ALIGN_AND_SIZE]
N
N/**
N * @brief Declare an embedded stack memory region
N *
N * Used for stacks embedded within other data structures. Use is highly
N * discouraged but in some cases necessary. For memory protection scenarios,
N * it is very important that any RAM preceding this member not be writable
N * by threads else a stack overflow will lead to silent corruption. In other
N * words, the containing data structure should live in RAM owned by the kernel.
N *
N * @param sym Thread stack symbol name
N * @param size Size of the stack memory region
N */
N#define _ARCH_THREAD_STACK_MEMBER(sym, size) \
N	struct _k_thread_stack_element __aligned(STACK_ALIGN) \
N		sym[size+MPU_GUARD_ALIGN_AND_SIZE]
X#define _ARCH_THREAD_STACK_MEMBER(sym, size) 	struct _k_thread_stack_element __aligned(STACK_ALIGN) 		sym[size+MPU_GUARD_ALIGN_AND_SIZE]
N
N/**
N * @brief Return the size in bytes of a stack memory region
N *
N * Convenience macro for passing the desired stack size to k_thread_create()
N * since the underlying implementation may actually create something larger
N * (for instance a guard area).
N *
N * The value returned here is guaranteed to match the 'size' parameter
N * passed to K_THREAD_STACK_DEFINE and related macros.
N *
N * @param sym Stack memory symbol
N * @return Size of the stack
N */
N#define _ARCH_THREAD_STACK_SIZEOF(sym) (sizeof(sym) - MPU_GUARD_ALIGN_AND_SIZE)
N
N/**
N * @brief Get a pointer to the physical stack buffer
N *
N * Convenience macro to get at the real underlying stack buffer used by
N * the CPU. Guaranteed to be a character pointer of size K_THREAD_STACK_SIZEOF.
N * This is really only intended for diagnostic tools which want to examine
N * stack memory contents.
N *
N * @param sym Declared stack symbol name
N * @return The buffer itself, a char *
N */
N#define _ARCH_THREAD_STACK_BUFFER(sym) \
N		((char *)(sym) + MPU_GUARD_ALIGN_AND_SIZE)
X#define _ARCH_THREAD_STACK_BUFFER(sym) 		((char *)(sym) + MPU_GUARD_ALIGN_AND_SIZE)
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* _ARM_ARCH__H_ */
L 13 "..\..\..\..\include\arch/cpu.h" 2
N
N#endif /* __ARCHCPU_H__ */
L 3719 "..\..\..\..\include\kernel.h" 2
N
N#ifdef _ARCH_EXCEPT
N/* This archtecture has direct support for triggering a CPU exception */
N#define _k_except_reason(reason)	_ARCH_EXCEPT(reason)
N#else
S
S#include <misc/printk.h>
S
S/* NOTE: This is the implementation for arches that do not implement
S * _ARCH_EXCEPT() to generate a real CPU exception.
S *
S * We won't have a real exception frame to determine the PC value when
S * the oops occurred, so print file and line number before we jump into
S * the fatal error handler.
S */
S#define _k_except_reason(reason) do { \
S		printk("@ %s:%d:\n", __FILE__,  __LINE__); \
S		_NanoFatalErrorHandler(reason, &_default_esf); \
S		CODE_UNREACHABLE; \
S	} while (0)
X#define _k_except_reason(reason) do { 		printk("@ %s:%d:\n", __FILE__,  __LINE__); 		_NanoFatalErrorHandler(reason, &_default_esf); 		CODE_UNREACHABLE; 	} while (0)
S
N#endif /* _ARCH__EXCEPT */
N
N/**
N * @brief Fatally terminate a thread
N *
N * This should be called when a thread has encountered an unrecoverable
N * runtime condition and needs to terminate. What this ultimately
N * means is determined by the _fatal_error_handler() implementation, which
N * will be called will reason code _NANO_ERR_KERNEL_OOPS.
N *
N * If this is called from ISR context, the default system fatal error handler
N * will treat it as an unrecoverable system error, just like k_panic().
N */
N#define k_oops()	_k_except_reason(_NANO_ERR_KERNEL_OOPS)
N
N/**
N * @brief Fatally terminate the system
N *
N * This should be called when the Zephyr kernel has encountered an
N * unrecoverable runtime condition and needs to terminate. What this ultimately
N * means is determined by the _fatal_error_handler() implementation, which
N * will be called will reason code _NANO_ERR_KERNEL_PANIC.
N */
N#define k_panic()	_k_except_reason(_NANO_ERR_KERNEL_PANIC)
N
N/*
N * private APIs that are utilized by one or more public APIs
N */
N
Nextern void _init_static_threads(void);
N
N
Nextern int _is_thread_essential(void);
Nextern void _timer_expiration_handler(struct _timeout *t);
N
N/* arch/cpu.h may declare an architecture or platform-specific macro
N * for properly declaring stacks, compatible with MMU/MPU constraints if
N * enabled
N */
N#ifdef _ARCH_THREAD_STACK_DEFINE
N#define K_THREAD_STACK_DEFINE(sym, size) _ARCH_THREAD_STACK_DEFINE(sym, size)
N#define K_THREAD_STACK_ARRAY_DEFINE(sym, nmemb, size) \
N		_ARCH_THREAD_STACK_ARRAY_DEFINE(sym, nmemb, size)
X#define K_THREAD_STACK_ARRAY_DEFINE(sym, nmemb, size) 		_ARCH_THREAD_STACK_ARRAY_DEFINE(sym, nmemb, size)
N#define K_THREAD_STACK_MEMBER(sym, size) _ARCH_THREAD_STACK_MEMBER(sym, size)
N#define K_THREAD_STACK_SIZEOF(sym) _ARCH_THREAD_STACK_SIZEOF(sym)
Nstatic inline char *K_THREAD_STACK_BUFFER(k_thread_stack_t sym)
N{
N	return _ARCH_THREAD_STACK_BUFFER(sym);
X	return ((char *)(sym) + 0);
N}
N#else
S/**
S * @brief Declare a toplevel thread stack memory region
S *
S * This declares a region of memory suitable for use as a thread's stack.
S *
S * This is the generic, historical definition. Align to STACK_ALIGN and put in
S * 'noinit' section so that it isn't zeroed at boot
S *
S * The declared symbol will always be a k_thread_stack_t which can be passed to
S * k_thread_create, but should otherwise not be manipulated. If the buffer
S * inside needs to be examined, use K_THREAD_STACK_BUFFER().
S *
S * It is legal to precede this definition with the 'static' keyword.
S *
S * It is NOT legal to take the sizeof(sym) and pass that to the stackSize
S * parameter of k_thread_create(), it may not be the same as the
S * 'size' parameter. Use K_THREAD_STACK_SIZEOF() instead.
S *
S * @param sym Thread stack symbol name
S * @param size Size of the stack memory region
S */
S#define K_THREAD_STACK_DEFINE(sym, size) \
S	struct _k_thread_stack_element __noinit __aligned(STACK_ALIGN) sym[size]
X#define K_THREAD_STACK_DEFINE(sym, size) 	struct _k_thread_stack_element __noinit __aligned(STACK_ALIGN) sym[size]
S
S/**
S * @brief Declare a toplevel array of thread stack memory regions
S *
S * Create an array of equally sized stacks. See K_THREAD_STACK_DEFINE
S * definition for additional details and constraints.
S *
S * This is the generic, historical definition. Align to STACK_ALIGN and put in
S * 'noinit' section so that it isn't zeroed at boot
S *
S * @param sym Thread stack symbol name
S * @param nmemb Number of stacks to declare
S * @param size Size of the stack memory region
S */
S
S#define K_THREAD_STACK_ARRAY_DEFINE(sym, nmemb, size) \
S	struct _k_thread_stack_element __noinit \
S		__aligned(STACK_ALIGN) sym[nmemb][size]
X#define K_THREAD_STACK_ARRAY_DEFINE(sym, nmemb, size) 	struct _k_thread_stack_element __noinit 		__aligned(STACK_ALIGN) sym[nmemb][size]
S
S/**
S * @brief Declare an embedded stack memory region
S *
S * Used for stacks embedded within other data structures. Use is highly
S * discouraged but in some cases necessary. For memory protection scenarios,
S * it is very important that any RAM preceding this member not be writable
S * by threads else a stack overflow will lead to silent corruption. In other
S * words, the containing data structure should live in RAM owned by the kernel.
S *
S * @param sym Thread stack symbol name
S * @param size Size of the stack memory region
S */
S#define K_THREAD_STACK_MEMBER(sym, size) \
S	struct _k_thread_stack_element __aligned(STACK_ALIGN) sym[size]
X#define K_THREAD_STACK_MEMBER(sym, size) 	struct _k_thread_stack_element __aligned(STACK_ALIGN) sym[size]
S
S/**
S * @brief Return the size in bytes of a stack memory region
S *
S * Convenience macro for passing the desired stack size to k_thread_create()
S * since the underlying implementation may actually create something larger
S * (for instance a guard area).
S *
S * The value returned here is guaranteed to match the 'size' parameter
S * passed to K_THREAD_STACK_DEFINE.
S *
S * Do not use this for stacks declared with K_THREAD_STACK_ARRAY_DEFINE(),
S * it is not guaranteed to return the original value since each array
S * element must be aligned.
S *
S * @param sym Stack memory symbol
S * @return Size of the stack
S */
S#define K_THREAD_STACK_SIZEOF(sym) sizeof(sym)
S
S/**
S * @brief Get a pointer to the physical stack buffer
S *
S * Convenience macro to get at the real underlying stack buffer used by
S * the CPU. Guaranteed to be a character pointer of size K_THREAD_STACK_SIZEOF.
S * This is really only intended for diagnostic tools which want to examine
S * stack memory contents.
S *
S * @param sym Declared stack symbol name
S * @return The buffer itself, a char *
S */
Sstatic inline char *K_THREAD_STACK_BUFFER(k_thread_stack_t sym)
S{
S	return (char *)sym;
S}
S
N#endif /* _ARCH_DECLARE_STACK */
N
N#ifdef __cplusplus
S}
N#endif
N
N#if defined(CONFIG_CPLUSPLUS) && defined(__cplusplus)
X#if 0L && 0L
S/*
S * Define new and delete operators.
S * At this moment, the operators do nothing since objects are supposed
S * to be statically allocated.
S */
Sinline void operator delete(void *ptr)
S{
S	(void)ptr;
S}
S
Sinline void operator delete[](void *ptr)
S{
S	(void)ptr;
S}
S
Sinline void *operator new(size_t size)
S{
S	(void)size;
S	return NULL;
S}
S
Sinline void *operator new[](size_t size)
S{
S	(void)size;
S	return NULL;
S}
S
S/* Placement versions of operator new and delete */
Sinline void operator delete(void *ptr1, void *ptr2)
S{
S	(void)ptr1;
S	(void)ptr2;
S}
S
Sinline void operator delete[](void *ptr1, void *ptr2)
S{
S	(void)ptr1;
S	(void)ptr2;
S}
S
Sinline void *operator new(size_t size, void *ptr)
S{
S	(void)size;
S	return ptr;
S}
S
Sinline void *operator new[](size_t size, void *ptr)
S{
S	(void)size;
S	return ptr;
S}
S
N#endif /* defined(CONFIG_CPLUSPLUS) && defined(__cplusplus) */
N
N#endif /* !_ASMLANGUAGE */
N
N#endif /* _kernel__h_ */
L 8 "..\..\..\..\drivers\uart\uart_acts.c" 2
N#include <init.h>
L 1 "..\..\..\..\include\init.h" 1
N
N/*
N * Copyright (c) 2015 Intel Corporation.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N#ifndef _INIT_H_
N#define _INIT_H_
N
N#include <device.h>
L 1 "..\..\..\..\include\device.h" 1
N
N/*
N * Copyright (c) 2015 Intel Corporation.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N#ifndef _DEVICE_H_
N#define _DEVICE_H_
N
N#include <kernel.h>
N
N/**
N * @brief Device Driver APIs
N * @defgroup io_interfaces Device Driver APIs
N * @{
N * @}
N */
N/**
N * @brief Device Model APIs
N * @defgroup device_model Device Model APIs
N * @{
N */
N
N#include <zephyr/types.h>
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
Nstatic const int _INIT_LEVEL_PRE_KERNEL_1 = 1;
Nstatic const int _INIT_LEVEL_PRE_KERNEL_2 = 1;
Nstatic const int _INIT_LEVEL_POST_KERNEL = 1;
Nstatic const int _INIT_LEVEL_APPLICATION = 1;
N
N/**
N * @def DEVICE_INIT
N *
N * @brief Create device object and set it up for boot time initialization.
N *
N * @details This macro defines a device object that is automatically
N * configured by the kernel during system initialization.
N *
N * @param dev_name Device name.
N *
N * @param drv_name The name this instance of the driver exposes to
N * the system.
N *
N * @param init_fn Address to the init function of the driver.
N *
N * @param data Pointer to the device's configuration data.
N *
N * @param cfg_info The address to the structure containing the
N * configuration information for this instance of the driver.
N *
N * @param level The initialization level at which configuration occurs.
N * Must be one of the following symbols, which are listed in the order
N * they are performed by the kernel:
N * \n
N * \li PRE_KERNEL_1: Used for devices that have no dependencies, such as those
N * that rely solely on hardware present in the processor/SOC. These devices
N * cannot use any kernel services during configuration, since they are not
N * yet available.
N * \n
N * \li PRE_KERNEL_2: Used for devices that rely on the initialization of devices
N * initialized as part of the PRIMARY level. These devices cannot use any
N * kernel services during configuration, since they are not yet available.
N * \n
N * \li POST_KERNEL: Used for devices that require kernel services during
N * configuration.
N * \n
N * \li APPLICATION: Used for application components (i.e. non-kernel components)
N * that need automatic configuration. These devices can use all services
N * provided by the kernel during configuration.
N *
N * @param prio The initialization priority of the device, relative to
N * other devices of the same initialization level. Specified as an integer
N * value in the range 0 to 99; lower values indicate earlier initialization.
N * Must be a decimal integer literal without leading zeroes or sign (e.g. 32),
N * or an equivalent symbolic name (e.g. \#define MY_INIT_PRIO 32); symbolic
N * expressions are *not* permitted
N * (e.g. CONFIG_KERNEL_INIT_PRIORITY_DEFAULT + 5).
N */
N
N/**
N * @def DEVICE_AND_API_INIT
N *
N * @brief Create device object and set it up for boot time initialization,
N * with the option to set driver_api.
N *
N * @copydetails DEVICE_INIT
N * @param api Provides an initial pointer to the API function struct
N * used by the driver. Can be NULL.
N * @details The driver api is also set here, eliminating the need to do that
N * during initialization.
N */
N
N/**
N * @def DEVICE_DEFINE
N *
N * @brief Create device object and set it up for boot time initialization,
N * with the option to device_pm_control.
N *
N * @copydetails DEVICE_AND_API_INIT
N * @param pm_control_fn Pointer to device_pm_control function.
N * Can be empty function (device_pm_control_nop) if not implemented.
N */
N#define DEVICE_DEFINE(dev_name, drv_name, init_fn, pm_control_fn, \
N		      data, cfg_info, level, prio, api) \
N	\
N	static struct device_config _CONCAT(__config_, dev_name) __used \
N	__attribute__((__section__(".devconfig.init"))) = { \
N		.name = drv_name, .init = (init_fn), \
N		.device_pm_control = (pm_control_fn), \
N		.config_info = (cfg_info) \
N	}; \
N	static struct device _CONCAT(__device_, dev_name) __used \
N	__attribute__((__section__(".init_" #level STRINGIFY(prio)))) = { \
N		 .config = &_CONCAT(__config_, dev_name), \
N		 .driver_api = api, \
N		 .driver_data = data \
N	}
X#define DEVICE_DEFINE(dev_name, drv_name, init_fn, pm_control_fn, 		      data, cfg_info, level, prio, api) 		static struct device_config _CONCAT(__config_, dev_name) __used 	__attribute__((__section__(".devconfig.init"))) = { 		.name = drv_name, .init = (init_fn), 		.device_pm_control = (pm_control_fn), 		.config_info = (cfg_info) 	}; 	static struct device _CONCAT(__device_, dev_name) __used 	__attribute__((__section__(".init_" #level STRINGIFY(prio)))) = { 		 .config = &_CONCAT(__config_, dev_name), 		 .driver_api = api, 		 .driver_data = data 	}
N/*
N * Use the default device_pm_control for devices that do not call the
N * DEVICE_DEFINE macro so that caller of hook functions
N * need not check device_pm_control != NULL.
N */
N#define DEVICE_AND_API_INIT(dev_name, drv_name, init_fn, data, cfg_info, \
N			    level, prio, api) \
N	DEVICE_DEFINE(dev_name, drv_name, init_fn, \
N		      device_pm_control_nop, data, cfg_info, level, \
N		      prio, api)
X#define DEVICE_AND_API_INIT(dev_name, drv_name, init_fn, data, cfg_info, 			    level, prio, api) 	DEVICE_DEFINE(dev_name, drv_name, init_fn, 		      device_pm_control_nop, data, cfg_info, level, 		      prio, api)
N
N#define DEVICE_INIT(dev_name, drv_name, init_fn, data, cfg_info, level, prio) \
N	DEVICE_AND_API_INIT(dev_name, drv_name, init_fn, data, cfg_info, \
N			    level, prio, NULL)
X#define DEVICE_INIT(dev_name, drv_name, init_fn, data, cfg_info, level, prio) 	DEVICE_AND_API_INIT(dev_name, drv_name, init_fn, data, cfg_info, 			    level, prio, NULL)
N
N/**
N * @def DEVICE_NAME_GET
N *
N * @brief Expands to the full name of a global device object
N *
N * @details Return the full name of a device object symbol created by
N * DEVICE_INIT(), using the dev_name provided to DEVICE_INIT().
N *
N * It is meant to be used for declaring extern symbols pointing on device
N * objects before using the DEVICE_GET macro to get the device object.
N *
N * @param name The same as dev_name provided to DEVICE_INIT()
N *
N * @return The expanded name of the device object created by DEVICE_INIT()
N */
N#define DEVICE_NAME_GET(name) (_CONCAT(__device_, name))
N
N/**
N * @def DEVICE_GET
N *
N * @brief Obtain a pointer to a device object by name
N *
N * @details Return the address of a device object created by
N * DEVICE_INIT(), using the dev_name provided to DEVICE_INIT().
N *
N * @param name The same as dev_name provided to DEVICE_INIT()
N *
N * @return A pointer to the device object created by DEVICE_INIT()
N */
N#define DEVICE_GET(name) (&DEVICE_NAME_GET(name))
N
N /** @def DEVICE_DECLARE
N  *
N  * @brief Declare a static device object
N  *
N  * This macro can be used at the top-level to declare a device, such
N  * that DEVICE_GET() may be used before the full declaration in
N  * DEVICE_INIT().
N  *
N  * This is often useful when configuring interrupts statically in a
N  * device's init or per-instance config function, as the init function
N  * itself is required by DEVICE_INIT() and use of DEVICE_GET()
N  * inside it creates a circular dependency.
N  *
N  * @param name Device name
N  */
N#define DEVICE_DECLARE(name) static struct device DEVICE_NAME_GET(name)
N
Nstruct device;
N
N/**
N * @brief Device Power Management APIs
N * @defgroup device_power_management_api Device Power Management APIs
N * @ingroup power_management_api
N * @{
N */
N
N/**
N * @}
N */
N
N/** @def DEVICE_PM_ACTIVE_STATE
N *
N * @brief device is in ACTIVE power state
N *
N * @details Normal operation of the device. All device context is retained.
N */
N#define DEVICE_PM_ACTIVE_STATE		1
N
N/** @def DEVICE_PM_LOW_POWER_STATE
N *
N * @brief device is in LOW power state
N *
N * @details Device context is preserved by the HW and need not be
N * restored by the driver.
N */
N#define DEVICE_PM_LOW_POWER_STATE	2
N
N/** @def DEVICE_PM_SUSPEND_STATE
N *
N * @brief device is in SUSPEND power state
N *
N * @details Most device context is lost by the hardware.
N * Device drivers must save and restore or reinitialize any context
N * lost by the hardware
N */
N#define DEVICE_PM_SUSPEND_STATE		3
N
N/** @def DEVICE_PM_OFF_STATE
N *
N * @brief device is in OFF power state
N *
N * @details - Power has been fully removed from the device.
N * The device context is lost when this state is entered, so the OS
N * software will reinitialize the device when powering it back on
N */
N#define DEVICE_PM_OFF_STATE		4
N
N/* Constants defining support device power commands */
N#define DEVICE_PM_SET_POWER_STATE	1
N#define DEVICE_PM_GET_POWER_STATE	2
N
N/**
N * @brief Static device information (In ROM) Per driver instance
N *
N * @param name name of the device
N * @param init init function for the driver
N * @param config_info address of driver instance config information
N */
Nstruct device_config {
N	char	*name;
N	int (*init)(struct device *device);
N	int (*device_pm_control)(struct device *device, u32_t command,
N			      void *context);
N	const void *config_info;
N};
N
N/**
N * @brief Runtime device structure (In memory) Per driver instance
N * @param device_config Build time config information
N * @param driver_api pointer to structure containing the API functions for
N * the device type. This pointer is filled in by the driver at init time.
N * @param driver_data driver instance data. For driver use only
N */
Nstruct device {
N	struct device_config *config;
N	const void *driver_api;
N	void *driver_data;
N};
N
Nvoid _sys_device_do_config_level(int level);
N
N/**
N * @brief Retrieve the device structure for a driver by name
N *
N * @details Device objects are created via the DEVICE_INIT() macro and
N * placed in memory by the linker. If a driver needs to bind to another driver
N * it can use this function to retrieve the device structure of the lower level
N * driver by the name the driver exposes to the system.
N *
N * @param name device name to search for.
N *
N * @return pointer to device structure; NULL if not found or cannot be used.
N */
Nstruct device *device_get_binding(const char *name);
N
N/**
N * @brief Device Power Management APIs
N * @defgroup device_power_management_api Device Power Management APIs
N * @ingroup power_management_api
N * @{
N */
N
N/**
N * @brief Indicate that the device is in the middle of a transaction
N *
N * Called by a device driver to indicate that it is in the middle of a
N * transaction.
N *
N * @param busy_dev Pointer to device structure of the driver instance.
N */
Nvoid device_busy_set(struct device *busy_dev);
N
N/**
N * @brief Indicate that the device has completed its transaction
N *
N * Called by a device driver to indicate the end of a transaction.
N *
N * @param busy_dev Pointer to device structure of the driver instance.
N */
Nvoid device_busy_clear(struct device *busy_dev);
N
N/*
N * Device PM functions
N */
N
N/**
N * @brief No-op function to initialize unimplemented hook
N *
N * This function should be used to initialize device hook
N * for which a device has no PM operations.
N *
N * @param unused_device Unused
N * @param unused_ctrl_command Unused
N * @param unused_context Unused
N *
N * @retval 0 Always returns 0
N */
Nint device_pm_control_nop(struct device *unused_device,
N		       u32_t unused_ctrl_command, void *unused_context);
N/**
N * @brief Call the set power state function of a device
N *
N * Called by the application or power management service to let the device do
N * required operations when moving to the required power state
N * Note that devices may support just some of the device power states
N * @param device Pointer to device structure of the driver instance.
N * @param device_power_state Device power state to be set
N *
N * @retval 0 If successful.
N * @retval Errno Negative errno code if failure.
N */
Nstatic inline int device_set_power_state(struct device *device,
N					 u32_t device_power_state)
N{
N	return device->config->device_pm_control(device,
N			DEVICE_PM_SET_POWER_STATE, &device_power_state);
X			1, &device_power_state);
N}
N
N/**
N * @brief Call the get power state function of a device
N *
N * This function lets the caller know the current device
N * power state at any time. This state will be one of the defined
N * power states allowed for the devices in that system
N *
N * @param device pointer to device structure of the driver instance.
N * @param device_power_state Device power state to be filled by the device
N *
N * @retval 0 If successful.
N * @retval Errno Negative errno code if failure.
N */
Nstatic inline int device_get_power_state(struct device *device,
N					 u32_t *device_power_state)
N{
N	return device->config->device_pm_control(device,
N				DEVICE_PM_GET_POWER_STATE, device_power_state);
X				2, device_power_state);
N}
N
N/**
N * @brief Gets the device structure list array and device count
N *
N * Called by the Power Manager application to get the list of
N * device structures associated with the devices in the system.
N * The PM app would use this list to create its own sorted list
N * based on the order it wishes to suspend or resume the devices.
N *
N * @param device_list Pointer to receive the device list array
N * @param device_count Pointer to receive the device count
N */
Nvoid device_list_get(struct device **device_list, int *device_count);
N
N
N/**
N * @brief Check if any device is in the middle of a transaction
N *
N * Called by an application to see if any device is in the middle
N * of a critical transaction that cannot be interrupted.
N *
N * @retval 0 if no device is busy
N * @retval -EBUSY if any device is busy
N */
Nint device_any_busy_check(void);
N
N/**
N * @brief Check if a specific device is in the middle of a transaction
N *
N * Called by an application to see if a particular device is in the
N * middle of a critical transaction that cannot be interrupted.
N *
N * @param chk_dev Pointer to device structure of the specific device driver
N * the caller is interested in.
N * @retval 0 if the device is not busy
N * @retval -EBUSY if the device is busy
N */
Nint device_busy_check(struct device *chk_dev);
N
Nextern struct device Image$$RW_IRAM_DEVICE_PRE_KERNEL_1$$Base[];
Nextern struct device Image$$RW_IRAM_DEVICE_PRE_KERNEL_2$$Base[];
Nextern struct device Image$$RW_IRAM_DEVICE_POST_KERNEL$$Base[];
Nextern struct device Image$$RW_IRAM_DEVICE_APPLICATION$$Base[];
Nextern struct device Image$$RW_IRAM_DEVICE_APPLICATION$$Limit[];
N
Nextern u32_t Image$$RW_IRAM_DEVICE_BUSY$$Base[];
N
Nextern u8_t *__device_busy_start;
Nextern u16_t DEVICE_BUSY_SIZE;
N
Nvoid set_devices_state(u32_t state);
Nvoid dump_all_devices(void);
N/**
N * @}
N */
N
N#ifdef __cplusplus
S}
N#endif
N/**
N * @}
N */
N#endif /* _DEVICE_H_ */
L 12 "..\..\..\..\include\init.h" 2
N#include <toolchain.h>
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N/*
N * System initialization levels. The PRE_KERNEL_1 and PRE_KERNEL_2 levels are
N * executed in the kernel's initialization context, which uses the interrupt
N * stack. The remaining levels are executed in the kernel's main task.
N */
N
N#define _SYS_INIT_LEVEL_PRE_KERNEL_1	0
N#define _SYS_INIT_LEVEL_PRE_KERNEL_2	1
N#define _SYS_INIT_LEVEL_POST_KERNEL	2
N#define _SYS_INIT_LEVEL_APPLICATION	3
N
N
N/* Counter use to avoid issues if two or more system devices are declared
N * in the same C file with the same init function
N */
N#define _SYS_NAME(init_fn) _CONCAT(_CONCAT(sys_init_, init_fn), __COUNTER__)
N
N/**
N * @def SYS_INIT
N *
N * @brief Run an initialization function at boot at specified priority
N *
N * @details This macro lets you run a function at system boot.
N *
N * @param init_fn Pointer to the boot function to run
N *
N * @param level The initialization level, See DEVICE_INIT for details.
N *
N * @param prio Priority within the selected initialization level. See
N * DEVICE_INIT for details.
N */
N#define SYS_INIT(init_fn, level, prio) \
N	DEVICE_INIT(_SYS_NAME(init_fn), "", init_fn, NULL, NULL, level, prio)
X#define SYS_INIT(init_fn, level, prio) 	DEVICE_INIT(_SYS_NAME(init_fn), "", init_fn, NULL, NULL, level, prio)
N
N/**
N * @def SYS_DEVICE_DEFINE
N *
N * @brief Run an initialization function at boot at specified priority,
N * and define device PM control function.
N *
N * @copydetails SYS_INIT
N * @param pm_control_fn Pointer to device_pm_control function.
N * Can be empty function (device_pm_control_nop) if not implemented.
N * @param drv_name Name of this system device
N */
N#define SYS_DEVICE_DEFINE(drv_name, init_fn, pm_control_fn, level, prio) \
N	DEVICE_DEFINE(_SYS_NAME(init_fn), drv_name, init_fn, pm_control_fn, \
N		      NULL, NULL, level, prio, NULL)
X#define SYS_DEVICE_DEFINE(drv_name, init_fn, pm_control_fn, level, prio) 	DEVICE_DEFINE(_SYS_NAME(init_fn), drv_name, init_fn, pm_control_fn, 		      NULL, NULL, level, prio, NULL)
N
N
Nvoid register_kernel_cfg_func(void *Keil_kernel_cfg_func);
N
Nvoid app_main(void);
Nvoid _sys_device_do_config_level(int level);
Nstruct device *device_get_binding(const char *name);
Nint kernel_config_init(void);
Nvoid _PrepC(void *keil_isr_table, void *keil_app_main,
N				void *keil_sys_device_do_config_level,
N				void *keil_device_get_binding, void *Keil_kernel_cfg_func);
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* _INIT_H_ */
L 9 "..\..\..\..\drivers\uart\uart_acts.c" 2
N#include <soc.h>
L 1 "..\..\..\..\arch\soc\atb110x\soc.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file SoC configuration macros.
N */
N
N#ifndef _ACTIONS_SOC_H_
N#define _ACTIONS_SOC_H_
N
N#include <stdbool.h>
N#include "soc_patch.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_patch.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file ADC configuration macros for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_PATCH_H_
N#define	_ACTIONS_SOC_PATCH_H_
N
N#ifndef _ASMLANGUAGE
N
N#define     __IO    volatile             /*!< Defines 'read / write' permissions */
N
N#define MEM_CTL_BASE (0x40009000)
N
N#define FUNREPLACE_NUM_MAX 12
Ntypedef struct {
N	__IO u32_t CTL;          /* Offset: 0x0070 (R/W)  CodeReplace CTL */
X	volatile u32_t CTL;           
N	u32_t RESERVE1[3];
N	__IO u32_t func[FUNREPLACE_NUM_MAX];           /* Offset: 0x80~0xAC (R/W)  The alternate instruction code. */
X	volatile u32_t func[12];            
N	__IO u32_t FIXADDR[FUNREPLACE_NUM_MAX];           /* Offset: 0xB0~0xDC (R/W)  Fix address */
X	volatile u32_t FIXADDR[12];            
N} FUNREPLACE_TypeDef;
N
N#define FUNREPLACE				((FUNREPLACE_TypeDef *) (MEM_CTL_BASE + 0x70))
N
N#define CODEREPLACE_NUM_MAX 12
Ntypedef struct {
N	__IO u32_t CTL;          /* Offset: 0x0C (R/W)  CodeReplace CTL */
X	volatile u32_t CTL;           
N	__IO u32_t Instr[CODEREPLACE_NUM_MAX];           /* Offset: 0x10~0x3C (R/W)  The alternate instruction code. */
X	volatile u32_t Instr[12];            
N	__IO u32_t FIXADDR[CODEREPLACE_NUM_MAX];           /* Offset: 0x040~0x6C (R/W)  Fix address */
X	volatile u32_t FIXADDR[12];            
N} CODEREPLACE_TypeDef;
N
N#define CODEREPLACE				((CODEREPLACE_TypeDef *) (MEM_CTL_BASE + 0x0c))
N
Nstruct function_patch_t {
N	void *new_function;
N	void *old_function;
N};
N
Nstruct code_patch_t {
N	void *new_code;
N	void *old_code_addr;
N};
N
N#define FUNCTION_PATCH_REGISTER(name, patch, function) \
N	static struct function_patch_t __function_patch_##name __used \
N__attribute__((used, section(".patch_hw_func")))  = { \
N		  .new_function = (void *)patch, \
N		  .old_function = (void *)function \
N	}
X#define FUNCTION_PATCH_REGISTER(name, patch, function) 	static struct function_patch_t __function_patch_##name __used __attribute__((used, section(".patch_hw_func")))  = { 		  .new_function = (void *)patch, 		  .old_function = (void *)function 	}
N
N#define CODE_PATCH_REGISTER(name, code, addr) \
N	static struct code_patch_t __code_patch_##name __used \
N__attribute__((used, section(".patch_hw_code")))  = { \
N		  .new_code = (void *)code, \
N		  .old_code_addr = (void *)addr \
N	}
X#define CODE_PATCH_REGISTER(name, code, addr) 	static struct code_patch_t __code_patch_##name __used __attribute__((used, section(".patch_hw_code")))  = { 		  .new_code = (void *)code, 		  .old_code_addr = (void *)addr 	}
N
Nextern struct shell_module Image$$RW_IRAM_PATCH_HW_FUNC$$Base[];
Nextern struct shell_module Image$$RW_IRAM_PATCH_HW_FUNC$$Limit[];
N
Nextern struct code_patch_t Image$$RW_IRAM_PATCH_HW_CODE$$Base[];
Nextern struct code_patch_t Image$$RW_IRAM_PATCH_HW_CODE$$Limit[];
N
Nvoid patch_hw_func(void);
Nvoid patch_hw_code(void);
Nvoid patch_sw(void);
N
N
N#endif /* _ASMLANGUAGE */
N
N#endif /* _ACTIONS_SOC_PATCH_H_	*/
L 16 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N
N#include "soc_regs.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_regs.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file register address for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_REGS_H_
N#define	_ACTIONS_SOC_REGS_H_
N
N#define CMU_DIGITAL_BASE		0x40002000
N#define CMU_ANALOG_BASE			0x40002100
N
N#define CMU_DEVRST_BASE			0x40002010
N#define CMU_DEVCLKEN_BASE		0x40002014
N
N#define CMU_SPI0CLK			0x40002018
N#define CMU_SPI1CLK			0x4000201c
N#define CMU_SPI2CLK			0x40002020
N#define CMU_PWM0CLK			0x40002024
N#define CMU_PWM1CLK			0x40002028
N#define CMU_PWM2CLK			0x4000202c
N#define CMU_PWM3CLK			0x40002030
N#define CMU_PWM4CLK			0x40002034
N#define CMU_AUDIOCLK 		0x40002038
N
N#define RTC_REG_BASE		0x40004000
N
N#define	TIMER_REG_BASE		0x40004100
N#define	HCL_REG_BASE			0x40004200
N
N#define PMU_REG_BASE			0x40008000
N#define ADC_REG_BASE			0x40008010
N
N#define MEMCTL_REG_BASE		0x40009000
N
N#define DMA_REG_BASE			0x4000a000
N
N#define I2C0_REG_BASE			0x4000b000
N#define I2C1_REG_BASE			0x4000c000
N
N#define UART0_REG_BASE		0x4000d000
N#define UART1_REG_BASE		0x4000e000
N#define UART2_REG_BASE		0x4000f000
N
N#define SPI0_REG_BASE			0x40011000
N#define SPI1_REG_BASE			0x40012000
N#define SPI2_REG_BASE			0x40013000
N
N#define GPIO_REG_BASE			0x40016000
N
N#define PWM_REG_BASE			0x40017000
N
N#define KEY_REG_BASE			0x40018000
N
N
N#endif /* _ACTIONS_SOC_REGS_H_	*/
L 18 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_clock.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_clock.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file peripheral clock configuration macros for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_CLOCK_H_
N#define	_ACTIONS_SOC_CLOCK_H_
N
N#include "soc_regs.h"
N
N#define	CLOCK_ID_DMA			0
N#define	CLOCK_ID_SPICACHE		1
N#define	CLOCK_ID_SPI0			2
N#define	CLOCK_ID_SPI1			3
N#define	CLOCK_ID_SPI2			4
N#define	CLOCK_ID_UART0			5
N#define	CLOCK_ID_UART1			6
N#define	CLOCK_ID_UART2			7
N#define	CLOCK_ID_I2C0			8
N#define	CLOCK_ID_I2C1			9
N#define	CLOCK_ID_PWM0			10
N#define	CLOCK_ID_PWM1			11
N#define	CLOCK_ID_PWM2			12
N#define	CLOCK_ID_PWM3			13
N#define	CLOCK_ID_PWM4			14
N#define	CLOCK_ID_KEY			15
N#define	CLOCK_ID_ADC			16
N#define	CLOCK_ID_I2STX			17
N#define	CLOCK_ID_I2SRX			18
N#define	CLOCK_ID_LRADC			19
N#define	CLOCK_ID_TIMER0			20
N#define	CLOCK_ID_TIMER1			21
N#define	CLOCK_ID_TIMER2			22
N#define	CLOCK_ID_TIMER3			23
N#define	CLOCK_ID_BLE_HCLK_EN		24
N#define	CLOCK_ID_BLE_HCLK_GATING	25
N#define	CLOCK_ID_BLE_LLCC_CLK_REQ_O	26
N#define	CLOCK_ID_HCL4HZ		27
N#define	CLOCK_ID_HCL32K		28
N#define	CLOCK_ID_EFUSE		29
N#define	CLOCK_ID_IRC_RXCLK		30
N#define	CLOCK_ID_IRC_CLK		31
N
N#define	CLOCK_ID_MAX_ID			31
N
N#define     ACT_32M_CTL                                                       (CMU_DIGITAL_BASE+0x0004)
N#define     ACT_3M_CTL                                                        (CMU_DIGITAL_BASE+0x0008)
N#define     CMU_SYSCLK                                                        (CMU_DIGITAL_BASE+0x000C)
N
N#define     CMU_DEVRST                                                        (CMU_DIGITAL_BASE+0x0010)
N#define     CMU_DEVCLKEN                                                      (CMU_DIGITAL_BASE+0x0014)
N
N#define     CMU_TIMER0CLK                                                        (CMU_DIGITAL_BASE+0x003C)
N#define     CMU_TIMER1CLK                                                        (CMU_DIGITAL_BASE+0x0040)
N
N#define     ACT_32M_CTL_XTAL32M_EN                                            0
N
N#define     ACT_3M_CTL_RC3M_OK                                                5
N#define     ACT_3M_CTL_RSEL_E                                                 4
N#define     ACT_3M_CTL_RSEL_SHIFT                                             1
N#define     ACT_3M_CTL_RSEL_MASK                                              (0xF<<1)
N#define     ACT_3M_CTL_RC3MEN                                                 0
N
N#define     CMU_SYSCLK_BLE_HCLK_DIV_E                                         17
N#define     CMU_SYSCLK_BLE_HCLK_DIV_SHIFT                                     16
N#define     CMU_SYSCLK_BLE_HCLK_DIV_MASK                                      (0x3<<16)
N#define     CMU_SYSCLK_BLE_HCLK_SEL                                           15
N#define     CMU_SYSCLK_HOSC_A                                                 13
N#define     CMU_SYSCLK_LOSC_A                                                 12
N#define     CMU_SYSCLK_APB_DIV_E                                              11
N#define     CMU_SYSCLK_APB_DIV_SHIFT                                          10
N#define     CMU_SYSCLK_APB_DIV_MASK                                           (0x3<<10)
N#define     CMU_SYSCLK_AHB_DIV                                                9
N#define     CMU_SYSCLK_CPU_COREPLL                                            8
N#define     CMU_SYSCLK_CPU_32M                                                7
N#define     CMU_SYSCLK_CPU_32K                                                6
N#define     CMU_SYSCLK_CPU_3M                                                 5
N#define     CMU_SYSCLK_SLEEP_HFCLK_SEL                                        4
N#define     CMU_SYSCLK_PCLKG                                                  2
N#define     CMU_SYSCLK_CPU_CLK_SEL_E                                          1
N#define     CMU_SYSCLK_CPU_CLK_SEL_SHIFT                                      0
N#define     CMU_SYSCLK_CPU_CLK_SEL_MASK                                       (0x3<<0)
N
N#ifndef _ASMLANGUAGE
N
Nvoid acts_clock_peripheral_enable(int clock_id);
Nvoid acts_clock_peripheral_disable(int clock_id);
N
Nvoid acts_request_rc_3M(bool ena);
Xvoid acts_request_rc_3M(_Bool ena);
Nvoid acts_request_pclk_gating(bool ena);
Xvoid acts_request_pclk_gating(_Bool ena);
N
N#endif /* _ASMLANGUAGE */
N
N#endif /* _ACTIONS_SOC_CLOCK_H_	*/
L 19 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_reset.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_reset.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file peripheral reset configuration macros for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_RESET_H_
N#define	_ACTIONS_SOC_RESET_H_
N
N#define	RESET_ID_DMA			0
N#define	RESET_ID_SPICACHE		1
N#define	RESET_ID_SPI0			2
N#define	RESET_ID_SPI1			3
N#define	RESET_ID_SPI2			4
N#define	RESET_ID_UART0			5
N#define	RESET_ID_UART1			6
N#define	RESET_ID_UART2			7
N#define	RESET_ID_I2C0			8
N#define	RESET_ID_I2C1			9
N#define	RESET_ID_PWM			10
N#define	RESET_ID_KEY			15
N#define	RESET_ID_AUDIO			16
N#define	RESET_ID_HRESET			24
N#define	RESET_ID_BLE			25
N#define	RESET_ID_LLCC			26
N#define	RESET_ID_BIST			30
N#define	RESET_ID_IRC			31
N
N#define	RESET_ID_MAX_ID			31
N
N#ifndef _ASMLANGUAGE
N
Nvoid acts_reset_peripheral_assert(int reset_id);
Nvoid acts_reset_peripheral_deassert(int reset_id);
Nvoid acts_reset_peripheral(int reset_id);
N
N#endif /* _ASMLANGUAGE */
N
N#endif /* _ACTIONS_SOC_RESET_H_	*/
L 20 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_clk_32k.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_clk_32k.h" 1
N/*
N * Copyright (c) 2017 Actions Semiconductor Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file 32k clk configuration macros for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_CLK_32K_H_
N#define	_ACTIONS_SOC_CLK_32K_H_
N
N#include "soc_regs.h"
N
N#define		BLE_32K_CTL								(CMU_DIGITAL_BASE+0x0000)
N#define		BLE_32K_CTL_CKO32K_OEN			2
N#define		BLE_32K_CTL_XTAL32K_ON			1
N#define		BLE_32K_CTL_XTAL32K_REQ		0
N
N/*--------------REGISTER ADDRESS----------------------*/
N#define		HCL32K_CTL										(HCL_REG_BASE+0x0000)
N#define		HCL32K_DATA										(HCL_REG_BASE+0x0004)
N
N/*--------------BITS LOCATION--------------------------*/
N#define		HCL32K_CTL_HCL_UPDATA								13
N#define		HCL32K_CTL_HCL_MS										12
N#define		HCL32K_CTL_CAL_DELAY_TIME_BPS				11
N#define		HCL32K_CTL_HCL_INTERVAL_E						10
N#define		HCL32K_CTL_HCL_INTERVAL_SHIFT				8
N#define		HCL32K_CTL_HCL_INTERVAL_MASK				(0x7<<8)
N#define		HCL32K_CTL_HOSC_OSC_TIME_E					7
N#define		HCL32K_CTL_HOSC_OSC_TIME_SHIFT			6
N#define		HCL32K_CTL_HOSC_OSC_TIME_MASK				(0x3<<6)
N#define		HCL32K_CTL_CAL_DELAY_TIME_E					5
N#define		HCL32K_CTL_CAL_DELAY_TIME_SHIFT			4
N#define		HCL32K_CTL_CAL_DELAY_TIME_MASK			(0x3<<4)
N#define		HCL32K_CTL_HCL_32K_EN								0
N
N#define		HCL32K_DATA_DATA_E									16
N#define		HCL32K_DATA_DATA_SHIFT							0
N#define		HCL32K_DATA_DATA_MASK								(0x1FFFF<<0)
N
N
N#ifndef _ASMLANGUAGE
N
Ntypedef enum {
N	HCL_NORMAL = 0x0,
N	HCL_AUTO = 0x1,
N} hcl_mode_t;
N
Nvoid acts_set_hcl_32k(hcl_mode_t mode);
Nvoid acts_set_xtal_32k(void);
Nvoid acts_wait_xtal_32k_on(void);
N
N#endif /* _ASMLANGUAGE */
N
N#endif /* _ACTIONS_SOC_CLK_32K_H_	*/
L 21 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_irq.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_irq.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file IRQ configuration macros for Actions SoC
N */
N
N#ifndef _ACTIONS_SOC_IRQ_H_
N#define _ACTIONS_SOC_IRQ_H_
N
N#define IRQ_ID_TIMER0		0
N#define IRQ_ID_TIMER1		1
N#define IRQ_ID_RTC		2
N#define IRQ_ID_WATCHDOG		3
N#define IRQ_ID_BLE0		4
N#define IRQ_ID_BLE1		5
N#define IRQ_ID_BLE2		6
N#define IRQ_ID_BLE3		7
N#define IRQ_ID_BLE4		8
N#define IRQ_ID_BLE5		9
N#define IRQ_ID_BLE6		10
N#define IRQ_ID_BLE7		11
N#define IRQ_ID_BLE8		12
N#define IRQ_ID_DMA		13
N#define IRQ_ID_GPIO		14
N#define IRQ_ID_UART0		15
N#define IRQ_ID_UART1		16
N#define IRQ_ID_UART2		17
N#define IRQ_ID_SPI0		18
N#define IRQ_ID_SPI1		19
N#define IRQ_ID_SPI2		20
N#define IRQ_ID_SARADC		21
N#define IRQ_ID_UART_WAKEUP	22
N#define IRQ_ID_IRC		23
N#define IRQ_ID_I2C0		24
N#define IRQ_ID_I2C1		25
N#define IRQ_ID_AUDIO		26
N#define IRQ_ID_KEY		27
N#define IRQ_ID_SARADC_WAKEUP	28
N#define IRQ_ID_ID_TIMER2	29
N#define IRQ_ID_ID_TIMER3	30
N#define IRQ_ID_KEY_WAKEUP	31
N
N#endif	/* _ACTIONS_SOC_IRQ_H_ */
L 22 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_gpio.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_gpio.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file GPIO/PINMUX configuration macros for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_GPIO_H_
N#define	_ACTIONS_SOC_GPIO_H_
N
N#define GPIO_MAX_PIN_NUM			31
N
N#define JTAG_EN         (GPIO_REG_BASE+0x0)
N
N#define GPIO_CTL0				0x4
N#define GPIO_ODAT0				0x100
N#define GPIO_BSR0				0x108
N#define GPIO_BRR0				0x110
N#define GPIO_IDAT0				0x118
N#define GPIO_IRQ_PD0				0x120
N
N#define GPIO_REG_CTL(base, pin)			((base) + GPIO_CTL0 + (pin) * 4)
N#define GPIO_REG_ODAT(base, pin)		((base) + GPIO_ODAT0 + (pin) / 32 * 4)
N#define GPIO_REG_IDAT(base, pin)		((base) + GPIO_IDAT0 + (pin) / 32 * 4)
N#define GPIO_REG_BSR(base, pin)			((base) + GPIO_BSR0 + (pin) / 32 * 4)
N#define GPIO_REG_BRR(base, pin)			((base) + GPIO_BRR0 + (pin) / 32 * 4)
N#define GPIO_REG_IRQ_PD(base, pin)		((base) + GPIO_IRQ_PD0 + (pin) / 32 * 4)
N#define GPIO_BIT(pin)				(1 << ((pin) % 32))
N
N#define GPIO_CTL_MFP_SHIFT			(0)
N#define GPIO_CTL_MFP_MASK			(0x1f << GPIO_CTL_MFP_SHIFT)
N#define GPIO_CTL_MFP_GPIO			(0x0 << GPIO_CTL_MFP_SHIFT)
N#define GPIO_CTL_GPIO_OUTEN			(0x1 << 6)
N#define GPIO_CTL_GPIO_INEN			(0x1 << 7)
N#define GPIO_CTL_PULL_MASK			(0x7 << 8)
N#define GPIO_CTL_PULLUP				(0x1 << 8)
N#define GPIO_CTL_PULLDOWN			(0x1 << 9)
N#define GPIO_CTL_PULLUP_STRONG			(0x1 << 10)
N#define GPIO_CTL_SMIT				(0x1 << 11)
N#define GPIO_CTL_PADDRV_SHIFT			(12)
N#define GPIO_CTL_PADDRV_LEVEL(x)		((x) << GPIO_CTL_PADDRV_SHIFT)
N#define GPIO_CTL_PADDRV_MASK			GPIO_CTL_PADDRV_LEVEL(0x3)
N#define GPIO_CTL_INTC_EN			(0x1 << 24)
N#define GPIO_CTL_INC_TRIGGER_SHIFT		(25)
N#define GPIO_CTL_INC_TRIGGER(x)			((x) << GPIO_CTL_INC_TRIGGER_SHIFT)
N#define GPIO_CTL_INC_TRIGGER_MASK		GPIO_CTL_INC_TRIGGER(0x7)
N#define GPIO_CTL_INC_TRIGGER_HIGH_LEVEL		GPIO_CTL_INC_TRIGGER(0x0)
N#define GPIO_CTL_INC_TRIGGER_LOW_LEVEL		GPIO_CTL_INC_TRIGGER(0x1)
N#define GPIO_CTL_INC_TRIGGER_RISING_EDGE	GPIO_CTL_INC_TRIGGER(0x2)
N#define GPIO_CTL_INC_TRIGGER_FALLING_EDGE	GPIO_CTL_INC_TRIGGER(0x3)
N#define GPIO_CTL_INC_TRIGGER_DUAL_EDGE		GPIO_CTL_INC_TRIGGER(0x4)
N#define GPIO_CTL_INTC_MASK			(0x1 << 31)
N
N#define PINMUX_MODE_MASK			(GPIO_CTL_MFP_MASK | GPIO_CTL_PULL_MASK | GPIO_CTL_SMIT | GPIO_CTL_PADDRV_MASK)
N
N#endif /* _ACTIONS_SOC_GPIO_H_	*/
L 23 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_pinmux.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_pinmux.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file PINMUX interface for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_PINMUX_H_
N#define	_ACTIONS_SOC_PINMUX_H_
N
N#ifndef _ASMLANGUAGE
N
Nstruct acts_pin_config {
N	unsigned int pin_num;
N	unsigned int mode;
N};
N
Nint acts_pinmux_set(unsigned int pin, unsigned int mode);
Nint acts_pinmux_get(unsigned int pin, unsigned int *mode);
Nvoid acts_pinmux_setup_pins(const struct acts_pin_config *pinconf, int pins);
N
N#endif /* !_ASMLANGUAGE */
N
N#endif /* _ACTIONS_SOC_PINMUX_H_	*/
L 24 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_dma.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_dma.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file dma configuration macros for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_DMA_H_
N#define	_ACTIONS_SOC_DMA_H_
N
N#define	DMA_ID_MEMORY			0
N#define	DMA_ID_SPI0			1
N#define	DMA_ID_SPI1			2
N#define	DMA_ID_SPI2			3
N#define	DMA_ID_UART0			4
N#define	DMA_ID_UART1			5
N#define	DMA_ID_UART2			6
N#define	DMA_ID_I2C0			7
N#define	DMA_ID_I2C1			8
N#define	DMA_ID_I2S			9
N#define	DMA_ID_PWM			10
N
N#define	DMA_ID_MAX_ID			10
N
N#define DMA_GLOB_OFFS			0x0000
N#define DMA_CHAN_OFFS			0x0100
N
N#define DMA_CHAN(base, id)		((u32_t)(base) + DMA_CHAN_OFFS + (id) * 0x100)
N#define DMA_GLOBAL(base)		((base) + DMA_GLOB_OFFS)
N
N#define DMA_ACTS_MAX_CHANNELS	4	/* Number of streams per controller */
N
N/* Maximum data sent in single transfer (Bytes) */
N#define DMA_ACTS_MAX_DATA_ITEMS		0x1fff
N
N#ifndef _ASMLANGUAGE
N
N#endif /* _ASMLANGUAGE */
N
N#endif /* _ACTIONS_SOC_DMA_H_	*/
L 25 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_adc.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_adc.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file ADC configuration macros for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_ADC_H_
N#define	_ACTIONS_SOC_ADC_H_
N
N#define	ADC_ID_BATV			0
N#define	ADC_ID_TEMP			1
N#define	ADC_ID_CH0			2
N#define	ADC_ID_CH1			3
N#define	ADC_ID_CH2			4
N#define	ADC_ID_CH3			5
N#define	ADC_ID_CH4			6
N#define	ADC_ID_CH5			7
N
N#define	ADC_ID_MAX_ID			7
N
N#ifndef _ASMLANGUAGE
N
N#endif /* _ASMLANGUAGE */
N
N#endif /* _ACTIONS_SOC_ADC_H_	*/
L 26 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_pmu.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_pmu.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file pmu configuration macros for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_PMU_H_
N#define	_ACTIONS_SOC_PMU_H_
N
N#include "soc_regs.h"
N
N#define     VD12_CTL                                                          (PMU_REG_BASE+0x00)
N#define     VDD_CTL                                                           (PMU_REG_BASE+0x04)
N#define     RAM_CTL                                                           (PMU_REG_BASE+0x044)
N#define     BLE_CTL                                                           (PMU_REG_BASE+0x048)
N
N#define     VD12_CTL_VD12PD_EN                                                14
N#define     VD12_CTL_VD12PD_SET_E                                             13
N#define     VD12_CTL_VD12PD_SET_SHIFT                                         12
N#define     VD12_CTL_VD12PD_SET_MASK                                          (0x3<<12)
N#define     VD12_CTL_VD12_BIAS_SET_E                                          10
N#define     VD12_CTL_VD12_BIAS_SET_SHIFT                                      9
N#define     VD12_CTL_VD12_BIAS_SET_MASK                                       (0x3<<9)
N#define     VD12_CTL_VD12_LGBIAS_EN                                           8
N#define     VD12_CTL_VD12_EN                                                  4
N#define     VD12_CTL_VD12_SET_E                                               2
N#define     VD12_CTL_VD12_SET_SHIFT                                           0
N#define     VD12_CTL_VD12_SET_MASK                                            (0x7<<0)
N
N#define     VDD_CTL_VDD_BIAS_S3_E                                             5
N#define     VDD_CTL_VDD_BIAS_S3_SHIFT                                         4
N#define     VDD_CTL_VDD_BIAS_S3_MASK                                          (0x3<<4)
N#define     VDD_CTL_VDD_SET_E                                                 3
N#define     VDD_CTL_VDD_SET_SHIFT                                             0
N#define     VDD_CTL_VDD_SET_MASK                                              (0xF<<0)
N
N#define     BLE_CTL_BLE_SWV1V_REQ                                             5
N#define     BLE_CTL_BAT_STAT_REQ                                              4
N#define     BLE_CTL_LLCC_CLK_REQ_WK_EN                                        2
N#define     BLE_CTL_BLE_SLEEP_REQ                                             1
N#define     BLE_CTL_BLE_WAKE_REQ                                              0
N
N#define     BLE_STATE_LLCC_CLK_O                                              4
N#define     BLE_STATE_BLE_SWV1V_O                                             3
N#define     BLE_STATE_BLE_BATSTAT_O                                           2
N#define     BLE_STATE_BLE_AWAKE_O                                             1
N#define     BLE_STATE_BLE_3V                                                  0
N
N#ifndef _ASMLANGUAGE
N
Nvoid acts_request_vd12_pd(bool ena);
Xvoid acts_request_vd12_pd(_Bool ena);
Nvoid acts_request_vd12_largebias(bool ena);
Xvoid acts_request_vd12_largebias(_Bool ena);
Nvoid acts_request_vd12(bool ena);
Xvoid acts_request_vd12(_Bool ena);
N
Ntypedef enum {
N	VDD_80 = 0x0,
N	VDD_85,
N	VDD_90,
N	VDD_95,
N	VDD_100,
N	VDD_105,
N	VDD_110,
N	VDD_115,
N	VDD_120,
N	VDD_125,
N	VDD_130,
N} vdd_val_t;
N
N
Nextern vdd_val_t normal_vdd_val;
Nextern vdd_val_t dp_vdd_val;
N
Nvoid acts_set_vdd(vdd_val_t val);
N
N#endif /* _ASMLANGUAGE */
N
N#endif /* _ACTIONS_SOC_PMU_H_	*/
L 27 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_efuse.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_efuse.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file efuse configuration macros for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_EFUSE_H_
N#define	_ACTIONS_SOC_EFUSE_H_
N
N#define     EFUSE_BASE                                                        0x40008000
N#define     PMU_DEBUG                                                         (EFUSE_BASE+0x7C)
N
N#define     PMU_DEBUG_DBGPWR_WK_EN                                            2
N#define     PMU_DEBUG_PMU_DEBUG_SEL_E                                         1
N#define     PMU_DEBUG_PMU_DEBUG_SEL_SHIFT                                     0
N#define     PMU_DEBUG_PMU_DEBUG_SEL_MASK                                      (0x3<<0)
N
N#ifndef _ASMLANGUAGE
N
Nuint32_t acts_efuse_read(uint8_t offset);
Nuint32_t acts_efuse_read_val(uint8_t offset);
Nvoid acts_set_vd12_before_efuse_read(void);
Nvoid acts_set_vd12_after_efuse_read(void);
N
N#endif /* _ASMLANGUAGE */
N
N#endif /* _ACTIONS_SOC_EFUSE_H_	*/
L 28 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_memctl.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_memctl.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file pmu configuration macros for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_MEM_CTL_H_
N#define	_ACTIONS_SOC_MEM_CTL_H_
N
N#include "soc_regs.h"
N
N#define   MEM_CTL               (MEMCTL_REG_BASE+0x00)
N#define   VECTOR_BASE           (MEMCTL_REG_BASE+0x04)
N
N#define   MEM_CTL_VECTOR_TABLE_SEL    (1 << 0)
N
N#ifndef _ASMLANGUAGE
N
N
N#endif /* _ASMLANGUAGE */
N
N#endif /* _ACTIONS_SOC_MEM_CTL_H_	*/
L 29 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_timer.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_timer.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file peripheral clock configuration macros for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_TIMER_H_
N#define	_ACTIONS_SOC_TIMER_H_
N
N#include "soc_regs.h"
N
N#define     T0_CTL                                                       (TIMER_REG_BASE+0x000)
N#define     T0_VAL                                                       (TIMER_REG_BASE+0x004)
N#define     T0_CNT                                                       (TIMER_REG_BASE+0x008)
N
N#define     T1_CTL                                                       (TIMER_REG_BASE+0x040)
N#define     T1_VAL                                                       (TIMER_REG_BASE+0x044)
N#define     T1_CNT                                                       (TIMER_REG_BASE+0x048)
N
N#ifndef _ASMLANGUAGE
N
N
N
N#endif /* _ASMLANGUAGE */
N
N#endif /* _ACTIONS_SOC_TIMER_H_	*/
L 30 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_uart.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_uart.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file ADC configuration macros for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_UART_H_
N#define	_ACTIONS_SOC_UART_H_
N
N#include "soc_regs.h"
N
N#define     UART0_CTL                                                       (UART0_REG_BASE+0x000)
N#define     UART0_RXDAT                                                     (UART0_REG_BASE+0x004)
N#define     UART0_TXDAT                                                     (UART0_REG_BASE+0x008)
N#define     UART0_STA                                                       (UART0_REG_BASE+0x00C)
N#define     UART0_BR                                                        (UART0_REG_BASE+0x010)
N
N#define     UART1_CTL                                                       (UART1_REG_BASE+0x000)
N#define     UART1_RXDAT                                                     (UART1_REG_BASE+0x004)
N#define     UART1_TXDAT                                                     (UART1_REG_BASE+0x008)
N#define     UART1_STA                                                       (UART1_REG_BASE+0x00C)
N#define     UART1_BR                                                        (UART1_REG_BASE+0x010)
N
N#define     UART2_CTL                                                       (UART2_REG_BASE+0x000)
N#define     UART2_RXDAT                                                     (UART2_REG_BASE+0x004)
N#define     UART2_TXDAT                                                     (UART2_REG_BASE+0x008)
N#define     UART2_STA                                                       (UART2_REG_BASE+0x00C)
N#define     UART2_BR                                                        (UART2_REG_BASE+0x010)
N
N#ifndef _ASMLANGUAGE
N
N#endif /* _ASMLANGUAGE */
N
N#endif /* _ACTIONS_SOC_UART_H_	*/
L 31 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_pm.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_pm.h" 1
N/*
N * Copyright (c) 2017 Actions Semiconductor Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file pmu configuration macros for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_PM_H_
N#define	_ACTIONS_SOC_PM_H_
N
N#include "soc_regs.h"
N
N#ifndef _ASMLANGUAGE
N
Nextern int (*p_sys_soc_suspend)(s32_t);
N
Nextern u32_t Image$$RAM_RETENTION$$Base[];
Nextern u32_t Image$$RAM_RETENTION$$Limit[];
N
Nextern u32_t application_wake_lock;
N
Nvoid app_get_wake_lock(void);
Nvoid app_release_wake_lock(void);
N
Nint _sys_soc_suspend(s32_t ticks);
N
Nvoid idle(void *unused1, void *unused2, void *unused3);
N
Nvoid _timer_deepsleep_enter(s32_t ticks);
Nvoid _timer_deepsleep_exit(void);
N
N
N#endif /* _ASMLANGUAGE */
N
N#endif /* _ACTIONS_SOC_PM_H_	*/
L 32 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N#include "soc_rtc.h"
L 1 "..\..\..\..\arch\soc\atb110x\soc_rtc.h" 1
N/*
N * Copyright (c) 2018 Actions (Zhuhai) Technology Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file peripheral clock configuration macros for Actions SoC
N */
N
N#ifndef	_ACTIONS_SOC_RTC_H_
N#define	_ACTIONS_SOC_RTC_H_
N
N#include "soc_regs.h"
N
N#define     WD_CTL				(RTC_REG_BASE+0x001c)
N#define     RTC_BAK0			(RTC_REG_BASE+0x0030)
N#define     RTC_BAK1			(RTC_REG_BASE+0x0034)
N#define     RTC_BAK2			(RTC_REG_BASE+0x0038)
N#define     RTC_BAK3			(RTC_REG_BASE+0x003c)
N
N#ifndef _ASMLANGUAGE
N
N
N#endif /* _ASMLANGUAGE */
N
N#endif /* _ACTIONS_SOC_RTC_H_	*/
L 33 "..\..\..\..\arch\soc\atb110x\soc.h" 2
N
N#ifndef _ASMLANGUAGE
N
N#define SYSTEM_CLOCK_32M (32000000UL)
N#define SYSTEM_CLOCK_3M (3000000UL)
N#define SYSTEM_CLOCK_32K (32000UL)
N
N#define REBOOT_REASON_ADDR		0x40004038  /* RTC BAK2 & RTC BAK3 */
N#define REBOOT_REASON_MAGIC		0x544f4252	/* 'RBOT' */
N
N#define REBOOT_TYPE_GOTO_ADFU 0x1000
N#define REBOOT_TYPE_GOTO_DTM  0x1200
N#define REBOOT_TYPE_GOTO_OTA	0x1300
N#define REBOOT_TYPE_GOTO_APP  0x1400
N
Nextern u32_t system_core_clock;
N
Nvoid sys_pm_reboot(int type);
Nvoid acts_delay_us(uint32_t us);
Nvoid debug_uart_put_char(char data);
N
N#endif /* !_ASMLANGUAGE */
N
N
N
N#endif /* _ACTIONS_SOC_H_ */
L 10 "..\..\..\..\drivers\uart\uart_acts.c" 2
N#include "uart_acts.h"
L 1 "..\..\..\..\drivers\uart\uart_acts.h" 1
N/*
N * Copyright (c) 2017 Actions Semiconductor Co., Ltd
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief common code for GPIO
N */
N
N#ifndef __UART_ACTS_H__
N#define __UART_ACTS_H__
N
N#include <zephyr/types.h>
N#include <uart.h>
L 1 "..\..\..\..\include\uart.h" 1
N/*
N * Copyright (c) 2015 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/**
N * @file
N * @brief Public APIs for UART drivers
N */
N
N#ifndef __INCuarth
N#define __INCuarth
N
N/**
N * @brief UART Interface
N * @defgroup uart_interface UART Interface
N * @ingroup io_interfaces
N * @{
N */
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N#include "errno.h"
L 1 "..\..\..\..\lib\libc\minimal\include\errno.h" 1
N/* errno.h - errno numbers */
N
N/*
N * Copyright (c) 1984-1999, 2012 Wind River Systems, Inc.
N *
N * SPDX-License-Identifier: Apache-2.0
N */
N
N/*
N * Copyright (c) 1982, 1986 Regents of the University of California.
N * All rights reserved.  The Berkeley software License Agreement
N * specifies the terms and conditions for redistribution.
N *
N *	@(#)errno.h	7.1 (Berkeley) 6/4/86
N */
N
N#ifndef __INCerrnoh
N#define __INCerrnoh
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N
Nextern int *__errno(void);
N#define errno (*__errno())
N
N/*
N * POSIX Error codes
N */
N
N#define EPERM 1		/* Not owner */
N#define ENOENT 2	/* No such file or directory */
N#define ESRCH 3		/* No such context */
N#define EINTR 4		/* Interrupted system call */
N#define EIO 5		/* I/O error */
N#define ENXIO 6		/* No such device or address */
N#define E2BIG 7		/* Arg list too long */
N#define ENOEXEC 8       /* Exec format error */
N#define EBADF 9		/* Bad file number */
N#define ECHILD 10       /* No children */
N#define EAGAIN 11       /* No more contexts */
N#define ENOMEM 12       /* Not enough core */
N#define EACCES 13       /* Permission denied */
N#define EFAULT 14       /* Bad address */
N#define ENOTEMPTY 15    /* Directory not empty */
N#define EBUSY 16	/* Mount device busy */
N#define EEXIST 17       /* File exists */
N#define EXDEV 18	/* Cross-device link */
N#define ENODEV 19       /* No such device */
N#define ENOTDIR 20      /* Not a directory */
N#define EISDIR 21       /* Is a directory */
N#define EINVAL 22       /* Invalid argument */
N#define ENFILE 23       /* File table overflow */
N#define EMFILE 24       /* Too many open files */
N#define ENOTTY 25       /* Not a typewriter */
N#define ENAMETOOLONG 26 /* File name too long */
N#define EFBIG 27	/* File too large */
N#define ENOSPC 28       /* No space left on device */
N#define ESPIPE 29       /* Illegal seek */
N#define EROFS 30	/* Read-only file system */
N#define EMLINK 31       /* Too many links */
N#define EPIPE 32	/* Broken pipe */
N#define EDEADLK 33      /* Resource deadlock avoided */
N#define ENOLCK 34       /* No locks available */
N#define ENOTSUP 35      /* Unsupported value */
N#define EMSGSIZE 36     /* Message size */
N
N/* ANSI math software */
N#define EDOM 37   /* Argument too large */
N#define ERANGE 38 /* Result too large */
N
N/* ipc/network software */
N
N/* argument errors */
N#define EDESTADDRREQ 40    /* Destination address required */
N#define EPROTOTYPE 41      /* Protocol wrong type for socket */
N#define ENOPROTOOPT 42     /* Protocol not available */
N#define EPROTONOSUPPORT 43 /* Protocol not supported */
N#define ESOCKTNOSUPPORT 44 /* Socket type not supported */
N#define EOPNOTSUPP 45      /* Operation not supported on socket */
N#define EPFNOSUPPORT 46    /* Protocol family not supported */
N#define EAFNOSUPPORT 47    /* Addr family not supported */
N#define EADDRINUSE 48      /* Address already in use */
N#define EADDRNOTAVAIL 49   /* Can't assign requested address */
N#define ENOTSOCK 50	/* Socket operation on non-socket */
N
N/* operational errors */
N#define ENETUNREACH 51  /* Network is unreachable */
N#define ENETRESET 52    /* Network dropped connection on reset */
N#define ECONNABORTED 53 /* Software caused connection abort */
N#define ECONNRESET 54   /* Connection reset by peer */
N#define ENOBUFS 55      /* No buffer space available */
N#define EISCONN 56      /* Socket is already connected */
N#define ENOTCONN 57     /* Socket is not connected */
N#define ESHUTDOWN 58    /* Can't send after socket shutdown */
N#define ETOOMANYREFS 59 /* Too many references: can't splice */
N#define ETIMEDOUT 60    /* Connection timed out */
N#define ECONNREFUSED 61 /* Connection refused */
N#define ENETDOWN 62     /* Network is down */
N#define ETXTBSY 63      /* Text file busy */
N#define ELOOP 64	/* Too many levels of symbolic links */
N#define EHOSTUNREACH 65 /* No route to host */
N#define ENOTBLK 66      /* Block device required */
N#define EHOSTDOWN 67    /* Host is down */
N
N/* non-blocking and interrupt i/o */
N#define EINPROGRESS 68 /* Operation now in progress */
N#define EALREADY 69    /* Operation already in progress */
N#define EWOULDBLOCK EAGAIN /* Operation would block */
N
N#define ENOSYS 71 /* Function not implemented */
N
N/* aio errors (should be under posix) */
N#define ECANCELED 72 /* Operation canceled */
N
N#define ERRMAX 81
N
N/* specific STREAMS errno values */
N
N#define ENOSR 74   /* Insufficient memory */
N#define ENOSTR 75  /* STREAMS device required */
N#define EPROTO 76  /* Generic STREAMS error */
N#define EBADMSG 77 /* Invalid STREAMS message */
N#define ENODATA 78 /* Missing expected message data */
N#define ETIME 79   /* STREAMS timeout occurred */
N#define ENOMSG 80  /* Unexpected message type */
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif /* __INCerrnoh */
L 27 "..\..\..\..\include\uart.h" 2
N#include <stddef.h>
N
N#include <device.h>
N
N#ifdef CONFIG_PCI
S#include <drivers/pci/pci.h>
S#include <drivers/pci/pci_mgr.h>
N#endif
N/**
N * @brief Options for @a UART initialization.
N */
N#define UART_OPTION_AFCE 0x01
N
N/** Common line controls for UART.*/
N#define LINE_CTRL_BAUD_RATE	(1 << 0)
N#define LINE_CTRL_RTS		(1 << 1)
N#define LINE_CTRL_DTR		(1 << 2)
N#define LINE_CTRL_DCD		(1 << 3)
N#define LINE_CTRL_DSR		(1 << 4)
N
N/* Common communication errors for UART.*/
N
N/** @brief Overrun error */
N#define UART_ERROR_OVERRUN  (1 << 0)
N
N/** @brief Parity error */
N#define UART_ERROR_PARITY   (1 << 1)
N
N/** @brief Framing error */
N#define UART_ERROR_FRAMING  (1 << 2)
N
N/**
N * @brief Break interrupt error:
N *
N * A break interrupt was received. This happens when the serial input is
N * held at a logic '0' state for longer than the sum of start time + data bits
N * + parity + stop bits.
N */
N#define UART_ERROR_BREAK    (1 << 3)
N
N/**
N * @typedef uart_irq_callback_t
N * @brief Define the application callback function signature for UART.
N *
N * @param port Device struct for the UART device.
N */
Ntypedef void (*uart_irq_callback_t)(struct device *port);
N
N/**
N * @typedef uart_irq_config_func_t
N * @brief For configuring IRQ on each individual UART device.
N *
N * @internal
N */
Ntypedef void (*uart_irq_config_func_t)(struct device *port);
N
N/**
N * @brief UART device configuration.
N *
N * @param port Base port number
N * @param base Memory mapped base address
N * @param regs Register address
N * @param sys_clk_freq System clock frequency in Hz
N */
Nstruct uart_device_config {
N	union {
N		u32_t port;
N		u8_t *base;
N		u32_t regs;
N	};
N
N	u32_t sys_clk_freq;
N
N#ifdef CONFIG_PCI
S	struct pci_dev_info  pci_dev;
N#endif /* CONFIG_PCI */
N
N	uart_irq_config_func_t	irq_config_func;
N};
N
N/** @brief Driver API structure. */
Nstruct uart_driver_api {
N	/** Console I/O function */
N	int (*poll_in)(struct device *dev, unsigned char *p_char);
N	unsigned char (*poll_out)(struct device *dev, unsigned char out_char);
N
N	/** Console I/O function */
N	int (*err_check)(struct device *dev);
N
N	/** Interrupt driven FIFO fill function */
N	int (*fifo_fill)(struct device *dev, const u8_t *tx_data, int len);
N
N	/** Interrupt driven FIFO read function */
N	int (*fifo_read)(struct device *dev, u8_t *rx_data, const int size);
N
N	/** Interrupt driven transfer enabling function */
N	void (*irq_tx_enable)(struct device *dev);
N
N	/** Interrupt driven transfer disabling function */
N	void (*irq_tx_disable)(struct device *dev);
N
N	/** Interrupt driven transfer ready function */
N	int (*irq_tx_ready)(struct device *dev);
N
N	/** Interrupt driven receiver enabling function */
N	void (*irq_rx_enable)(struct device *dev);
N
N	/** Interrupt driven receiver disabling function */
N	void (*irq_rx_disable)(struct device *dev);
N
N	/** Interrupt driven transfer complete function */
N	int (*irq_tx_complete)(struct device *dev);
N
N	/** Interrupt driven receiver ready function */
N	int (*irq_rx_ready)(struct device *dev);
N
N	/** Interrupt driven error enabling function */
N	void (*irq_err_enable)(struct device *dev);
N
N	/** Interrupt driven error disabling function */
N	void (*irq_err_disable)(struct device *dev);
N
N	/** Interrupt driven pending status function */
N	int (*irq_is_pending)(struct device *dev);
N
N	/** Interrupt driven interrupt update function */
N	int (*irq_update)(struct device *dev);
N
N	/** Set the callback function */
N	void (*irq_callback_set)(struct device *dev, uart_irq_callback_t cb);
N
N#ifdef CONFIG_UART_LINE_CTRL
S	int (*line_ctrl_set)(struct device *dev, u32_t ctrl, u32_t val);
S	int (*line_ctrl_get)(struct device *dev, u32_t ctrl, u32_t *val);
N#endif
N
N#ifdef CONFIG_UART_DRV_CMD
S	int (*drv_cmd)(struct device *dev, u32_t cmd, u32_t p);
N#endif
N
N};
N
N/**
N * @brief Check whether an error was detected.
N *
N * @param dev UART device structure.
N *
N * @retval UART_ERROR_OVERRUN if an overrun error was detected.
N * @retval UART_ERROR_PARITY if a parity error was detected.
N * @retval UART_ERROR_FRAMING if a framing error was detected.
N * @retval UART_ERROR_BREAK if a break error was detected.
N * @retval 0 Otherwise.
N */
Nstatic inline int uart_err_check(struct device *dev)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if (api->err_check) {
N		return api->err_check(dev);
N	}
N	return 0;
N}
N
N
N/**
N * @brief Poll the device for input.
N *
N * @param dev UART device structure.
N * @param p_char Pointer to character.
N *
N * @retval 0 If a character arrived.
N * @retval -1 If no character was available to read (i.e., the UART
N *            input buffer was empty).
N * @retval -ENOTSUP If the operation is not supported.
N */
Nstatic inline int uart_poll_in(struct device *dev, unsigned char *p_char)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	return api->poll_in(dev, p_char);
N}
N
N/**
N * @brief Output a character in polled mode.
N *
N * This routine checks if the transmitter is empty.
N * When the transmitter is empty, it writes a character to the data
N * register.
N *
N * To send a character when hardware flow control is enabled, the handshake
N * signal CTS must be asserted.
N *
N * @param dev UART device structure.
N * @param out_char Character to send.
N *
N * @retval char Sent character.
N */
Nstatic inline unsigned char uart_poll_out(struct device *dev,
N					  unsigned char out_char)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	return api->poll_out(dev, out_char);
N}
N
N
N
N/**
N * @brief Fill FIFO with data.
N *
N * @details This function is expected to be called from UART
N * interrupt handler (ISR), if uart_irq_tx_ready() returns true.
N * Result of calling this function not from an ISR is undefined
N * (hardware-dependent). Likewise, *not* calling this function
N * from an ISR if uart_irq_tx_ready() returns true may lead to
N * undefined behavior, e.g. infinite interrupt loops. It's
N * mandatory to test return value of this function, as different
N * hardware has different FIFO depth (oftentimes just 1).
N *
N * @param dev UART device structure.
N * @param tx_data Data to transmit.
N * @param size Number of bytes to send.
N *
N * @return Number of bytes sent.
N */
Nstatic inline int uart_fifo_fill(struct device *dev, const u8_t *tx_data,
N				 int size)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if (api->fifo_fill) {
N		return api->fifo_fill(dev, tx_data, size);
N	}
N
N	return 0;
N}
N
N/**
N * @brief Read data from FIFO.
N *
N * @details This function is expected to be called from UART
N * interrupt handler (ISR), if uart_irq_rx_ready() returns true.
N * Result of calling this function not from an ISR is undefined
N * (hardware-dependent). It's unspecified whether "RX ready"
N * condition as returned by uart_irq_rx_ready() is level- or
N * edge- triggered. That means that once uart_irq_rx_ready() is
N * detected, uart_fifo_read() must be called until it reads all
N * available data in the FIFO (i.e. until it returns less data
N * than was requested).
N *
N * @param dev UART device structure.
N * @param rx_data Data container.
N * @param size Container size.
N *
N * @return Number of bytes read.
N */
Nstatic inline int uart_fifo_read(struct device *dev, u8_t *rx_data,
N				 const int size)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if (api->fifo_read) {
N		return api->fifo_read(dev, rx_data, size);
N	}
N
N	return 0;
N}
N
N/**
N * @brief Enable TX interrupt in IER.
N *
N * @param dev UART device structure.
N *
N * @return N/A
N */
Nstatic inline void uart_irq_tx_enable(struct device *dev)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if (api->irq_tx_enable) {
N		api->irq_tx_enable(dev);
N	}
N}
N/**
N * @brief Disable TX interrupt in IER.
N *
N * @param dev UART device structure.
N *
N * @return N/A
N */
Nstatic inline void uart_irq_tx_disable(struct device *dev)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if (api->irq_tx_disable) {
N		api->irq_tx_disable(dev);
N	}
N}
N
N/**
N * @brief Check if UART TX buffer can accept a new char
N *
N * @details Check if UART TX buffer can accept at least one character
N * for transmission (i.e. uart_fifo_fill() will succeed and return
N * non-zero). This function must be called in a UART interrupt
N * handler, or its result is undefined. Before calling this function
N * in the interrupt handler, uart_irq_update() must be called once per
N * the handler invocation.
N *
N * @param dev UART device structure.
N *
N * @retval 1 If at least one char can be written to UART.
N * @retval 0 Otherwise.
N */
Nstatic inline int uart_irq_tx_ready(struct device *dev)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if (api->irq_tx_ready) {
N		return api->irq_tx_ready(dev);
N	}
N
N	return 0;
N}
N
N/**
N * @brief Enable RX interrupt in IER.
N *
N * @param dev UART device structure.
N *
N * @return N/A
N */
Nstatic inline void uart_irq_rx_enable(struct device *dev)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if (api->irq_rx_enable) {
N		api->irq_rx_enable(dev);
N	}
N}
N
N/**
N * @brief Disable RX interrupt in IER.
N *
N * @param dev UART device structure.
N *
N * @return N/A
N */
Nstatic inline void uart_irq_rx_disable(struct device *dev)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if (api->irq_rx_disable) {
N		api->irq_rx_disable(dev);
N	}
N}
N
N/**
N * @brief Check if UART TX block finished transmission
N *
N * @details Check if any outgoing data buffered in UART TX block was
N * fully transmitted and TX block is idle. When this condition is
N * true, UART device (or whole system) can be power off. Note that
N * this function is *not* useful to check if UART TX can accept more
N * data, use uart_irq_tx_ready() for that. This function must be called
N * in a UART interrupt handler, or its result is undefined. Before
N * calling this function in the interrupt handler, uart_irq_update()
N * must be called once per the handler invocation.
N *
N * @param dev UART device structure.
N *
N * @retval 1 If nothing remains to be transmitted.
N * @retval 0 Otherwise.
N */
Nstatic inline int uart_irq_tx_complete(struct device *dev)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if (api->irq_tx_complete) {
N		return api->irq_tx_complete(dev);
N	}
N
N	return 0;
N}
N
N/**
N * @deprecated This API is deprecated.
N */
Nstatic inline int __deprecated uart_irq_tx_empty(struct device *dev)
Xstatic inline int __attribute__((deprecated)) uart_irq_tx_empty(struct device *dev)
N{
N	return uart_irq_tx_complete(dev);
N}
N
N/**
N * @brief Check if UART RX buffer has a received char
N *
N * @details Check if UART RX buffer has at least one pending character
N * (i.e. uart_fifo_read() will succeed and return non-zero). This function
N * must be called in a UART interrupt handler, or its result is undefined.
N * Before calling this function in the interrupt handler, uart_irq_update()
N * must be called once per the handler invocation. It's unspecified whether
N * condition as returned by this function is level- or edge- triggered (i.e.
N * if this function returns true when RX FIFO is non-empty, or when a new
N * char was received since last call to it). See description of
N * uart_fifo_read() for implication of this.
N *
N * @param dev UART device structure.
N *
N * @retval 1 If a received char is ready.
N * @retval 0 Otherwise.
N */
Nstatic inline int uart_irq_rx_ready(struct device *dev)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if (api->irq_rx_ready) {
N		return api->irq_rx_ready(dev);
N	}
N
N	return 0;
N}
N/**
N * @brief Enable error interrupt in IER.
N *
N * @param dev UART device structure.
N *
N * @return N/A
N */
Nstatic inline void uart_irq_err_enable(struct device *dev)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if (api->irq_err_enable) {
N		api->irq_err_enable(dev);
N	}
N}
N
N/**
N * @brief Disable error interrupt in IER.
N *
N * @param dev UART device structure.
N *
N * @retval 1 If an IRQ is ready.
N * @retval 0 Otherwise.
N */
Nstatic inline void uart_irq_err_disable(struct device *dev)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if (api->irq_err_disable) {
N		api->irq_err_disable(dev);
N	}
N}
N
N/**
N * @brief Check if any IRQs is pending.
N *
N * @param dev UART device structure.
N *
N * @retval 1 If an IRQ is pending.
N * @retval 0 Otherwise.
N */
N
Nstatic inline int uart_irq_is_pending(struct device *dev)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if (api->irq_is_pending)	{
N		return api->irq_is_pending(dev);
N	}
N
N	return 0;
N}
N
N/**
N * @brief Update cached contents of IIR.
N *
N * @param dev UART device structure.
N *
N * @retval 1 Always.
N */
Nstatic inline int uart_irq_update(struct device *dev)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if (api->irq_update) {
N		return api->irq_update(dev);
N	}
N
N	return 0;
N}
N
N/**
N * @brief Set the IRQ callback function pointer.
N *
N * This sets up the callback for IRQ. When an IRQ is triggered,
N * the specified function will be called.
N *
N * @param dev UART device structure.
N * @param cb Pointer to the callback function.
N *
N * @return N/A
N */
Nstatic inline void uart_irq_callback_set(struct device *dev,
N					 uart_irq_callback_t cb)
N{
N	const struct uart_driver_api *api = dev->driver_api;
N
N	if ((api != NULL) && (api->irq_callback_set != NULL)) {
X	if ((api != 0) && (api->irq_callback_set != 0)) {
N		api->irq_callback_set(dev, cb);
N	}
N}
N
N
N#ifdef CONFIG_UART_LINE_CTRL
S
S/**
S * @brief Manipulate line control for UART.
S *
S * @param dev UART device structure.
S * @param ctrl The line control to manipulate.
S * @param val Value to set to the line control.
S *
S * @retval 0 If successful.
S * @retval failed Otherwise.
S */
Sstatic inline int uart_line_ctrl_set(struct device *dev,
S				     u32_t ctrl, u32_t val)
S{
S	const struct uart_driver_api *api = dev->driver_api;
S
S	if (api->line_ctrl_set) {
S		return api->line_ctrl_set(dev, ctrl, val);
S	}
S
S	return -ENOTSUP;
S}
S
S/**
S * @brief Retrieve line control for UART.
S *
S * @param dev UART device structure.
S * @param ctrl The line control to manipulate.
S * @param val Value to get for the line control.
S *
S * @retval 0 If successful.
S * @retval failed Otherwise.
S */
Sstatic inline int uart_line_ctrl_get(struct device *dev,
S				     u32_t ctrl, u32_t *val)
S{
S	const struct uart_driver_api *api = dev->driver_api;
S
S	if (api && api->line_ctrl_get) {
S		return api->line_ctrl_get(dev, ctrl, val);
S	}
S
S	return -ENOTSUP;
S}
S
N#endif /* CONFIG_UART_LINE_CTRL */
N
N#ifdef CONFIG_UART_DRV_CMD
S
S/**
S * @brief Send extra command to driver.
S *
S * Implementation and accepted commands are driver specific.
S * Refer to the drivers for more information.
S *
S * @param dev UART device structure.
S * @param cmd Command to driver.
S * @param p Parameter to the command.
S *
S * @retval 0 If successful.
S * @retval failed Otherwise.
S */
Sstatic inline int uart_drv_cmd(struct device *dev, u32_t cmd, u32_t p)
S{
S	const struct uart_driver_api *api = dev->driver_api;
S
S	if (api->drv_cmd) {
S		return api->drv_cmd(dev, cmd, p);
S	}
S
S	return -ENOTSUP;
S}
S
N#endif /* CONFIG_UART_DRV_CMD */
N
N#ifdef __cplusplus
S}
N#endif
N
N/**
N * @}
N */
N
N#endif /* __INCuarth */
L 17 "..\..\..\..\drivers\uart\uart_acts.h" 2
N
N#ifdef __cplusplus
Sextern "C" {
N#endif
N
N
N#define UART0_IRQn 15
N#define UART1_IRQn 16
N#define UART2_IRQn 17
N
N#define	UART0_BASE	0x4000D000
N#define	UART1_BASE	0x4000E000
N#define	UART2_BASE	0x4000F000
N
N#define UART_CTRL 0x0
N#define UART_STA 0xc
N#define UART_BAUDRATE	0x10
N
N#define UART_STA_UTBB 21
N
N/* Device data structure */
Nstruct uart_acts_dev_data_t {
N	u32_t baud_rate;	        /**< Baud rate */
N	u32_t clock_id;
N	u32_t reset_id;
N	u32_t ctl_reg_val;
N	u32_t baud_reg_val;
N
N	uart_irq_callback_t     cb;     /**< Callback function pointer */
N};
N
N#define DEV_DATA(dev) \
N	((struct uart_acts_dev_data_t * )(dev)->driver_data)
X#define DEV_DATA(dev) 	((struct uart_acts_dev_data_t * )(dev)->driver_data)
N
N#define DEV_CFG(dev) \
N	((const struct uart_device_config *)(dev)->config->config_info)
X#define DEV_CFG(dev) 	((const struct uart_device_config *)(dev)->config->config_info)
N		
Nint uart_acts_init(struct device *dev);
Nvoid uart_acts_isr(void *arg);
N
N#ifdef __cplusplus
S}
N#endif
N
N#endif	/* __UART_ACTS_H__ */
L 11 "..\..\..\..\drivers\uart\uart_acts.c" 2
N
Nint uart_acts_suspend_device(struct device *dev)
N{
N	struct uart_acts_dev_data_t *dev_data = DEV_DATA(dev);
X	struct uart_acts_dev_data_t *dev_data = ((struct uart_acts_dev_data_t * )(dev)->driver_data);
N
N	/* wait tx not busy
N	 * while((sys_read32(DEV_CFG(dev)->regs + UART_STA) & (0x1 << UART_STA_UTBB)) != 0)
N	 */
N
N	dev_data->ctl_reg_val = sys_read32(DEV_CFG(dev)->regs + UART_CTRL);
X	dev_data->ctl_reg_val = sys_read32(((const struct uart_device_config *)(dev)->config->config_info)->regs + 0x0);
N	dev_data->baud_reg_val = sys_read32(DEV_CFG(dev)->regs + UART_BAUDRATE);
X	dev_data->baud_reg_val = sys_read32(((const struct uart_device_config *)(dev)->config->config_info)->regs + 0x10);
N	acts_clock_peripheral_disable(dev_data->clock_id);
N
N	return 0;
N}
N
Nint uart_acts_resume_device_from_suspend(struct device *dev)
N{
N	struct uart_acts_dev_data_t *dev_data = DEV_DATA(dev);
X	struct uart_acts_dev_data_t *dev_data = ((struct uart_acts_dev_data_t * )(dev)->driver_data);
N
N	acts_reset_peripheral(dev_data->reset_id);
N	acts_clock_peripheral_enable(dev_data->clock_id);
N
N	sys_write32(dev_data->baud_reg_val, DEV_CFG(dev)->regs + UART_BAUDRATE);
X	sys_write32(dev_data->baud_reg_val, ((const struct uart_device_config *)(dev)->config->config_info)->regs + 0x10);
N	sys_write32(dev_data->ctl_reg_val, DEV_CFG(dev)->regs + UART_CTRL);
X	sys_write32(dev_data->ctl_reg_val, ((const struct uart_device_config *)(dev)->config->config_info)->regs + 0x0);
N
N	return 0;
N}
N
Nstatic int uart_device_ctrl(struct device *dev, u32_t ctrl_command,
N			   void *context)
N{
N	if (ctrl_command == DEVICE_PM_SET_POWER_STATE) {
X	if (ctrl_command == 1) {
N		if (*((u32_t *)context) == DEVICE_PM_SUSPEND_STATE) {
X		if (*((u32_t *)context) == 3) {
N			return uart_acts_suspend_device(dev);
N		} else if (*((u32_t *)context) == DEVICE_PM_ACTIVE_STATE) {
X		} else if (*((u32_t *)context) == 1) {
N			return uart_acts_resume_device_from_suspend(dev);
N		}
N	}
N
N	return 0;
N}
N
N#ifdef CONFIG_UART_0
N/* Forward declare function */
Nstatic void uart_acts_irq_config_0(struct device *port);
N
Nstatic const struct uart_device_config uart_acts_dev_cfg_0 = {
N	.base = (u8_t *)UART0_BASE,
X	.base = (u8_t *)0x4000D000,
N	.irq_config_func = uart_acts_irq_config_0,
N};
N
Nstatic struct uart_acts_dev_data_t uart_acts_dev_data_0 = {
N	.baud_rate = 115200,
N	.clock_id = 5,
N	.reset_id = 5,
N};
N
NDEVICE_DEFINE(uart_acts_0, "UART_0", &uart_acts_init,
N		      uart_device_ctrl, &uart_acts_dev_data_0, &uart_acts_dev_cfg_0, PRE_KERNEL_1,
N		      CONFIG_KERNEL_INIT_PRIORITY_DEVICE, NULL);
Xstatic struct device_config __config_uart_acts_0 __attribute__((__used__)) __attribute__((__section__(".devconfig.init"))) = { . name = "UART_0", . init = (&uart_acts_init), . device_pm_control = (uart_device_ctrl), . config_info = (&uart_acts_dev_cfg_0) }; static struct device __device_uart_acts_0 __attribute__((__used__)) __attribute__((__section__(".init_" "PRE_KERNEL_1" "50"))) = { . config = &__config_uart_acts_0, . driver_api = 0, . driver_data = &uart_acts_dev_data_0 };
N
Nstatic __init_once_text void uart_acts_irq_config_0(struct device *port)
Xstatic __attribute__((section("." "init.once.text" "." "\"..\\\\..\\\\..\\\\..\\\\drivers\\\\uart\\\\uart_acts.c\"" "." "0"))) void uart_acts_irq_config_0(struct device *port)
N{
N	IRQ_CONNECT(UART0_IRQn,
N		    0,
N		    uart_acts_isr, DEVICE_GET(uart_acts_0),
N		    0);
X	({ _sw_isr_table[15]. arg = (void *)(&(__device_uart_acts_0)); _sw_isr_table[15]. isr = (void (*)(void *))uart_acts_isr;; _irq_priority_set(15, 0, 0); 15; });
N	irq_enable(UART0_IRQn);
X	_arch_irq_enable(15);
N}
N#endif
N
N#ifdef CONFIG_UART_1
S
S/* Forward declare function */
Sstatic void uart_acts_irq_config_1(struct device *port);
S
Sstatic const struct uart_device_config uart_acts_dev_cfg_1 = {
S	.base = (u8_t *)UART1_BASE,
S	.irq_config_func = uart_acts_irq_config_1,
S};
S
Sstatic struct uart_acts_dev_data_t uart_acts_dev_data_1 = {
S	.baud_rate = 115200,
S	.clock_id = 6,
S	.reset_id = 6,
S};
S
SDEVICE_DEFINE(uart_acts_1, "UART_1", &uart_acts_init,
S		      uart_device_ctrl, &uart_acts_dev_data_1, &uart_acts_dev_cfg_1, PRE_KERNEL_1,
S		      CONFIG_KERNEL_INIT_PRIORITY_DEVICE, NULL);
S
Sstatic void uart_acts_irq_config_1(struct device *port)
S{
S	IRQ_CONNECT(UART1_IRQn,
S		    0,
S		    uart_acts_isr, DEVICE_GET(uart_acts_1),
S		    0);
S	irq_enable(UART1_IRQn);
S}
N#endif
N
N#ifdef CONFIG_UART_2
S/* Forward declare function */
Sstatic void uart_acts_irq_config_2(struct device *port);
S
Sstatic const struct uart_device_config uart_acts_dev_cfg_2 = {
S	.base = (u8_t *)UART2_BASE,
S	.irq_config_func = uart_acts_irq_config_2,
S};
S
Sstatic struct uart_acts_dev_data_t uart_acts_dev_data_2 = {
S	.baud_rate = 115200,
S	.clock_id = 7,
S	.reset_id = 7,
S};
S
SDEVICE_DEFINE(uart_acts_2, "UART_2", &uart_acts_init,
S	    uart_device_ctrl, &uart_acts_dev_data_2, &uart_acts_dev_cfg_2, PRE_KERNEL_1,
S	    CONFIG_KERNEL_INIT_PRIORITY_DEVICE, NULL);
S
Sstatic void uart_acts_irq_config_2(struct device *port)
S{
S	IRQ_CONNECT(UART2_IRQn,
S		    2,
S		    uart_acts_isr, DEVICE_GET(uart_acts_2),
S		    0);
S	irq_enable(UART2_IRQn);
S}
N#endif
